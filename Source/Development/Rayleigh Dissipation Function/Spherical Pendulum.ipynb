{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import sys \n",
                "sys.path.append(r'../../Python Script/')\n",
                "from sympy import symbols, simplify, derive_by_array\n",
                "from scipy.integrate import solve_ivp\n",
                "from xLSINDy import *\n",
                "from sympy.physics.mechanics import *\n",
                "from sympy import *\n",
                "import sympy\n",
                "import torch\n",
                "import HLsearch as HL\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "states are: (x0, x1, x0_t, x1_t)\n",
                        "states derivatives are:  (x0_t, x1_t, x0_tt, x1_tt)\n"
                    ]
                }
            ],
            "source": [
                "states_dim = 4\n",
                "states = ()\n",
                "states_dot = ()\n",
                "for i in range(states_dim):\n",
                "    if(i<states_dim//2):\n",
                "        states = states + (symbols('x{}'.format(i)),)\n",
                "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
                "    else:\n",
                "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
                "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
                "print('states are:',states)\n",
                "print('states derivatives are: ', states_dot)\n",
                "\n",
                "#Turn from sympy to str\n",
                "states_sym = states\n",
                "states_dot_sym = states_dot\n",
                "states = list(str(descr) for descr in states)\n",
                "states_dot = list(str(descr) for descr in states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [],
            "source": [
                "#For friction force\n",
                "x0 = Symbol(states[0], real=True)\n",
                "x1 = Symbol(states[1], real=True)\n",
                "x0_t = Symbol(states[2],real=True)\n",
                "x1_t = Symbol(states[3],real=True)\n",
                "q = sympy.Array([x0, x1])\n",
                "qdot = sympy.Array([x0_t, x1_t])\n",
                "\n",
                "#True Rayleigh Dissipation function\n",
                "dummy = Symbol('a', real = True)\n",
                "R = dummy #0.5*k1*x0_t**2 + 0.5*k2*(x1_t - x0_t)**2 #+ k1*Abs(x0_t) + k2*Abs(x1_t - x0_t)\n",
                "\n",
                "#friction force\n",
                "f_forcing = sympy.Matrix(derive_by_array(R, qdot)) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [],
            "source": [
                "#for lagrangian\n",
                "x0 = dynamicsymbols(states[0], real=True)\n",
                "x1 = dynamicsymbols(states[1], real=True)\n",
                "x0_t = dynamicsymbols(states[0],1, real=True)\n",
                "x1_t = dynamicsymbols(states[1],1, real=True)\n",
                "T = symbols('T')\n",
                "m = symbols('m')\n",
                "L = symbols('L')\n",
                "g = symbols('g')\n",
                "\n",
                "#True Lagrangian\n",
                "L = 0.5*m*L**2*(x0_t**2 + x1_t**2*sin(x0)**2) + m*g*L*cos(x0)\n",
                "# Lagrange's method\n",
                "LM = LagrangesMethod(L, [x0,x1])\n",
                "LM.form_lagranges_equations()\n",
                "i_forcing = LM.forcing #internal forcing and gravity\n",
                "e_forcing = sympy.Matrix([0, T]) #external generalized force"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Substituting dynamic symbols\n",
                "\n",
                "i_forcing = i_forcing.subs(x0_t, states_sym[2])\n",
                "i_forcing = i_forcing.subs(x1_t, states_sym[3])\n",
                "i_forcing = i_forcing.subs(x0, states_sym[0])\n",
                "i_forcing = i_forcing.subs(x1, states_sym[1])\n",
                "\n",
                "M = LM.mass_matrix\n",
                "M = M.subs(x0, states_sym[0])\n",
                "M = M.subs(x1, states_sym[1])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generating equation of motion\n",
                "t_forcing = i_forcing + e_forcing - f_forcing\n",
                "eom = M.inv()*sympy.Matrix(t_forcing)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": [
                "eom = simplify(eom)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Equation 0: 1.0*(L*x1_t**2*cos(x0) - g)*sin(x0)/L\n",
                        "\n",
                        "\n",
                        "Equation 1: 1.0*(-L**2*m*x0_t*x1_t*sin(2*x0) + T)/(L**2*m*sin(x0)**2)\n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "''' Please copy the string shown to the definition of equation in the function of double pendulum'''\n",
                "for i in range(len(eom)):\n",
                "    print('Equation ' + str(i) +': ' + str(eom[i]))\n",
                "    print('\\n')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "g = 9.81\n",
                "m = 1\n",
                "L,l = 1,1\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "def spherePend(t,y,T = 1.0):\n",
                "    from numpy import sin, cos, sign\n",
                "    x0,x1,x0_t,x1_t = y\n",
                "    x0_tt = 1.0*(L*x1_t**2*cos(x0) - g)*sin(x0)/L\n",
                "    x1_tt =  1.0*(-L**2*m*x0_t*x1_t*sin(2*x0) + T)/(L**2*m*sin(x0)**2)\n",
                "    return x0_t,x1_t,x0_tt,x1_tt\n",
                "\n",
                "\n",
                "def spherePendH(t,y,Moment=0.0):\n",
                "    from numpy import sin, cos\n",
                "    theta, psi, theta_t, psi_t = y\n",
                "    theta_2t = sin(theta)*cos(theta)*psi_t**2 - (g/l)*sin(theta)\n",
                "    psi_2t = -2*theta_t*psi_t*cos(theta)/sin(theta)\n",
                "    return theta_t,psi_t,theta_2t,psi_2t\n",
                "\n",
                "def generate_data(func, time, init_values):\n",
                "    sol = solve_ivp(func,[time[0],time[-1]],init_values,t_eval=time, method='LSODA', rtol=1e-10,atol=1e-10)\n",
                "    return sol.y.T, np.array([func(time[i],sol.y.T[i,:]) for i in range(sol.y.T.shape[0])],dtype=np.float64)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Saving Directory\n",
                "rootdir = \"../../Spherical Pendulum/Data/Active/\"\n",
                "\n",
                "num_sample = 100\n",
                "create_data = False\n",
                "training = True\n",
                "save = True\n",
                "noiselevel = 6e-2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Create training data\n",
                "if(create_data):\n",
                "    print(\"Creating Data . . .\")\n",
                "    X, Xdot = [], []\n",
                "    Tau = []\n",
                "    for i in range(num_sample):\n",
                "        t = np.arange(0,5,0.01)\n",
                "        theta = np.random.uniform(np.pi/3, np.pi/2)\n",
                "        \n",
                "        tau = np.zeros((len(t), 2))\n",
                "        tau[:,1] = 1.0    \n",
                "        y0=np.array([theta, 0, 0, np.pi])\n",
                "        x,xdot = generate_data(spherePend,t,y0)\n",
                "        \n",
                "        #Omega.append(omega)\n",
                "        Tau.append(tau)\n",
                "        X.append(x)\n",
                "        Xdot.append(xdot)\n",
                "\n",
                "    X = np.vstack(X)\n",
                "    Xdot = np.vstack(Xdot)\n",
                "    Tau = np.vstack(Tau)\n",
                "    if(save==True):\n",
                "        np.save(rootdir + \"X.npy\", X)\n",
                "        np.save(rootdir + \"Xdot.npy\",Xdot)\n",
                "        np.save(rootdir + \"Tau.npy\", Tau)\n",
                "else:\n",
                "    X = np.load(rootdir + \"X.npy\")\n",
                "    Xdot = np.load(rootdir + \"Xdot.npy\")\n",
                "    Tau = np.load(rootdir + \"Tau.npy\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [],
            "source": [
                "#adding noise\n",
                "mu, sigma = 0, noiselevel\n",
                "noise = np.random.normal(mu, sigma, X.shape[0])\n",
                "for i in range(X.shape[1]):\n",
                "    X[:,i] = X[:,i]+noise\n",
                "    Xdot[:,i] = Xdot[:,i]+noise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "states are: (x0, x1, x0_t, x1_t)\n",
                        "states derivatives are:  (x0_t, x1_t, x0_tt, x1_tt)\n"
                    ]
                }
            ],
            "source": [
                "states_dim = 4\n",
                "states = ()\n",
                "states_dot = ()\n",
                "for i in range(states_dim):\n",
                "    if(i<states_dim//2):\n",
                "        states = states + (symbols('x{}'.format(i)),)\n",
                "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
                "    else:\n",
                "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
                "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
                "print('states are:',states)\n",
                "print('states derivatives are: ', states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Turn from sympy to str\n",
                "states_sym = states\n",
                "states_dot_sym = states_dot\n",
                "states = list(str(descr) for descr in states)\n",
                "states_dot = list(str(descr) for descr in states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [],
            "source": [
                "#build function expression for the library in str\n",
                "exprdummy = HL.buildFunctionExpressions(1,states_dim,states,use_sine=True)\n",
                "polynom = exprdummy[1:4]\n",
                "trig = [exprdummy[4], exprdummy[6]]\n",
                "polynom = HL.buildFunctionExpressions(2,len(polynom),polynom)\n",
                "trig = HL.buildFunctionExpressions(2, len(trig),trig)\n",
                "product = []\n",
                "for p in polynom:\n",
                "    for t in trig:\n",
                "        product.append(p + '*' + t)\n",
                "expr = polynom + trig + product"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Boundaries for debugging with only the correct terms ###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Creating library tensor\n",
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Case I, input torque provided##\n",
                "expr = np.array(expr)\n",
                "i0 = np.where(expr == 'x0_t**2*cos(x0)**2')[0]\n",
                "\n",
                "idx = np.arange(0,len(expr))\n",
                "idx = np.delete(idx,[0])\n",
                "expr = np.delete(expr,[i0])\n",
                "\n",
                "\n",
                "#non-penalty index from prev knowledge\n",
                "i1 = np.where(expr == 'x0_t**2')[0][0]\n",
                "i2 = np.where(expr == 'cos(x0)')[0][0]\n",
                "\n",
                "nonpenaltyidx = [i1,i2]\n",
                "\n",
                "expr = expr.tolist()\n",
                "\n",
                "Zeta = Zeta[:,:,idx,:]\n",
                "Eta = Eta[:,:,idx,:]\n",
                "Delta = Delta[:,idx,:]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Moving to Cuda\n",
                "device = 'cuda:0'\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n",
                "\n",
                "#computing upsilon\n",
                "UpsilonR = Upsilonforward(Zeta, Eta, Delta, Xdot, device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [],
            "source": [
                "xi_L = torch.ones(len(expr), device=device).data.uniform_(-10,10)\n",
                "prevxi_L = xi_L.clone().detach()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [],
            "source": [
                "def loss(pred, targ):\n",
                "    loss = torch.mean((pred - targ)**2) \n",
                "    return loss "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clip(w, alpha):\n",
                "    clipped = torch.minimum(w,alpha)\n",
                "    clipped = torch.maximum(clipped,-alpha)\n",
                "    return clipped\n",
                "\n",
                "def proxL1norm(w_hat, alpha, nonpenaltyidx):\n",
                "    if(torch.is_tensor(alpha)==False):\n",
                "        alpha = torch.tensor(alpha)\n",
                "    w = w_hat - clip(w_hat,alpha)\n",
                "    for idx in nonpenaltyidx:\n",
                "        w[idx] = w_hat[idx]\n",
                "    return w"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "metadata": {},
            "outputs": [],
            "source": [
                "def training_loop(coef, prevcoef, UpsilonR, Tau, xdot, bs, lr, lam, momentum=True):\n",
                "    loss_list = []\n",
                "    tl = xdot.shape[0]\n",
                "    n = xdot.shape[1]\n",
                "\n",
                "    if(torch.is_tensor(xdot)==False):\n",
                "        xdot = torch.from_numpy(xdot).to(device).float()\n",
                "    if(torch.is_tensor(Tau)==False):\n",
                "        Tau = torch.from_numpy(Tau).to(device).float()\n",
                "\n",
                "    v = coef.clone().detach().requires_grad_(True)\n",
                "    prev = v\n",
                "    \n",
                "    for i in range(tl//bs):\n",
                "                \n",
                "        #computing acceleration with momentum\n",
                "        if(momentum==True):\n",
                "            vhat = (v + ((i-1)/(i+2))*(v - prev)).clone().detach().requires_grad_(True)\n",
                "        else:\n",
                "            vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
                "   \n",
                "        prev = v\n",
                "\n",
                "        #Computing loss\n",
                "        upsilonR = UpsilonR[:,:,i*bs:(i+1)*bs]\n",
                "        tau = Tau[i*bs:(i+1)*bs]\n",
                "\n",
                "\n",
                "        #forward\n",
                "        pred = torch.einsum('jkl,k->jl', upsilonR, vhat)\n",
                "        targ = tau.T\n",
                "        \n",
                "        lossval = loss(pred, targ)\n",
                "        \n",
                "        #Backpropagation\n",
                "        lossval.backward()\n",
                "\n",
                "        with torch.no_grad():\n",
                "            v = vhat - lr*vhat.grad\n",
                "            v = (proxL1norm(v,lr*lam,nonpenaltyidx))\n",
                "            \n",
                "            # Manually zero the gradients after updating weights\n",
                "            vhat.grad = None\n",
                "        \n",
                "        \n",
                "    \n",
                "        \n",
                "        loss_list.append(lossval.item())\n",
                "    print(\"Average loss : \" , torch.tensor(loss_list).mean().item())\n",
                "    return v, prevcoef, torch.tensor(loss_list).mean().item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 82,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 0/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515112668275833\n",
                        "Epoch 1/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149714052677155\n",
                        "Epoch 2/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151499941945076\n",
                        "Epoch 3/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149955451488495\n",
                        "Epoch 4/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149687230587006\n",
                        "Epoch 5/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515008807182312\n",
                        "Epoch 6/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514991819858551\n",
                        "Epoch 7/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149876475334167\n",
                        "Epoch 8/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149830281734467\n",
                        "Epoch 9/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149793028831482\n",
                        "Epoch 10/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149547159671783\n",
                        "Epoch 11/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149778127670288\n",
                        "Epoch 12/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150022506713867\n",
                        "Epoch 13/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150021016597748\n",
                        "Epoch 14/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514967530965805\n",
                        "Epoch 15/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150009095668793\n",
                        "Epoch 16/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150459110736847\n",
                        "Epoch 17/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514977514743805\n",
                        "Epoch 18/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149974822998047\n",
                        "Epoch 19/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149903297424316\n",
                        "Epoch 20/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514997035264969\n",
                        "Epoch 21/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149584412574768\n",
                        "Epoch 22/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149931609630585\n",
                        "Epoch 23/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150412917137146\n",
                        "Epoch 24/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150555968284607\n",
                        "Epoch 25/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514986753463745\n",
                        "Epoch 26/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149976313114166\n",
                        "Epoch 27/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149974822998047\n",
                        "Epoch 28/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149740874767303\n",
                        "Epoch 29/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150098502635956\n",
                        "Epoch 30/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150196850299835\n",
                        "Epoch 31/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514991968870163\n",
                        "Epoch 32/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514928638935089\n",
                        "Epoch 33/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149769186973572\n",
                        "Epoch 34/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149758756160736\n",
                        "Epoch 35/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149544179439545\n",
                        "Epoch 36/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149949491024017\n",
                        "Epoch 37/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149849653244019\n",
                        "Epoch 38/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149769186973572\n",
                        "Epoch 39/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149997174739838\n",
                        "Epoch 40/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149661898612976\n",
                        "Epoch 41/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151499941945076\n",
                        "Epoch 42/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514994353055954\n",
                        "Epoch 43/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514969915151596\n",
                        "Epoch 44/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150171518325806\n",
                        "Epoch 45/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514982432126999\n",
                        "Epoch 46/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149490535259247\n",
                        "Epoch 47/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149952471256256\n",
                        "Epoch 48/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515020728111267\n",
                        "Epoch 49/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514984369277954\n",
                        "Epoch 50/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514986753463745\n",
                        "Epoch 51/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149879455566406\n",
                        "Epoch 52/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514998972415924\n",
                        "Epoch 53/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149874985218048\n",
                        "Epoch 54/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515013873577118\n",
                        "Epoch 55/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149857103824615\n",
                        "Epoch 56/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515052616596222\n",
                        "Epoch 57/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149681270122528\n",
                        "Epoch 58/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150299668312073\n",
                        "Epoch 59/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515018492937088\n",
                        "Epoch 60/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151495561003685\n",
                        "Epoch 61/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150043368339539\n",
                        "Epoch 62/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514967828989029\n",
                        "Epoch 63/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514999121427536\n",
                        "Epoch 64/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150025486946106\n",
                        "Epoch 65/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150578320026398\n",
                        "Epoch 66/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150293707847595\n",
                        "Epoch 67/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149986743927002\n",
                        "Epoch 68/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149971842765808\n",
                        "Epoch 69/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149642527103424\n",
                        "Epoch 70/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149672329425812\n",
                        "Epoch 71/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150000154972076\n",
                        "Epoch 72/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149863064289093\n",
                        "Epoch 73/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149565041065216\n",
                        "Epoch 74/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149755775928497\n",
                        "Epoch 75/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149766206741333\n",
                        "Epoch 76/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514982134103775\n",
                        "Epoch 77/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515016257762909\n",
                        "Epoch 78/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149745345115662\n",
                        "Epoch 79/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150190889835358\n",
                        "Epoch 80/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149874985218048\n",
                        "Epoch 81/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149512887001038\n",
                        "Epoch 82/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149571001529694\n",
                        "Epoch 83/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149767696857452\n",
                        "Epoch 84/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515038162469864\n",
                        "Epoch 85/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151497483253479\n",
                        "Epoch 86/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515001803636551\n",
                        "Epoch 87/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149684250354767\n",
                        "Epoch 88/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149767696857452\n",
                        "Epoch 89/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150202810764313\n",
                        "Epoch 90/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150035917758942\n",
                        "Epoch 91/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515006721019745\n",
                        "Epoch 92/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149883925914764\n",
                        "Epoch 93/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514979600906372\n",
                        "Epoch 94/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149779617786407\n",
                        "Epoch 95/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514987349510193\n",
                        "Epoch 96/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149743854999542\n",
                        "Epoch 97/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515021026134491\n",
                        "Epoch 98/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515023112297058\n",
                        "Epoch 99/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514975130558014\n",
                        "Epoch 100/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149934589862823\n",
                        "Epoch 101/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149827301502228\n",
                        "Epoch 102/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150009095668793\n",
                        "Epoch 103/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149946510791779\n",
                        "Epoch 104/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514953076839447\n",
                        "Epoch 105/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150006115436554\n",
                        "Epoch 106/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150031447410583\n",
                        "Epoch 107/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150168538093567\n",
                        "Epoch 108/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149949491024017\n",
                        "Epoch 109/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150007605552673\n",
                        "Epoch 110/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149694681167603\n",
                        "Epoch 111/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149585902690887\n",
                        "Epoch 112/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149983763694763\n",
                        "Epoch 113/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514981985092163\n",
                        "Epoch 114/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514991819858551\n",
                        "Epoch 115/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149615705013275\n",
                        "Epoch 116/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150058269500732\n",
                        "Epoch 117/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149877965450287\n",
                        "Epoch 118/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150295197963715\n",
                        "Epoch 119/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149646997451782\n",
                        "Epoch 120/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150119364261627\n",
                        "Epoch 121/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151496022939682\n",
                        "Epoch 122/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149886906147003\n",
                        "Epoch 123/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149693191051483\n",
                        "Epoch 124/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149980783462524\n",
                        "Epoch 125/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149936079978943\n",
                        "Epoch 126/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149757266044617\n",
                        "Epoch 127/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514964997768402\n",
                        "Epoch 128/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149620175361633\n",
                        "Epoch 129/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149874985218048\n",
                        "Epoch 130/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149664878845215\n",
                        "Epoch 131/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149790048599243\n",
                        "Epoch 132/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149478614330292\n",
                        "Epoch 133/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149886906147003\n",
                        "Epoch 134/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150262415409088\n",
                        "Epoch 135/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149962902069092\n",
                        "Epoch 136/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149906277656555\n",
                        "Epoch 137/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149502456188202\n",
                        "Epoch 138/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514953374862671\n",
                        "Epoch 139/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515001803636551\n",
                        "Epoch 140/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149933099746704\n",
                        "Epoch 141/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151500403881073\n",
                        "Epoch 142/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149658918380737\n",
                        "Epoch 143/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149547159671783\n",
                        "Epoch 144/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150347352027893\n",
                        "Epoch 145/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149639546871185\n",
                        "Epoch 146/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149852633476257\n",
                        "Epoch 147/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150044858455658\n",
                        "Epoch 148/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514960676431656\n",
                        "Epoch 149/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514969915151596\n",
                        "Epoch 150/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150021016597748\n",
                        "Epoch 151/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149688720703125\n",
                        "Epoch 152/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149705111980438\n",
                        "Epoch 153/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150031447410583\n",
                        "Epoch 154/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149569511413574\n",
                        "Epoch 155/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149825811386108\n",
                        "Epoch 156/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149976313114166\n",
                        "Epoch 157/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149812400341034\n",
                        "Epoch 158/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149971842765808\n",
                        "Epoch 159/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149641036987305\n",
                        "Epoch 160/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149778127670288\n",
                        "Epoch 161/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150035917758942\n",
                        "Epoch 162/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149924159049988\n",
                        "Epoch 163/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149801969528198\n",
                        "Epoch 164/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150082111358643\n",
                        "Epoch 165/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149657428264618\n",
                        "Epoch 166/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150068700313568\n",
                        "Epoch 167/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514909714460373\n",
                        "Epoch 168/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150350332260132\n",
                        "Epoch 169/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149500966072083\n",
                        "Epoch 170/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149864554405212\n",
                        "Epoch 171/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149851143360138\n",
                        "Epoch 172/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150052309036255\n",
                        "Epoch 173/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514998972415924\n",
                        "Epoch 174/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149974822998047\n",
                        "Epoch 175/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151496022939682\n",
                        "Epoch 176/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149733424186707\n",
                        "Epoch 177/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150010585784912\n",
                        "Epoch 178/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149933099746704\n",
                        "Epoch 179/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149834752082825\n",
                        "Epoch 180/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514948010444641\n",
                        "Epoch 181/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514974981546402\n",
                        "Epoch 182/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150205790996552\n",
                        "Epoch 183/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149511396884918\n",
                        "Epoch 184/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149545669555664\n",
                        "Epoch 185/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514977067708969\n",
                        "Epoch 186/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149621665477753\n",
                        "Epoch 187/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149971842765808\n",
                        "Epoch 188/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150311589241028\n",
                        "Epoch 189/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149424970149994\n",
                        "Epoch 190/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150105953216553\n",
                        "Epoch 191/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515016406774521\n",
                        "Epoch 192/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150193870067596\n",
                        "Epoch 193/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149487555027008\n",
                        "Epoch 194/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514974683523178\n",
                        "Epoch 195/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515001803636551\n",
                        "Epoch 196/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150125324726105\n",
                        "Epoch 197/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149644017219543\n",
                        "Epoch 198/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514999270439148\n",
                        "Epoch 199/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150214731693268\n",
                        "Epoch 200/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150120854377747\n",
                        "Epoch 201/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149982273578644\n",
                        "Epoch 202/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514996886253357\n",
                        "Epoch 203/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149523317813873\n",
                        "Epoch 204/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151500403881073\n",
                        "Epoch 205/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149812400341034\n",
                        "Epoch 206/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149803459644318\n",
                        "Epoch 207/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149889886379242\n",
                        "Epoch 208/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150001645088196\n",
                        "Epoch 209/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149834752082825\n",
                        "Epoch 210/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150070190429688\n",
                        "Epoch 211/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149986743927002\n",
                        "Epoch 212/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150462090969086\n",
                        "Epoch 213/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149784088134766\n",
                        "Epoch 214/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149836242198944\n",
                        "Epoch 215/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149739384651184\n",
                        "Epoch 216/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149885416030884\n",
                        "Epoch 217/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149778127670288\n",
                        "Epoch 218/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151499405503273\n",
                        "Epoch 219/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149866044521332\n",
                        "Epoch 220/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149831771850586\n",
                        "Epoch 221/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149511396884918\n",
                        "Epoch 222/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149681270122528\n",
                        "Epoch 223/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149936079978943\n",
                        "Epoch 224/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149547159671783\n",
                        "Epoch 225/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150178968906403\n",
                        "Epoch 226/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150028467178345\n",
                        "Epoch 227/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149936079978943\n",
                        "Epoch 228/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150009095668793\n",
                        "Epoch 229/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514994502067566\n",
                        "Epoch 230/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514946073293686\n",
                        "Epoch 231/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149833261966705\n",
                        "Epoch 232/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150009095668793\n",
                        "Epoch 233/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151505246758461\n",
                        "Epoch 234/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149594843387604\n",
                        "Epoch 235/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149573981761932\n",
                        "Epoch 236/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149812400341034\n",
                        "Epoch 237/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150007605552673\n",
                        "Epoch 238/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149590373039246\n",
                        "Epoch 239/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150119364261627\n",
                        "Epoch 240/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149834752082825\n",
                        "Epoch 241/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515018492937088\n",
                        "Epoch 242/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149782598018646\n",
                        "Epoch 243/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149779617786407\n",
                        "Epoch 244/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150108933448792\n",
                        "Epoch 245/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149155259132385\n",
                        "Epoch 246/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149453282356262\n",
                        "Epoch 247/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150344371795654\n",
                        "Epoch 248/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150237083435059\n",
                        "Epoch 249/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514972597360611\n",
                        "Epoch 250/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150068700313568\n",
                        "Epoch 251/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149548649787903\n",
                        "Epoch 252/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149381756782532\n",
                        "Epoch 253/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150122344493866\n",
                        "Epoch 254/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149858593940735\n",
                        "Epoch 255/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514957845211029\n",
                        "Epoch 256/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149280428886414\n",
                        "Epoch 257/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149888396263123\n",
                        "Epoch 258/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150247514247894\n",
                        "Epoch 259/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149857103824615\n",
                        "Epoch 260/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149584412574768\n",
                        "Epoch 261/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149536728858948\n",
                        "Epoch 262/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149684250354767\n",
                        "Epoch 263/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149421989917755\n",
                        "Epoch 264/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514979898929596\n",
                        "Epoch 265/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150046348571777\n",
                        "Epoch 266/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149834752082825\n",
                        "Epoch 267/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149852633476257\n",
                        "Epoch 268/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149861574172974\n",
                        "Epoch 269/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514950841665268\n",
                        "Epoch 270/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514982432126999\n",
                        "Epoch 271/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149736404418945\n",
                        "Epoch 272/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149854123592377\n",
                        "Epoch 273/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514962613582611\n",
                        "Epoch 274/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515008956193924\n",
                        "Epoch 275/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515001505613327\n",
                        "Epoch 276/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149709582328796\n",
                        "Epoch 277/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150044858455658\n",
                        "Epoch 278/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515018492937088\n",
                        "Epoch 279/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150271356105804\n",
                        "Epoch 280/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150009095668793\n",
                        "Epoch 281/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514977365732193\n",
                        "Epoch 282/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149644017219543\n",
                        "Epoch 283/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149864554405212\n",
                        "Epoch 284/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150228142738342\n",
                        "Epoch 285/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150314569473267\n",
                        "Epoch 286/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149812400341034\n",
                        "Epoch 287/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514940708875656\n",
                        "Epoch 288/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149474143981934\n",
                        "Epoch 289/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150150656700134\n",
                        "Epoch 290/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149778127670288\n",
                        "Epoch 291/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150202810764313\n",
                        "Epoch 292/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150000154972076\n",
                        "Epoch 293/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149584412574768\n",
                        "Epoch 294/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514970362186432\n",
                        "Epoch 295/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514972597360611\n",
                        "Epoch 296/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150012075901031\n",
                        "Epoch 297/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149569511413574\n",
                        "Epoch 298/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150588750839233\n",
                        "Epoch 299/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514989286661148\n",
                        "Epoch 300/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150068700313568\n",
                        "Epoch 301/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149962902069092\n",
                        "Epoch 302/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149636566638947\n",
                        "Epoch 303/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150146186351776\n",
                        "Epoch 304/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149672329425812\n",
                        "Epoch 305/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150128304958344\n",
                        "Epoch 306/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149687230587006\n",
                        "Epoch 307/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515001803636551\n",
                        "Epoch 308/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514991819858551\n",
                        "Epoch 309/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515001803636551\n",
                        "Epoch 310/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149687230587006\n",
                        "Epoch 311/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149636566638947\n",
                        "Epoch 312/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514962613582611\n",
                        "Epoch 313/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149354934692383\n",
                        "Epoch 314/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149646997451782\n",
                        "Epoch 315/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514996886253357\n",
                        "Epoch 316/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149745345115662\n",
                        "Epoch 317/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149137377738953\n",
                        "Epoch 318/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149852633476257\n",
                        "Epoch 319/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151496022939682\n",
                        "Epoch 320/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514967530965805\n",
                        "Epoch 321/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514989286661148\n",
                        "Epoch 322/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514974683523178\n",
                        "Epoch 323/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149730443954468\n",
                        "Epoch 324/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150244534015656\n",
                        "Epoch 325/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151500403881073\n",
                        "Epoch 326/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150393545627594\n",
                        "Epoch 327/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149758756160736\n",
                        "Epoch 328/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149573981761932\n",
                        "Epoch 329/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149469673633575\n",
                        "Epoch 330/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150350332260132\n",
                        "Epoch 331/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514938771724701\n",
                        "Epoch 332/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149974822998047\n",
                        "Epoch 333/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149597823619843\n",
                        "Epoch 334/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514950543642044\n",
                        "Epoch 335/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149681270122528\n",
                        "Epoch 336/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149688720703125\n",
                        "Epoch 337/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149344503879547\n",
                        "Epoch 338/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149830281734467\n",
                        "Epoch 339/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151504248380661\n",
                        "Epoch 340/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149535238742828\n",
                        "Epoch 341/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150132775306702\n",
                        "Epoch 342/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150102972984314\n",
                        "Epoch 343/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150019526481628\n",
                        "Epoch 344/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149827301502228\n",
                        "Epoch 345/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149982273578644\n",
                        "Epoch 346/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150043368339539\n",
                        "Epoch 347/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150143206119537\n",
                        "Epoch 348/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149585902690887\n",
                        "Epoch 349/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149781107902527\n",
                        "Epoch 350/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514977663755417\n",
                        "Epoch 351/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149539709091187\n",
                        "Epoch 352/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149612724781036\n",
                        "Epoch 353/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150153636932373\n",
                        "Epoch 354/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514999121427536\n",
                        "Epoch 355/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151503786444664\n",
                        "Epoch 356/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149354934692383\n",
                        "Epoch 357/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149830281734467\n",
                        "Epoch 358/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514967978000641\n",
                        "Epoch 359/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149736404418945\n",
                        "Epoch 360/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514987200498581\n",
                        "Epoch 361/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149861574172974\n",
                        "Epoch 362/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149718523025513\n",
                        "Epoch 363/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515006721019745\n",
                        "Epoch 364/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514972299337387\n",
                        "Epoch 365/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150262415409088\n",
                        "Epoch 366/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150152146816254\n",
                        "Epoch 367/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149645507335663\n",
                        "Epoch 368/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149889886379242\n",
                        "Epoch 369/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514986902475357\n",
                        "Epoch 370/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150299668312073\n",
                        "Epoch 371/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514974981546402\n",
                        "Epoch 372/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149903297424316\n",
                        "Epoch 373/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149860084056854\n",
                        "Epoch 374/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149804949760437\n",
                        "Epoch 375/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150025486946106\n",
                        "Epoch 376/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515040546655655\n",
                        "Epoch 377/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150167047977448\n",
                        "Epoch 378/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149681270122528\n",
                        "Epoch 379/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149609744548798\n",
                        "Epoch 380/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149912238121033\n",
                        "Epoch 381/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149962902069092\n",
                        "Epoch 382/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149755775928497\n",
                        "Epoch 383/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149709582328796\n",
                        "Epoch 384/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149615705013275\n",
                        "Epoch 385/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514960080385208\n",
                        "Epoch 386/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150371193885803\n",
                        "Epoch 387/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149565041065216\n",
                        "Epoch 388/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515004187822342\n",
                        "Epoch 389/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149830281734467\n",
                        "Epoch 390/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150080621242523\n",
                        "Epoch 391/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149670839309692\n",
                        "Epoch 392/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150000154972076\n",
                        "Epoch 393/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514998823404312\n",
                        "Epoch 394/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514994353055954\n",
                        "Epoch 395/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515004187822342\n",
                        "Epoch 396/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149949491024017\n",
                        "Epoch 397/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149793028831482\n",
                        "Epoch 398/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149971842765808\n",
                        "Epoch 399/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515006273984909\n",
                        "Epoch 400/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.151499941945076\n",
                        "Epoch 401/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514970064163208\n",
                        "Epoch 402/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515008956193924\n",
                        "Epoch 403/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150131285190582\n",
                        "Epoch 404/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149758756160736\n",
                        "Epoch 405/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514955312013626\n",
                        "Epoch 406/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149644017219543\n",
                        "Epoch 407/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149949491024017\n",
                        "Epoch 408/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149836242198944\n",
                        "Epoch 409/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149709582328796\n",
                        "Epoch 410/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150028467178345\n",
                        "Epoch 411/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514965146780014\n",
                        "Epoch 412/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149757266044617\n",
                        "Epoch 413/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514984369277954\n",
                        "Epoch 414/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150186419487\n",
                        "Epoch 415/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149877965450287\n",
                        "Epoch 416/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149545669555664\n",
                        "Epoch 417/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149764716625214\n",
                        "Epoch 418/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150099992752075\n",
                        "Epoch 419/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514977663755417\n",
                        "Epoch 420/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514994502067566\n",
                        "Epoch 421/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150076150894165\n",
                        "Epoch 422/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514972597360611\n",
                        "Epoch 423/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149709582328796\n",
                        "Epoch 424/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515025645494461\n",
                        "Epoch 425/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149980783462524\n",
                        "Epoch 426/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515006273984909\n",
                        "Epoch 427/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150119364261627\n",
                        "Epoch 428/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149849653244019\n",
                        "Epoch 429/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149712562561035\n",
                        "Epoch 430/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149889886379242\n",
                        "Epoch 431/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149731934070587\n",
                        "Epoch 432/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149979293346405\n",
                        "Epoch 433/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514965146780014\n",
                        "Epoch 434/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149547159671783\n",
                        "Epoch 435/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514946073293686\n",
                        "Epoch 436/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149907767772675\n",
                        "Epoch 437/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150003135204315\n",
                        "Epoch 438/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149857103824615\n",
                        "Epoch 439/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149283409118652\n",
                        "Epoch 440/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514960378408432\n",
                        "Epoch 441/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149755775928497\n",
                        "Epoch 442/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150034427642822\n",
                        "Epoch 443/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149883925914764\n",
                        "Epoch 444/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149737894535065\n",
                        "Epoch 445/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149769186973572\n",
                        "Epoch 446/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150204300880432\n",
                        "Epoch 447/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149910748004913\n",
                        "Epoch 448/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515016257762909\n",
                        "Epoch 449/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515069156885147\n",
                        "Epoch 450/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514977067708969\n",
                        "Epoch 451/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514996588230133\n",
                        "Epoch 452/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514994353055954\n",
                        "Epoch 453/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514984667301178\n",
                        "Epoch 454/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149882435798645\n",
                        "Epoch 455/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150250494480133\n",
                        "Epoch 456/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149995684623718\n",
                        "Epoch 457/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149633586406708\n",
                        "Epoch 458/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150457620620728\n",
                        "Epoch 459/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149693191051483\n",
                        "Epoch 460/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150131285190582\n",
                        "Epoch 461/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149644017219543\n",
                        "Epoch 462/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149833261966705\n",
                        "Epoch 463/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150251984596252\n",
                        "Epoch 464/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150363743305206\n",
                        "Epoch 465/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514938920736313\n",
                        "Epoch 466/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515021175146103\n",
                        "Epoch 467/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515020877122879\n",
                        "Epoch 468/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514982432126999\n",
                        "Epoch 469/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514977216720581\n",
                        "Epoch 470/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150043368339539\n",
                        "Epoch 471/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149925649166107\n",
                        "Epoch 472/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150053799152374\n",
                        "Epoch 473/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149258077144623\n",
                        "Epoch 474/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150241553783417\n",
                        "Epoch 475/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514992117881775\n",
                        "Epoch 476/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150012075901031\n",
                        "Epoch 477/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514989584684372\n",
                        "Epoch 478/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149365365505219\n",
                        "Epoch 479/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150077641010284\n",
                        "Epoch 480/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514955312013626\n",
                        "Epoch 481/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149931609630585\n",
                        "Epoch 482/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150247514247894\n",
                        "Epoch 483/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514982432126999\n",
                        "Epoch 484/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1514987051486969\n",
                        "Epoch 485/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150566399097443\n",
                        "Epoch 486/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149907767772675\n",
                        "Epoch 487/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149696171283722\n",
                        "Epoch 488/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150059759616852\n",
                        "Epoch 489/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149550139904022\n",
                        "Epoch 490/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149562060832977\n",
                        "Epoch 491/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149696171283722\n",
                        "Epoch 492/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15150171518325806\n",
                        "Epoch 493/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149670839309692\n",
                        "Epoch 494/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515001803636551\n",
                        "Epoch 495/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149636566638947\n",
                        "Epoch 496/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149563550949097\n",
                        "Epoch 497/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149858593940735\n",
                        "Epoch 498/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.1515015810728073\n",
                        "Epoch 499/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149621665477753\n",
                        "Epoch 500/500\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.15149584412574768\n"
                    ]
                }
            ],
            "source": [
                "Epoch = 500\n",
                "i = 0\n",
                "lr = 1e-4\n",
                "lam = 0\n",
                "temp = 200\n",
                "while(i<=Epoch):\n",
                "    print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
                "    print(\"Learning rate : \", lr)\n",
                "    xi_L, prevxi_L, lossitem= training_loop(xi_L,prevxi_L,UpsilonR,Tau,Xdot,128,lr=lr,lam=lam)\n",
                "    temp = lossitem\n",
                "    i+=1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result stage 1:  0.03*x0_t**2 - 0.03*x1**2 + 0.03*x1_t**2*sin(x0)**2 + 0.51*cos(x0)\n"
                    ]
                }
            ],
            "source": [
                "## Thresholding\n",
                "threshold = 1e-2\n",
                "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
                "expr = np.array(expr)[surv_index].tolist()\n",
                "\n",
                "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "\n",
                "## obtaining analytical model\n",
                "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
                "L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-3)\n",
                "print(\"Result stage 1: \", simplify(L))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 84,
            "metadata": {},
            "outputs": [],
            "source": [
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)\n",
                "\n",
                "expr = np.array(expr)\n",
                "i1 = np.where(expr == 'x0_t**2')[0][0]\n",
                "i2 = np.where(expr == 'cos(x0)')[0][0]\n",
                "\n",
                "nonpenaltyidx = [i1,i2]\n",
                "\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n",
                "\n",
                "#computing upsilon\n",
                "UpsilonR = Upsilonforward(Zeta, Eta, Delta, Xdot, device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Debugging computation\n",
                "expr = np.array(expr)\n",
                "i0 = np.where(expr == 'x0_t**2')[0][0]\n",
                "i1 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i2 = np.where(expr == 'x1_t**2*sin(x0)**2')[0][0]\n",
                "\n",
                "expr = [expr[i0],expr[i1],expr[i2]]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Creating library tensor\n",
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#define the true coefficient\n",
                "xi_True = torch.ones(len(expr))\n",
                "xi_True[0] = 0.5\n",
                "xi_True[1] = 9.81\n",
                "xi_True[2] = 0.5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Moving to Cuda\n",
                "device = 'cuda:0'\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n",
                "\n",
                "xi_True = xi_True.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#compute tau prediction\n",
                "xdot = torch.from_numpy(Xdot).to(device).float()\n",
                "UpsilonL = Upsilonforward(Zeta, Eta, Delta, xdot, device)\n",
                "TauPred = torch.einsum('jkl,k->jl', UpsilonL, xi_L).detach().cpu().numpy().T\n",
                "\n",
                "\n",
                "TauEL = ELforward(xi_L, Zeta, Eta, Delta, xdot, device).detach().cpu().numpy().T"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#plot the figures\n",
                "left_boundary = 1000\n",
                "right_boundary = 2000\n",
                "t = np.arange(left_boundary,right_boundary)\n",
                "plt.plot(t,Tau[left_boundary:right_boundary,0])\n",
                "plt.plot(t,TauPred[left_boundary:right_boundary,0])\n",
                "plt.plot(t,TauEL[left_boundary:right_boundary,0])\n",
                "plt.show()\n",
                "\n",
                "plt.plot(t,Tau[left_boundary:right_boundary,1])\n",
                "plt.plot(t,TauPred[left_boundary:right_boundary,1])\n",
                "plt.plot(t,TauEL[left_boundary:right_boundary,1])\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(t, Xdot[left_boundary:right_boundary,2])\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "t = np.arange(0,5,0.01)\n",
                "theta = np.random.uniform(np.pi/3, np.pi/2)\n",
                "\n",
                "tau = np.zeros((len(t), 2))    \n",
                "y0=np.array([theta, 0, 0, np.pi])\n",
                "x,xdot = generate_data(spherePend,t,y0)\n",
                "x_, xdot_ = generate_data(spherePendH,t,y0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(t,x[:,0])\n",
                "plt.plot(t,x_[:,0])\n",
                "plt.show()\n",
                "\n",
                "plt.plot(t,x[:,1])\n",
                "plt.plot(t,x_[:,1])\n",
                "plt.show()\n",
                "\n",
                "plt.plot(t,x[:,2])\n",
                "plt.plot(t,x_[:,2])\n",
                "plt.show()\n",
                "\n",
                "plt.plot(t,x[:,3])\n",
                "plt.plot(t,x_[:,3])\n",
                "plt.show()\n",
                "\n",
                "plt.plot(t,xdot[:,2])\n",
                "plt.plot(t,xdot_[:,2])\n",
                "plt.show()\n",
                "\n",
                "plt.plot(t,xdot[:,3])\n",
                "plt.plot(t,xdot_[:,3])\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Debugging for training with only known terms\n",
                "expr = np.array(expr)\n",
                "i0 = np.where(expr == 'x0_t**2')[0][0]\n",
                "i1 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i2 = np.where(expr == 'x1_t**2*sin(x0)**2')[0][0]\n",
                "\n",
                "expr = [expr[i0],expr[i1],expr[i2]]\n",
                "\n",
                "#non-penalty index from prev knowledge\n",
                "expr = np.array(expr)\n",
                "i4 = np.where(expr == 'x0_t**2')[0][0]\n",
                "i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "nonpenaltyidx = [i4,i5]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Creating library tensor\n",
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Moving to Cuda\n",
                "device = 'cuda:0'\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#initialize coefficient\n",
                "xi_L = torch.ones(len(expr), device=device).data.uniform_(-10,10)\n",
                "prevxi_L = xi_L.clone().detach()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#compute Upsilon\n",
                "xdot = torch.from_numpy(Xdot).to(device).float()\n",
                "UpsilonR = Upsilonforward(Zeta, Eta, Delta, Xdot, device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.10 ('SystemIdentification')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "a4ed4680c7c46a218b8058c2660cec6a650dc98debbf7bcbd09838ba710de1ba"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
