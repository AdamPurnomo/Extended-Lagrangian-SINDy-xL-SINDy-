{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import sys \n",
                "sys.path.append(r'../Python Script/')\n",
                "\n",
                "from sympy import symbols, simplify, derive_by_array\n",
                "from scipy.integrate import solve_ivp\n",
                "from xLSINDy import *\n",
                "from sympy.physics.mechanics import *\n",
                "from sympy import *\n",
                "import sympy\n",
                "import torch\n",
                "import HLsearch as HL\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# System parameters\n",
                "L1, L2 = 1, 1\n",
                "m1, m2 = 1, 1\n",
                "k1, k2 = 0.5, 0.5\n",
                "g = 9.8"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "states are: (x0, x1, x0_t, x1_t)\n",
                        "states derivatives are:  (x0_t, x1_t, x0_tt, x1_tt)\n"
                    ]
                }
            ],
            "source": [
                "states_dim = 4\n",
                "states = ()\n",
                "states_dot = ()\n",
                "for i in range(states_dim):\n",
                "    if(i<states_dim//2):\n",
                "        states = states + (symbols('x{}'.format(i)),)\n",
                "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
                "    else:\n",
                "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
                "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
                "print('states are:',states)\n",
                "print('states derivatives are: ', states_dot)\n",
                "\n",
                "#Turn from sympy to str\n",
                "states_sym = states\n",
                "states_dot_sym = states_dot\n",
                "states = list(str(descr) for descr in states)\n",
                "states_dot = list(str(descr) for descr in states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "#For friction force\n",
                "x0 = Symbol(states[0], real=True)\n",
                "x1 = Symbol(states[1], real=True)\n",
                "x0_t = Symbol(states[2],real=True)\n",
                "x1_t = Symbol(states[3],real=True)\n",
                "q = sympy.Array([x0, x1])\n",
                "qdot = sympy.Array([x0_t, x1_t])\n",
                "\n",
                "#True Rayleigh Dissipation function\n",
                "dummy = Symbol('a', real = True)\n",
                "R = dummy #0.5*k1*x0_t**2 + 0.5*k2*(x1_t - x0_t)**2 #+ k1*Abs(x0_t) + k2*Abs(x1_t - x0_t)\n",
                "\n",
                "#friction force\n",
                "f_forcing = sympy.Matrix(derive_by_array(R, qdot)) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "#for lagrangian\n",
                "x0 = dynamicsymbols(states[0], real=True)\n",
                "x1 = dynamicsymbols(states[1], real=True)\n",
                "x0_t = dynamicsymbols(states[0],1, real=True)\n",
                "x1_t = dynamicsymbols(states[1],1, real=True)\n",
                "tau0 = symbols('tau0')\n",
                "tau1 = symbols('tau1')\n",
                "\n",
                "#True Lagrangian\n",
                "L = 0.5*(m1+m2)*L1**2*x0_t**2 + 0.5*m2*L2**2*x1_t**2 + m2*L2**2*x0_t*x1_t*cos(x0)*cos(x1) + m2*L2**2*x0_t*x1_t*sin(x0)*sin(x1) + (m1+m2)*g*L1*cos(x0) + m2*g*L2*cos(x1)\n",
                "\n",
                "# Lagrange's method\n",
                "LM = LagrangesMethod(L, [x0,x1])\n",
                "LM.form_lagranges_equations()\n",
                "i_forcing = LM.forcing #internal forcing and gravity\n",
                "e_forcing = sympy.Matrix([tau0-tau1, tau1]) #external generalized force"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Substituting dynamic symbols\n",
                "\n",
                "i_forcing = i_forcing.subs(x0_t, states_sym[2])\n",
                "i_forcing = i_forcing.subs(x1_t, states_sym[3])\n",
                "i_forcing = i_forcing.subs(x0, states_sym[0])\n",
                "i_forcing = i_forcing.subs(x1, states_sym[1])\n",
                "\n",
                "M = LM.mass_matrix\n",
                "M = M.subs(x0, states_sym[0])\n",
                "M = M.subs(x1, states_sym[1])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generating equation of motion\n",
                "t_forcing = i_forcing + e_forcing - f_forcing\n",
                "eom = M.inv()*sympy.Matrix(t_forcing)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Equation 0: 1.0*(2.0*sin(x0)*sin(x1) + 2.0*cos(x0)*cos(x1))*(tau1 + x0_t**2*sin(x0)*cos(x1) - x0_t**2*sin(x1)*cos(x0) - 9.8*sin(x1))/(2.0*sin(x0)**2*sin(x1)**2 + 4.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 2.0*cos(x0)**2*cos(x1)**2 - 4.0) - 2.0*(tau0 - tau1 - x1_t**2*sin(x0)*cos(x1) + x1_t**2*sin(x1)*cos(x0) - 19.6*sin(x0))/(2.0*sin(x0)**2*sin(x1)**2 + 4.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 2.0*cos(x0)**2*cos(x1)**2 - 4.0)\n",
                        "\n",
                        "\n",
                        "Equation 1: 1.0*(1.0*sin(x0)*sin(x1) + 1.0*cos(x0)*cos(x1))*(tau0 - tau1 - x1_t**2*sin(x0)*cos(x1) + x1_t**2*sin(x1)*cos(x0) - 19.6*sin(x0))/(1.0*sin(x0)**2*sin(x1)**2 + 2.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 1.0*cos(x0)**2*cos(x1)**2 - 2.0) - 2.0*(tau1 + x0_t**2*sin(x0)*cos(x1) - x0_t**2*sin(x1)*cos(x0) - 9.8*sin(x1))/(1.0*sin(x0)**2*sin(x1)**2 + 2.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 1.0*cos(x0)**2*cos(x1)**2 - 2.0)\n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "''' Please copy the string shown to the definition of equation in the function of double pendulum'''\n",
                "for i in range(len(eom)):\n",
                "    print('Equation ' + str(i) +': ' + str(eom[i]))\n",
                "    print('\\n')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "def torque(t,omega):\n",
                "    return 1*np.cos(omega*t), 1*np.sin(omega*t)\n",
                "\n",
                "def doublePendulum(t,y,omega):\n",
                "    from numpy import sin, cos, sign\n",
                "    x0,x1,x0_t,x1_t = y\n",
                "    tau0, tau1 = torque(t, omega)\n",
                "    x0_tt = 1.0*(2.0*sin(x0)*sin(x1) + 2.0*cos(x0)*cos(x1))*(tau1 + x0_t**2*sin(x0)*cos(x1) - x0_t**2*sin(x1)*cos(x0) - 9.8*sin(x1))/(2.0*sin(x0)**2*sin(x1)**2 + 4.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 2.0*cos(x0)**2*cos(x1)**2 - 4.0) - 2.0*(tau0 - tau1 - x1_t**2*sin(x0)*cos(x1) + x1_t**2*sin(x1)*cos(x0) - 19.6*sin(x0))/(2.0*sin(x0)**2*sin(x1)**2 + 4.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 2.0*cos(x0)**2*cos(x1)**2 - 4.0)\n",
                "    x1_tt =  1.0*(1.0*sin(x0)*sin(x1) + 1.0*cos(x0)*cos(x1))*(tau0 - tau1 - x1_t**2*sin(x0)*cos(x1) + x1_t**2*sin(x1)*cos(x0) - 19.6*sin(x0))/(1.0*sin(x0)**2*sin(x1)**2 + 2.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 1.0*cos(x0)**2*cos(x1)**2 - 2.0) - 2.0*(tau1 + x0_t**2*sin(x0)*cos(x1) - x0_t**2*sin(x1)*cos(x0) - 9.8*sin(x1))/(1.0*sin(x0)**2*sin(x1)**2 + 2.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 1.0*cos(x0)**2*cos(x1)**2 - 2.0)\n",
                "    return x0_t,x1_t,x0_tt,x1_tt\n",
                "\n",
                "\n",
                "def generate_data(func, time, init_values, omega):\n",
                "    sol = solve_ivp(func,[time[0],time[-1]],init_values,t_eval=time, method='LSODA', rtol=1e-10,atol=1e-10, args=[omega])\n",
                "    return sol.y.T, np.array([func(time[i],sol.y.T[i,:], omega = omega) for i in range(sol.y.T.shape[0])],dtype=np.float64)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Saving Directory\n",
                "rootdir = \"../Double Pendulum/Data/\"\n",
                "\n",
                "num_sample = 100\n",
                "create_data = True\n",
                "training = True\n",
                "save = False\n",
                "noiselevel = 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating Data . . .\n"
                    ]
                }
            ],
            "source": [
                "#Create training data\n",
                "if(create_data):\n",
                "    print(\"Creating Data . . .\")\n",
                "    num_sample = 100\n",
                "    X, Xdot = [], []\n",
                "    Tau = []\n",
                "    Omega = []\n",
                "    for i in range(num_sample):\n",
                "        t = np.arange(0,5,0.01)\n",
                "        theta1 = np.random.uniform(-np.pi, np.pi)\n",
                "        theta2 = np.random.uniform(-np.pi, np.pi)\n",
                "        thetadot = np.random.uniform(0,0)\n",
                "        omega = np.random.uniform(-np.pi, np.pi)\n",
                "        \n",
                "        tau0, tau1 = torque(t, omega)\n",
                "        tau = np.array([tau0 - tau1, tau1]).T    \n",
                "        y0=np.array([theta1, theta2, thetadot, thetadot])\n",
                "        x,xdot = generate_data(doublePendulum,t,y0,omega=omega)\n",
                "        \n",
                "        Omega.append(omega)\n",
                "        Tau.append(tau)\n",
                "        X.append(x)\n",
                "        Xdot.append(xdot)\n",
                "\n",
                "    X = np.vstack(X)\n",
                "    Xdot = np.vstack(Xdot)\n",
                "    Tau = np.vstack(Tau)\n",
                "    if(save==True):\n",
                "        np.save(rootdir + \"X.npy\", X)\n",
                "        np.save(rootdir + \"Xdot.npy\",Xdot)\n",
                "else:\n",
                "    X = np.load(rootdir + \"X.npy\")\n",
                "    Xdot = np.load(rootdir + \"Xdot.npy\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "#adding noise\n",
                "mu, sigma = 0, noiselevel\n",
                "noise = np.random.normal(mu, sigma, X.shape[0])\n",
                "for i in range(X.shape[1]):\n",
                "    X[:,i] = X[:,i]+noise\n",
                "    Xdot[:,i] = Xdot[:,i]+noise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "states are: (x0, x1, x0_t, x1_t)\n",
                        "states derivatives are:  (x0_t, x1_t, x0_tt, x1_tt)\n"
                    ]
                }
            ],
            "source": [
                "states_dim = 4\n",
                "states = ()\n",
                "states_dot = ()\n",
                "for i in range(states_dim):\n",
                "    if(i<states_dim//2):\n",
                "        states = states + (symbols('x{}'.format(i)),)\n",
                "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
                "    else:\n",
                "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
                "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
                "print('states are:',states)\n",
                "print('states derivatives are: ', states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Turn from sympy to str\n",
                "states_sym = states\n",
                "states_dot_sym = states_dot\n",
                "states = list(str(descr) for descr in states)\n",
                "states_dot = list(str(descr) for descr in states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "#build function expression for the library in str\n",
                "exprdummy = HL.buildFunctionExpressions(1,states_dim,states,use_sine=True)\n",
                "polynom = exprdummy[2:4]\n",
                "trig = exprdummy[4:]\n",
                "polynom = HL.buildFunctionExpressions(2,len(polynom),polynom)\n",
                "trig = HL.buildFunctionExpressions(2, len(trig),trig)\n",
                "product = []\n",
                "for p in polynom:\n",
                "    for t in trig:\n",
                "        product.append(p + '*' + t)\n",
                "expr = polynom + trig + product"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Creating library tensor\n",
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Case I, input torque provided##\n",
                "expr = np.array(expr)\n",
                "\n",
                "\n",
                "## Garbage terms ##\n",
                "\n",
                "'''\n",
                "Explanation :\n",
                "x0_t, x1_t terms are not needed and will always satisfy EL's equation.\n",
                "Since x0_t, x1_t are garbages, we want to avoid (x0_t*sin()**2 + x0_t*cos()**2), thus we remove\n",
                "one of them, either  x0_t*sin()**2 or x0_t*cos()**2. \n",
                "Since the known term is x0_t**2, we also want to avoid the solution of (x0_t**2*sin()**2 + x0_t**2*cos()**2),\n",
                "so we remove either one of x0_t**2*sin()**2 or x0_t**2*cos()**2.\n",
                "'''\n",
                "\n",
                "i7 = np.where(expr == 'x1_t*cos(x0)**2')[0]\n",
                "i8 = np.where(expr == 'x1_t*cos(x1)**2')[0]\n",
                "i9 = np.where(expr == 'x1_t')[0]\n",
                "i10 = np.where(expr == 'x0_t*cos(x0)**2')[0]\n",
                "i11 = np.where(expr == 'x0_t*cos(x1)**2')[0]\n",
                "i12 = np.where(expr == 'x0_t')[0]\n",
                "i13 = np.where(expr == 'cos(x0)**2')[0]\n",
                "i14 = np.where(expr == 'cos(x1)**2')[0]\n",
                "\n",
                "#Deleting unused terms \n",
                "idx = np.arange(0,len(expr))\n",
                "idx = np.delete(idx,[i7,i8,i9,i10,i11,i12,i13,i14])\n",
                "expr = np.delete(expr,[i7,i8,i9,i10,i11,i12,i13,i14])\n",
                "\n",
                "#non-penalty index from prev knowledge\n",
                "i1 = np.where(expr == 'x0_t**2')[0][0]\n",
                "i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "nonpenaltyidx = [i1, i4, i5, i6]\n",
                "\n",
                "expr = expr.tolist()\n",
                "\n",
                "Zeta = Zeta[:,:,idx,:]\n",
                "Eta = Eta[:,:,idx,:]\n",
                "Delta = Delta[:,idx,:]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Moving to Cuda\n",
                "device = 'cuda:0'\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "xi_L = torch.ones(len(expr), device=device).data.uniform_(-100,100)\n",
                "prevxi_L = xi_L.clone().detach()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "def loss(pred, targ):\n",
                "    loss = torch.mean((pred - targ)**2) \n",
                "    return loss "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clip(w, alpha):\n",
                "    clipped = torch.minimum(w,alpha)\n",
                "    clipped = torch.maximum(clipped,-alpha)\n",
                "    return clipped\n",
                "\n",
                "def proxL1norm(w_hat, alpha, nonpenaltyidx):\n",
                "    if(torch.is_tensor(alpha)==False):\n",
                "        alpha = torch.tensor(alpha)\n",
                "    w = w_hat - clip(w_hat,alpha)\n",
                "    for idx in nonpenaltyidx:\n",
                "        w[idx] = w_hat[idx]\n",
                "    return w"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "def training_loop(coef, prevcoef, RHS, Tau, xdot, bs, lr, lam, momentum=True):\n",
                "    loss_list = []\n",
                "    tl = xdot.shape[0]\n",
                "    n = xdot.shape[1]\n",
                "\n",
                "    Zeta, Eta, Delta = RHS\n",
                "\n",
                "    if(torch.is_tensor(xdot)==False):\n",
                "        xdot = torch.from_numpy(xdot).to(device).float()\n",
                "    if(torch.is_tensor(Tau)==False):\n",
                "        Tau = torch.from_numpy(Tau).to(device).float()\n",
                "\n",
                "    v = coef.clone().detach().requires_grad_(True)\n",
                "    prev = v\n",
                "    \n",
                "    for i in range(tl//bs):\n",
                "                \n",
                "        #computing acceleration with momentum\n",
                "        if(momentum==True):\n",
                "            vhat = (v + ((i-1)/(i+2))*(v - prev)).clone().detach().requires_grad_(True)\n",
                "        else:\n",
                "            vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
                "   \n",
                "        prev = v\n",
                "\n",
                "        #Computing loss\n",
                "        zeta = Zeta[:,:,:,i*bs:(i+1)*bs]\n",
                "        eta = Eta[:,:,:,i*bs:(i+1)*bs]\n",
                "        delta = Delta[:,:,i*bs:(i+1)*bs]\n",
                "\n",
                "        tau = Tau[i*bs:(i+1)*bs]\n",
                "\n",
                "        x_t = xdot[i*bs:(i+1)*bs,:]\n",
                "\n",
                "        #forward\n",
                "        pred = ELforward(vhat,zeta,eta,delta,x_t,device)\n",
                "        targ = tau.T\n",
                "        \n",
                "        lossval = loss(pred, targ)\n",
                "        \n",
                "        #Backpropagation\n",
                "        lossval.backward()\n",
                "\n",
                "        with torch.no_grad():\n",
                "            v = vhat - lr*vhat.grad\n",
                "            v = (proxL1norm(v,lr*lam,nonpenaltyidx))\n",
                "            \n",
                "            # Manually zero the gradients after updating weights\n",
                "            vhat.grad = None\n",
                "        \n",
                "        \n",
                "    \n",
                "        \n",
                "        loss_list.append(lossval.item())\n",
                "    print(\"Average loss : \" , torch.tensor(loss_list).mean().item())\n",
                "    return v, prevcoef, torch.tensor(loss_list).mean().item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "Epoch 0/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7434471.0\n",
                        "\n",
                        "\n",
                        "Epoch 1/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  514305.28125\n",
                        "\n",
                        "\n",
                        "Epoch 2/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  61801.19921875\n",
                        "\n",
                        "\n",
                        "Epoch 3/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  17080.517578125\n",
                        "\n",
                        "\n",
                        "Epoch 4/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  15210.4208984375\n",
                        "\n",
                        "\n",
                        "Epoch 5/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  16046.6220703125\n",
                        "\n",
                        "\n",
                        "Epoch 6/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  16475.3828125\n",
                        "\n",
                        "\n",
                        "Epoch 7/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  16267.810546875\n",
                        "\n",
                        "\n",
                        "Epoch 8/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  15728.6064453125\n",
                        "\n",
                        "\n",
                        "Epoch 9/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  15055.181640625\n",
                        "\n",
                        "\n",
                        "Epoch 10/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  14336.6484375\n",
                        "\n",
                        "\n",
                        "Epoch 11/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13665.470703125\n",
                        "\n",
                        "\n",
                        "Epoch 12/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13010.7666015625\n",
                        "\n",
                        "\n",
                        "Epoch 13/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12392.9111328125\n",
                        "\n",
                        "\n",
                        "Epoch 14/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11793.51171875\n",
                        "\n",
                        "\n",
                        "Epoch 15/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11235.3564453125\n",
                        "\n",
                        "\n",
                        "Epoch 16/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  10705.8896484375\n",
                        "\n",
                        "\n",
                        "Epoch 17/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  10170.619140625\n",
                        "\n",
                        "\n",
                        "Epoch 18/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  9639.1181640625\n",
                        "\n",
                        "\n",
                        "Epoch 19/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  9142.748046875\n",
                        "\n",
                        "\n",
                        "Epoch 20/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  8691.287109375\n",
                        "\n",
                        "\n",
                        "Epoch 21/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  8276.7705078125\n",
                        "\n",
                        "\n",
                        "Epoch 22/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7893.423828125\n",
                        "\n",
                        "\n",
                        "Epoch 23/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7536.06298828125\n",
                        "\n",
                        "\n",
                        "Epoch 24/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7206.81005859375\n",
                        "\n",
                        "\n",
                        "Epoch 25/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  6903.03466796875\n",
                        "\n",
                        "\n",
                        "Epoch 26/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  6638.37060546875\n",
                        "\n",
                        "\n",
                        "Epoch 27/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  6347.42822265625\n",
                        "\n",
                        "\n",
                        "Epoch 28/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  6074.1787109375\n",
                        "\n",
                        "\n",
                        "Epoch 29/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  5792.6181640625\n",
                        "\n",
                        "\n",
                        "Epoch 30/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  5516.4326171875\n",
                        "\n",
                        "\n",
                        "Epoch 31/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  5249.70361328125\n",
                        "\n",
                        "\n",
                        "Epoch 32/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  4992.046875\n",
                        "\n",
                        "\n",
                        "Epoch 33/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  4741.35400390625\n",
                        "\n",
                        "\n",
                        "Epoch 34/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  4502.05712890625\n",
                        "\n",
                        "\n",
                        "Epoch 35/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  4277.5263671875\n",
                        "\n",
                        "\n",
                        "Epoch 36/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  4060.3544921875\n",
                        "\n",
                        "\n",
                        "Epoch 37/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  3859.82666015625\n",
                        "\n",
                        "\n",
                        "Epoch 38/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  3666.7470703125\n",
                        "\n",
                        "\n",
                        "Epoch 39/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  3479.187255859375\n",
                        "\n",
                        "\n",
                        "Epoch 40/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  3303.978271484375\n",
                        "\n",
                        "\n",
                        "Epoch 41/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  3135.174560546875\n",
                        "\n",
                        "\n",
                        "Epoch 42/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  2965.912109375\n",
                        "\n",
                        "\n",
                        "Epoch 43/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  2772.742919921875\n",
                        "\n",
                        "\n",
                        "Epoch 44/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  2702.362548828125\n",
                        "\n",
                        "\n",
                        "Epoch 45/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  2610.65185546875\n",
                        "\n",
                        "\n",
                        "Epoch 46/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  2472.211181640625\n",
                        "\n",
                        "\n",
                        "Epoch 47/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  2334.607421875\n",
                        "\n",
                        "\n",
                        "Epoch 48/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  2206.9423828125\n",
                        "\n",
                        "\n",
                        "Epoch 49/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  2088.603759765625\n",
                        "\n",
                        "\n",
                        "Epoch 50/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1975.756103515625\n",
                        "\n",
                        "\n",
                        "Epoch 51/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1870.9627685546875\n",
                        "\n",
                        "\n",
                        "Epoch 52/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1774.150634765625\n",
                        "\n",
                        "\n",
                        "Epoch 53/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1677.547607421875\n",
                        "\n",
                        "\n",
                        "Epoch 54/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1588.5157470703125\n",
                        "\n",
                        "\n",
                        "Epoch 55/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1507.9293212890625\n",
                        "\n",
                        "\n",
                        "Epoch 56/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1434.6195068359375\n",
                        "\n",
                        "\n",
                        "Epoch 57/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1364.9453125\n",
                        "\n",
                        "\n",
                        "Epoch 58/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1298.75146484375\n",
                        "\n",
                        "\n",
                        "Epoch 59/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1236.21728515625\n",
                        "\n",
                        "\n",
                        "Epoch 60/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1168.439697265625\n",
                        "\n",
                        "\n",
                        "Epoch 61/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1099.7750244140625\n",
                        "\n",
                        "\n",
                        "Epoch 62/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1029.713134765625\n",
                        "\n",
                        "\n",
                        "Epoch 63/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  966.0623168945312\n",
                        "\n",
                        "\n",
                        "Epoch 64/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  904.54296875\n",
                        "\n",
                        "\n",
                        "Epoch 65/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  842.49609375\n",
                        "\n",
                        "\n",
                        "Epoch 66/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  786.2613525390625\n",
                        "\n",
                        "\n",
                        "Epoch 67/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  721.5154418945312\n",
                        "\n",
                        "\n",
                        "Epoch 68/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  670.4990844726562\n",
                        "\n",
                        "\n",
                        "Epoch 69/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  627.4865112304688\n",
                        "\n",
                        "\n",
                        "Epoch 70/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  584.8612060546875\n",
                        "\n",
                        "\n",
                        "Epoch 71/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  547.3359985351562\n",
                        "\n",
                        "\n",
                        "Epoch 72/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  510.6068115234375\n",
                        "\n",
                        "\n",
                        "Epoch 73/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  470.5682373046875\n",
                        "\n",
                        "\n",
                        "Epoch 74/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  436.62945556640625\n",
                        "\n",
                        "\n",
                        "Epoch 75/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  392.1164245605469\n",
                        "\n",
                        "\n",
                        "Epoch 76/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  355.5893859863281\n",
                        "\n",
                        "\n",
                        "Epoch 77/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  328.2314758300781\n",
                        "\n",
                        "\n",
                        "Epoch 78/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  306.6966857910156\n",
                        "\n",
                        "\n",
                        "Epoch 79/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  291.4377746582031\n",
                        "\n",
                        "\n",
                        "Epoch 80/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  283.8118591308594\n",
                        "\n",
                        "\n",
                        "Epoch 81/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  277.1532897949219\n",
                        "\n",
                        "\n",
                        "Epoch 82/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  265.6115417480469\n",
                        "\n",
                        "\n",
                        "Epoch 83/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  248.81832885742188\n",
                        "\n",
                        "\n",
                        "Epoch 84/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  232.00148010253906\n",
                        "\n",
                        "\n",
                        "Epoch 85/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  218.58819580078125\n",
                        "\n",
                        "\n",
                        "Epoch 86/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  202.67457580566406\n",
                        "\n",
                        "\n",
                        "Epoch 87/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  190.46676635742188\n",
                        "\n",
                        "\n",
                        "Epoch 88/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  179.35316467285156\n",
                        "\n",
                        "\n",
                        "Epoch 89/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  166.33470153808594\n",
                        "\n",
                        "\n",
                        "Epoch 90/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  153.8624267578125\n",
                        "\n",
                        "\n",
                        "Epoch 91/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  143.5332794189453\n",
                        "\n",
                        "\n",
                        "Epoch 92/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  133.39447021484375\n",
                        "\n",
                        "\n",
                        "Epoch 93/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  123.23168182373047\n",
                        "\n",
                        "\n",
                        "Epoch 94/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  114.69879913330078\n",
                        "\n",
                        "\n",
                        "Epoch 95/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  107.47191619873047\n",
                        "\n",
                        "\n",
                        "Epoch 96/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  100.5987777709961\n",
                        "\n",
                        "\n",
                        "Epoch 97/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  93.93867492675781\n",
                        "\n",
                        "\n",
                        "Epoch 98/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  88.15338897705078\n",
                        "\n",
                        "\n",
                        "Epoch 99/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  82.56810760498047\n",
                        "\n",
                        "\n",
                        "Epoch 100/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  77.32693481445312\n",
                        "\n",
                        "\n",
                        "Epoch 101/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  72.81624603271484\n",
                        "\n",
                        "\n",
                        "Epoch 102/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  69.5189437866211\n",
                        "\n",
                        "\n",
                        "Epoch 103/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  66.59127807617188\n",
                        "\n",
                        "\n",
                        "Epoch 104/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  63.973045349121094\n",
                        "\n",
                        "\n",
                        "Epoch 105/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  61.55635452270508\n",
                        "\n",
                        "\n",
                        "Epoch 106/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  59.40981674194336\n",
                        "\n",
                        "\n",
                        "Epoch 107/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  57.28125\n",
                        "\n",
                        "\n",
                        "Epoch 108/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  55.34792709350586\n",
                        "\n",
                        "\n",
                        "Epoch 109/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  53.02216720581055\n",
                        "\n",
                        "\n",
                        "Epoch 110/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  50.64396667480469\n",
                        "\n",
                        "\n",
                        "Epoch 111/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  48.477012634277344\n",
                        "\n",
                        "\n",
                        "Epoch 112/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  46.39378356933594\n",
                        "\n",
                        "\n",
                        "Epoch 113/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  44.34090042114258\n",
                        "\n",
                        "\n",
                        "Epoch 114/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  42.39400100708008\n",
                        "\n",
                        "\n",
                        "Epoch 115/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  40.5604133605957\n",
                        "\n",
                        "\n",
                        "Epoch 116/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  38.83218765258789\n",
                        "\n",
                        "\n",
                        "Epoch 117/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  36.9375114440918\n",
                        "\n",
                        "\n",
                        "Epoch 118/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  35.270023345947266\n",
                        "\n",
                        "\n",
                        "Epoch 119/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  33.917964935302734\n",
                        "\n",
                        "\n",
                        "Epoch 120/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  32.63155746459961\n",
                        "\n",
                        "\n",
                        "Epoch 121/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  31.298179626464844\n",
                        "\n",
                        "\n",
                        "Epoch 122/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  30.059459686279297\n",
                        "\n",
                        "\n",
                        "Epoch 123/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  29.08184051513672\n",
                        "\n",
                        "\n",
                        "Epoch 124/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  27.959423065185547\n",
                        "\n",
                        "\n",
                        "Epoch 125/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  26.790945053100586\n",
                        "\n",
                        "\n",
                        "Epoch 126/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  25.77528953552246\n",
                        "\n",
                        "\n",
                        "Epoch 127/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  24.90394401550293\n",
                        "\n",
                        "\n",
                        "Epoch 128/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  23.87500762939453\n",
                        "\n",
                        "\n",
                        "Epoch 129/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  23.06257438659668\n",
                        "\n",
                        "\n",
                        "Epoch 130/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  22.098722457885742\n",
                        "\n",
                        "\n",
                        "Epoch 131/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  21.494394302368164\n",
                        "\n",
                        "\n",
                        "Epoch 132/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.680543899536133\n",
                        "\n",
                        "\n",
                        "Epoch 133/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.046627044677734\n",
                        "\n",
                        "\n",
                        "Epoch 134/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.29991912841797\n",
                        "\n",
                        "\n",
                        "Epoch 135/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  18.694570541381836\n",
                        "\n",
                        "\n",
                        "Epoch 136/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  18.09722900390625\n",
                        "\n",
                        "\n",
                        "Epoch 137/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  17.521453857421875\n",
                        "\n",
                        "\n",
                        "Epoch 138/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  17.04787826538086\n",
                        "\n",
                        "\n",
                        "Epoch 139/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  16.450725555419922\n",
                        "\n",
                        "\n",
                        "Epoch 140/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  16.06682014465332\n",
                        "\n",
                        "\n",
                        "Epoch 141/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  15.53304672241211\n",
                        "\n",
                        "\n",
                        "Epoch 142/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  15.098504066467285\n",
                        "\n",
                        "\n",
                        "Epoch 143/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  14.62169361114502\n",
                        "\n",
                        "\n",
                        "Epoch 144/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  14.314742088317871\n",
                        "\n",
                        "\n",
                        "Epoch 145/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13.733671188354492\n",
                        "\n",
                        "\n",
                        "Epoch 146/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13.466445922851562\n",
                        "\n",
                        "\n",
                        "Epoch 147/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13.01117992401123\n",
                        "\n",
                        "\n",
                        "Epoch 148/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.621566772460938\n",
                        "\n",
                        "\n",
                        "Epoch 149/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.182453155517578\n",
                        "\n",
                        "\n",
                        "Epoch 150/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.84815502166748\n",
                        "\n",
                        "\n",
                        "Epoch 151/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.574763298034668\n",
                        "\n",
                        "\n",
                        "Epoch 152/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.323197364807129\n",
                        "\n",
                        "\n",
                        "Epoch 153/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.121490478515625\n",
                        "\n",
                        "\n",
                        "Epoch 154/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  10.85045337677002\n",
                        "\n",
                        "\n",
                        "Epoch 155/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  10.459471702575684\n",
                        "\n",
                        "\n",
                        "Epoch 156/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  10.18353271484375\n",
                        "\n",
                        "\n",
                        "Epoch 157/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  10.105879783630371\n",
                        "\n",
                        "\n",
                        "Epoch 158/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  9.997220993041992\n",
                        "\n",
                        "\n",
                        "Epoch 159/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  9.768829345703125\n",
                        "\n",
                        "\n",
                        "Epoch 160/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  9.626436233520508\n",
                        "\n",
                        "\n",
                        "Epoch 161/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  9.556456565856934\n",
                        "\n",
                        "\n",
                        "Epoch 162/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  9.28736686706543\n",
                        "\n",
                        "\n",
                        "Epoch 163/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  9.315144538879395\n",
                        "\n",
                        "\n",
                        "Epoch 164/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  9.155378341674805\n",
                        "\n",
                        "\n",
                        "Epoch 165/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  8.926590919494629\n",
                        "\n",
                        "\n",
                        "Epoch 166/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  8.85087776184082\n",
                        "\n",
                        "\n",
                        "Epoch 167/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  8.732579231262207\n",
                        "\n",
                        "\n",
                        "Epoch 168/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  8.559582710266113\n",
                        "\n",
                        "\n",
                        "Epoch 169/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  8.540186882019043\n",
                        "\n",
                        "\n",
                        "Epoch 170/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  8.356756210327148\n",
                        "\n",
                        "\n",
                        "Epoch 171/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  8.422890663146973\n",
                        "\n",
                        "\n",
                        "Epoch 172/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  8.173734664916992\n",
                        "\n",
                        "\n",
                        "Epoch 173/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  8.190263748168945\n",
                        "\n",
                        "\n",
                        "Epoch 174/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  8.015999794006348\n",
                        "\n",
                        "\n",
                        "Epoch 175/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.912113189697266\n",
                        "\n",
                        "\n",
                        "Epoch 176/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.855515003204346\n",
                        "\n",
                        "\n",
                        "Epoch 177/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.77915096282959\n",
                        "\n",
                        "\n",
                        "Epoch 178/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.842777729034424\n",
                        "\n",
                        "\n",
                        "Epoch 179/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.6574931144714355\n",
                        "\n",
                        "\n",
                        "Epoch 180/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.553092956542969\n",
                        "\n",
                        "\n",
                        "Epoch 181/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.476728439331055\n",
                        "\n",
                        "\n",
                        "Epoch 182/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.5341901779174805\n",
                        "\n",
                        "\n",
                        "Epoch 183/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.431490421295166\n",
                        "\n",
                        "\n",
                        "Epoch 184/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.389730930328369\n",
                        "\n",
                        "\n",
                        "Epoch 185/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.342950820922852\n",
                        "\n",
                        "\n",
                        "Epoch 186/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.291840553283691\n",
                        "\n",
                        "\n",
                        "Epoch 187/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.217498779296875\n",
                        "\n",
                        "\n",
                        "Epoch 188/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.161775588989258\n",
                        "\n",
                        "\n",
                        "Epoch 189/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.136302947998047\n",
                        "\n",
                        "\n",
                        "Epoch 190/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.107696056365967\n",
                        "\n",
                        "\n",
                        "Epoch 191/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.105424880981445\n",
                        "\n",
                        "\n",
                        "Epoch 192/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  7.018642425537109\n",
                        "\n",
                        "\n",
                        "Epoch 193/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  6.976466178894043\n",
                        "\n",
                        "\n",
                        "Epoch 194/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  6.96113920211792\n",
                        "\n",
                        "\n",
                        "Epoch 195/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  6.945553302764893\n",
                        "\n",
                        "\n",
                        "Epoch 196/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  6.874540328979492\n",
                        "\n",
                        "\n",
                        "Epoch 197/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  6.830918312072754\n",
                        "\n",
                        "\n",
                        "Epoch 198/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  6.808594226837158\n",
                        "\n",
                        "\n",
                        "Epoch 199/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  6.74805212020874\n",
                        "\n",
                        "\n",
                        "Epoch 200/200\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  6.733180046081543\n"
                    ]
                }
            ],
            "source": [
                "Epoch = 200\n",
                "i = 0\n",
                "lr = 5e-6\n",
                "lam = 5\n",
                "temp = 1000\n",
                "RHS = [Zeta, Eta, Delta]\n",
                "while(i<=Epoch):\n",
                "    print(\"\\n\")\n",
                "    print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
                "    print(\"Learning rate : \", lr)\n",
                "    xi_L, prevxi_L, lossitem= training_loop(xi_L,prevxi_L,RHS,Tau,Xdot,128,lr=lr,lam=lam)\n",
                "    temp = lossitem\n",
                "    i+=1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result stage 1:  -0.02*x0_t**2*sin(x0)**2 + 0.02*x0_t**2*sin(x1)**2 - 1.59*x0_t**2 - 1.53*x0_t*x1_t*cos(x0 - x1) - 0.02*x1_t**2*sin(x0 - x1) + 0.02*x1_t**2*cos(x0) - 0.79*x1_t**2 - 32.5*cos(x0) - 15.13*cos(x1)\n"
                    ]
                }
            ],
            "source": [
                "## Thresholding\n",
                "threshold = 1e-2\n",
                "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
                "expr = np.array(expr)[surv_index].tolist()\n",
                "\n",
                "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "\n",
                "## obtaining analytical model\n",
                "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
                "L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-3)\n",
                "print(\"Result stage 1: \", simplify(L))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)\n",
                "\n",
                "expr = np.array(expr)\n",
                "i1 = np.where(expr == 'x0_t**2')[0]\n",
                "i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "\n",
                "nonpenaltyidx = [i1,i4,i5,i6]\n",
                "\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "Epoch 0/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  6.319198131561279\n",
                        "\n",
                        "\n",
                        "Epoch 1/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  6.027316093444824\n",
                        "\n",
                        "\n",
                        "Epoch 2/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.968632221221924\n",
                        "\n",
                        "\n",
                        "Epoch 3/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.9244279861450195\n",
                        "\n",
                        "\n",
                        "Epoch 4/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.881994247436523\n",
                        "\n",
                        "\n",
                        "Epoch 5/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.845252513885498\n",
                        "\n",
                        "\n",
                        "Epoch 6/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.811527729034424\n",
                        "\n",
                        "\n",
                        "Epoch 7/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.7796244621276855\n",
                        "\n",
                        "\n",
                        "Epoch 8/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.74824857711792\n",
                        "\n",
                        "\n",
                        "Epoch 9/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.720623016357422\n",
                        "\n",
                        "\n",
                        "Epoch 10/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.693278789520264\n",
                        "\n",
                        "\n",
                        "Epoch 11/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.669127464294434\n",
                        "\n",
                        "\n",
                        "Epoch 12/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.643570899963379\n",
                        "\n",
                        "\n",
                        "Epoch 13/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.621496200561523\n",
                        "\n",
                        "\n",
                        "Epoch 14/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.600419521331787\n",
                        "\n",
                        "\n",
                        "Epoch 15/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.5826311111450195\n",
                        "\n",
                        "\n",
                        "Epoch 16/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.562000274658203\n",
                        "\n",
                        "\n",
                        "Epoch 17/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.5479583740234375\n",
                        "\n",
                        "\n",
                        "Epoch 18/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.534126281738281\n",
                        "\n",
                        "\n",
                        "Epoch 19/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.514283657073975\n",
                        "\n",
                        "\n",
                        "Epoch 20/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.506475925445557\n",
                        "\n",
                        "\n",
                        "Epoch 21/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.488492012023926\n",
                        "\n",
                        "\n",
                        "Epoch 22/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.475587368011475\n",
                        "\n",
                        "\n",
                        "Epoch 23/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.463096618652344\n",
                        "\n",
                        "\n",
                        "Epoch 24/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.4489970207214355\n",
                        "\n",
                        "\n",
                        "Epoch 25/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.42846155166626\n",
                        "\n",
                        "\n",
                        "Epoch 26/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.4261274337768555\n",
                        "\n",
                        "\n",
                        "Epoch 27/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.414324760437012\n",
                        "\n",
                        "\n",
                        "Epoch 28/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.397810935974121\n",
                        "\n",
                        "\n",
                        "Epoch 29/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.386476993560791\n",
                        "\n",
                        "\n",
                        "Epoch 30/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.375405788421631\n",
                        "\n",
                        "\n",
                        "Epoch 31/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.367944240570068\n",
                        "\n",
                        "\n",
                        "Epoch 32/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.35463285446167\n",
                        "\n",
                        "\n",
                        "Epoch 33/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.344376087188721\n",
                        "\n",
                        "\n",
                        "Epoch 34/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.3353753089904785\n",
                        "\n",
                        "\n",
                        "Epoch 35/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.321838855743408\n",
                        "\n",
                        "\n",
                        "Epoch 36/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.311617851257324\n",
                        "\n",
                        "\n",
                        "Epoch 37/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.300833702087402\n",
                        "\n",
                        "\n",
                        "Epoch 38/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.290543556213379\n",
                        "\n",
                        "\n",
                        "Epoch 39/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.280468940734863\n",
                        "\n",
                        "\n",
                        "Epoch 40/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.270712375640869\n",
                        "\n",
                        "\n",
                        "Epoch 41/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.2604498863220215\n",
                        "\n",
                        "\n",
                        "Epoch 42/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.2506561279296875\n",
                        "\n",
                        "\n",
                        "Epoch 43/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.240865230560303\n",
                        "\n",
                        "\n",
                        "Epoch 44/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.231191635131836\n",
                        "\n",
                        "\n",
                        "Epoch 45/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.222682952880859\n",
                        "\n",
                        "\n",
                        "Epoch 46/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.212248802185059\n",
                        "\n",
                        "\n",
                        "Epoch 47/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.204758644104004\n",
                        "\n",
                        "\n",
                        "Epoch 48/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.195501804351807\n",
                        "\n",
                        "\n",
                        "Epoch 49/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.183520317077637\n",
                        "\n",
                        "\n",
                        "Epoch 50/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.174118518829346\n",
                        "\n",
                        "\n",
                        "Epoch 51/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.164813041687012\n",
                        "\n",
                        "\n",
                        "Epoch 52/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.155749797821045\n",
                        "\n",
                        "\n",
                        "Epoch 53/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.146747589111328\n",
                        "\n",
                        "\n",
                        "Epoch 54/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.138613224029541\n",
                        "\n",
                        "\n",
                        "Epoch 55/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.131641387939453\n",
                        "\n",
                        "\n",
                        "Epoch 56/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.11903190612793\n",
                        "\n",
                        "\n",
                        "Epoch 57/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.110028266906738\n",
                        "\n",
                        "\n",
                        "Epoch 58/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.101330757141113\n",
                        "\n",
                        "\n",
                        "Epoch 59/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.0931782722473145\n",
                        "\n",
                        "\n",
                        "Epoch 60/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.083011150360107\n",
                        "\n",
                        "\n",
                        "Epoch 61/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.075551986694336\n",
                        "\n",
                        "\n",
                        "Epoch 62/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.065700531005859\n",
                        "\n",
                        "\n",
                        "Epoch 63/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.055927753448486\n",
                        "\n",
                        "\n",
                        "Epoch 64/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.048182487487793\n",
                        "\n",
                        "\n",
                        "Epoch 65/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.039056777954102\n",
                        "\n",
                        "\n",
                        "Epoch 66/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.028957843780518\n",
                        "\n",
                        "\n",
                        "Epoch 67/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.021141529083252\n",
                        "\n",
                        "\n",
                        "Epoch 68/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.0134100914001465\n",
                        "\n",
                        "\n",
                        "Epoch 69/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  5.002547740936279\n",
                        "\n",
                        "\n",
                        "Epoch 70/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.9940900802612305\n",
                        "\n",
                        "\n",
                        "Epoch 71/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.984963893890381\n",
                        "\n",
                        "\n",
                        "Epoch 72/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.97506856918335\n",
                        "\n",
                        "\n",
                        "Epoch 73/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.968777656555176\n",
                        "\n",
                        "\n",
                        "Epoch 74/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.959866523742676\n",
                        "\n",
                        "\n",
                        "Epoch 75/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.949735164642334\n",
                        "\n",
                        "\n",
                        "Epoch 76/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.940754413604736\n",
                        "\n",
                        "\n",
                        "Epoch 77/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.931643009185791\n",
                        "\n",
                        "\n",
                        "Epoch 78/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.921928405761719\n",
                        "\n",
                        "\n",
                        "Epoch 79/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.916098594665527\n",
                        "\n",
                        "\n",
                        "Epoch 80/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.904918670654297\n",
                        "\n",
                        "\n",
                        "Epoch 81/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.895806789398193\n",
                        "\n",
                        "\n",
                        "Epoch 82/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.886772155761719\n",
                        "\n",
                        "\n",
                        "Epoch 83/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.878286361694336\n",
                        "\n",
                        "\n",
                        "Epoch 84/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.869897365570068\n",
                        "\n",
                        "\n",
                        "Epoch 85/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.861091613769531\n",
                        "\n",
                        "\n",
                        "Epoch 86/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.852658271789551\n",
                        "\n",
                        "\n",
                        "Epoch 87/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.843677997589111\n",
                        "\n",
                        "\n",
                        "Epoch 88/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.834887504577637\n",
                        "\n",
                        "\n",
                        "Epoch 89/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.825583457946777\n",
                        "\n",
                        "\n",
                        "Epoch 90/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.816617488861084\n",
                        "\n",
                        "\n",
                        "Epoch 91/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.807161808013916\n",
                        "\n",
                        "\n",
                        "Epoch 92/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.799531936645508\n",
                        "\n",
                        "\n",
                        "Epoch 93/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.789921283721924\n",
                        "\n",
                        "\n",
                        "Epoch 94/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.78124475479126\n",
                        "\n",
                        "\n",
                        "Epoch 95/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.7738213539123535\n",
                        "\n",
                        "\n",
                        "Epoch 96/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.7652788162231445\n",
                        "\n",
                        "\n",
                        "Epoch 97/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.754064559936523\n",
                        "\n",
                        "\n",
                        "Epoch 98/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.745355606079102\n",
                        "\n",
                        "\n",
                        "Epoch 99/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.739392280578613\n",
                        "\n",
                        "\n",
                        "Epoch 100/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  4.728128910064697\n"
                    ]
                }
            ],
            "source": [
                "Epoch = 100\n",
                "i = 0\n",
                "lr = 1e-5\n",
                "lam = 1\n",
                "temp = 1000\n",
                "RHS = [Zeta, Eta, Delta]\n",
                "while(i<=Epoch):\n",
                "    print(\"\\n\")\n",
                "    print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
                "    print(\"Learning rate : \", lr)\n",
                "    xi_L, prevxi_L, lossitem= training_loop(xi_L,prevxi_L,RHS,Tau,Xdot,128,lr=lr,lam=lam)\n",
                "    temp = lossitem\n",
                "    i+=1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result stage 1:  -1.39*x0_t**2 - 1.35*x0_t*x1_t*sin(x0)*sin(x1) - 1.37*x0_t*x1_t*cos(x0)*cos(x1) - 0.68*x1_t**2 - 27.58*cos(x0) - 13.94*cos(x1)\n"
                    ]
                }
            ],
            "source": [
                "## Thresholding\n",
                "threshold = 1e-1\n",
                "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
                "expr = np.array(expr)[surv_index].tolist()\n",
                "\n",
                "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "\n",
                "## obtaining analytical model\n",
                "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
                "L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-3)\n",
                "print(\"Result stage 1: \", simplify(L))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "ename": "IndexError",
                    "evalue": "index 0 is out of bounds for axis 0 with size 0",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-49-3b0761aed0df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mexpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mi1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'x0_t**2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mi4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'x1_t**2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mi5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cos(x0)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mi6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cos(x1)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
                    ]
                }
            ],
            "source": [
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)\n",
                "\n",
                "expr = np.array(expr)\n",
                "i1 = np.where(expr == 'x0_t**2')[0]\n",
                "i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "\n",
                "nonpenaltyidx = [i1,i4,i5,i6]\n",
                "\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "Epoch 0/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.9937503337860107\n",
                        "\n",
                        "\n",
                        "Epoch 1/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.9770687818527222\n",
                        "\n",
                        "\n",
                        "Epoch 2/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.9651819467544556\n",
                        "\n",
                        "\n",
                        "Epoch 3/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.953314185142517\n",
                        "\n",
                        "\n",
                        "Epoch 4/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.9413597583770752\n",
                        "\n",
                        "\n",
                        "Epoch 5/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.9294625520706177\n",
                        "\n",
                        "\n",
                        "Epoch 6/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.9175994396209717\n",
                        "\n",
                        "\n",
                        "Epoch 7/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.9057775735855103\n",
                        "\n",
                        "\n",
                        "Epoch 8/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.8940163850784302\n",
                        "\n",
                        "\n",
                        "Epoch 9/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.8822723627090454\n",
                        "\n",
                        "\n",
                        "Epoch 10/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.87063467502594\n",
                        "\n",
                        "\n",
                        "Epoch 11/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.8590724468231201\n",
                        "\n",
                        "\n",
                        "Epoch 12/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.8475980758666992\n",
                        "\n",
                        "\n",
                        "Epoch 13/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.836107611656189\n",
                        "\n",
                        "\n",
                        "Epoch 14/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.8246649503707886\n",
                        "\n",
                        "\n",
                        "Epoch 15/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.8133491277694702\n",
                        "\n",
                        "\n",
                        "Epoch 16/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.8021280765533447\n",
                        "\n",
                        "\n",
                        "Epoch 17/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.790998101234436\n",
                        "\n",
                        "\n",
                        "Epoch 18/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.7799601554870605\n",
                        "\n",
                        "\n",
                        "Epoch 19/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.768990397453308\n",
                        "\n",
                        "\n",
                        "Epoch 20/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.7580536603927612\n",
                        "\n",
                        "\n",
                        "Epoch 21/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.7471603155136108\n",
                        "\n",
                        "\n",
                        "Epoch 22/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.7363227605819702\n",
                        "\n",
                        "\n",
                        "Epoch 23/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.7255358695983887\n",
                        "\n",
                        "\n",
                        "Epoch 24/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.7148634195327759\n",
                        "\n",
                        "\n",
                        "Epoch 25/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.704281210899353\n",
                        "\n",
                        "\n",
                        "Epoch 26/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.6937657594680786\n",
                        "\n",
                        "\n",
                        "Epoch 27/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.6832871437072754\n",
                        "\n",
                        "\n",
                        "Epoch 28/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.6728637218475342\n",
                        "\n",
                        "\n",
                        "Epoch 29/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.6624951362609863\n",
                        "\n",
                        "\n",
                        "Epoch 30/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.6521869897842407\n",
                        "\n",
                        "\n",
                        "Epoch 31/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.6419732570648193\n",
                        "\n",
                        "\n",
                        "Epoch 32/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.6318237781524658\n",
                        "\n",
                        "\n",
                        "Epoch 33/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.6217652559280396\n",
                        "\n",
                        "\n",
                        "Epoch 34/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.6117479801177979\n",
                        "\n",
                        "\n",
                        "Epoch 35/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.601794719696045\n",
                        "\n",
                        "\n",
                        "Epoch 36/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.5919255018234253\n",
                        "\n",
                        "\n",
                        "Epoch 37/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.5820993185043335\n",
                        "\n",
                        "\n",
                        "Epoch 38/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.5723185539245605\n",
                        "\n",
                        "\n",
                        "Epoch 39/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.5625864267349243\n",
                        "\n",
                        "\n",
                        "Epoch 40/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.5528795719146729\n",
                        "\n",
                        "\n",
                        "Epoch 41/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.5432274341583252\n",
                        "\n",
                        "\n",
                        "Epoch 42/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.5336663722991943\n",
                        "\n",
                        "\n",
                        "Epoch 43/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.5241303443908691\n",
                        "\n",
                        "\n",
                        "Epoch 44/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.5146480798721313\n",
                        "\n",
                        "\n",
                        "Epoch 45/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.5052298307418823\n",
                        "\n",
                        "\n",
                        "Epoch 46/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.495875597000122\n",
                        "\n",
                        "\n",
                        "Epoch 47/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.4865736961364746\n",
                        "\n",
                        "\n",
                        "Epoch 48/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.4772933721542358\n",
                        "\n",
                        "\n",
                        "Epoch 49/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.4681096076965332\n",
                        "\n",
                        "\n",
                        "Epoch 50/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.4589687585830688\n",
                        "\n",
                        "\n",
                        "Epoch 51/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.4498754739761353\n",
                        "\n",
                        "\n",
                        "Epoch 52/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.4408574104309082\n",
                        "\n",
                        "\n",
                        "Epoch 53/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.4318894147872925\n",
                        "\n",
                        "\n",
                        "Epoch 54/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.4229880571365356\n",
                        "\n",
                        "\n",
                        "Epoch 55/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.4141196012496948\n",
                        "\n",
                        "\n",
                        "Epoch 56/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.405294418334961\n",
                        "\n",
                        "\n",
                        "Epoch 57/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.3965239524841309\n",
                        "\n",
                        "\n",
                        "Epoch 58/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.387823462486267\n",
                        "\n",
                        "\n",
                        "Epoch 59/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.379172444343567\n",
                        "\n",
                        "\n",
                        "Epoch 60/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.370566964149475\n",
                        "\n",
                        "\n",
                        "Epoch 61/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.3619900941848755\n",
                        "\n",
                        "\n",
                        "Epoch 62/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.3534505367279053\n",
                        "\n",
                        "\n",
                        "Epoch 63/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.344963788986206\n",
                        "\n",
                        "\n",
                        "Epoch 64/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.336549997329712\n",
                        "\n",
                        "\n",
                        "Epoch 65/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.3281804323196411\n",
                        "\n",
                        "\n",
                        "Epoch 66/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.3198455572128296\n",
                        "\n",
                        "\n",
                        "Epoch 67/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.3115313053131104\n",
                        "\n",
                        "\n",
                        "Epoch 68/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.3032723665237427\n",
                        "\n",
                        "\n",
                        "Epoch 69/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.2950618267059326\n",
                        "\n",
                        "\n",
                        "Epoch 70/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.2869292497634888\n",
                        "\n",
                        "\n",
                        "Epoch 71/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.2788499593734741\n",
                        "\n",
                        "\n",
                        "Epoch 72/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.2708138227462769\n",
                        "\n",
                        "\n",
                        "Epoch 73/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.2628177404403687\n",
                        "\n",
                        "\n",
                        "Epoch 74/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.2548539638519287\n",
                        "\n",
                        "\n",
                        "Epoch 75/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.2469508647918701\n",
                        "\n",
                        "\n",
                        "Epoch 76/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.2391408681869507\n",
                        "\n",
                        "\n",
                        "Epoch 77/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.2313765287399292\n",
                        "\n",
                        "\n",
                        "Epoch 78/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.2236379384994507\n",
                        "\n",
                        "\n",
                        "Epoch 79/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.2159614562988281\n",
                        "\n",
                        "\n",
                        "Epoch 80/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.2083544731140137\n",
                        "\n",
                        "\n",
                        "Epoch 81/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.2007975578308105\n",
                        "\n",
                        "\n",
                        "Epoch 82/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.1932722330093384\n",
                        "\n",
                        "\n",
                        "Epoch 83/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.1857753992080688\n",
                        "\n",
                        "\n",
                        "Epoch 84/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.1783596277236938\n",
                        "\n",
                        "\n",
                        "Epoch 85/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.1709712743759155\n",
                        "\n",
                        "\n",
                        "Epoch 86/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.1636319160461426\n",
                        "\n",
                        "\n",
                        "Epoch 87/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.156343936920166\n",
                        "\n",
                        "\n",
                        "Epoch 88/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.1490992307662964\n",
                        "\n",
                        "\n",
                        "Epoch 89/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.141897201538086\n",
                        "\n",
                        "\n",
                        "Epoch 90/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.1347471475601196\n",
                        "\n",
                        "\n",
                        "Epoch 91/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.1276201009750366\n",
                        "\n",
                        "\n",
                        "Epoch 92/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.120500087738037\n",
                        "\n",
                        "\n",
                        "Epoch 93/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.1134172677993774\n",
                        "\n",
                        "\n",
                        "Epoch 94/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.1063915491104126\n",
                        "\n",
                        "\n",
                        "Epoch 95/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.099390983581543\n",
                        "\n",
                        "\n",
                        "Epoch 96/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.092444896697998\n",
                        "\n",
                        "\n",
                        "Epoch 97/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.085548996925354\n",
                        "\n",
                        "\n",
                        "Epoch 98/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.0786821842193604\n",
                        "\n",
                        "\n",
                        "Epoch 99/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.071860432624817\n",
                        "\n",
                        "\n",
                        "Epoch 100/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.0650843381881714\n",
                        "\n",
                        "\n",
                        "Epoch 101/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.0583523511886597\n",
                        "\n",
                        "\n",
                        "Epoch 102/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.0516613721847534\n",
                        "\n",
                        "\n",
                        "Epoch 103/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.0450093746185303\n",
                        "\n",
                        "\n",
                        "Epoch 104/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.0383777618408203\n",
                        "\n",
                        "\n",
                        "Epoch 105/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.0317914485931396\n",
                        "\n",
                        "\n",
                        "Epoch 106/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.0252506732940674\n",
                        "\n",
                        "\n",
                        "Epoch 107/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.0187426805496216\n",
                        "\n",
                        "\n",
                        "Epoch 108/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.0122716426849365\n",
                        "\n",
                        "\n",
                        "Epoch 109/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  1.005842924118042\n",
                        "\n",
                        "\n",
                        "Epoch 110/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9994487762451172\n",
                        "\n",
                        "\n",
                        "Epoch 111/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9930886030197144\n",
                        "\n",
                        "\n",
                        "Epoch 112/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9867891073226929\n",
                        "\n",
                        "\n",
                        "Epoch 113/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9805254936218262\n",
                        "\n",
                        "\n",
                        "Epoch 114/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9742943644523621\n",
                        "\n",
                        "\n",
                        "Epoch 115/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9680919051170349\n",
                        "\n",
                        "\n",
                        "Epoch 116/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9619335532188416\n",
                        "\n",
                        "\n",
                        "Epoch 117/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9558233022689819\n",
                        "\n",
                        "\n",
                        "Epoch 118/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9497403502464294\n",
                        "\n",
                        "\n",
                        "Epoch 119/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9436955451965332\n",
                        "\n",
                        "\n",
                        "Epoch 120/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9376860857009888\n",
                        "\n",
                        "\n",
                        "Epoch 121/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9317083954811096\n",
                        "\n",
                        "\n",
                        "Epoch 122/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.925765335559845\n",
                        "\n",
                        "\n",
                        "Epoch 123/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9198625683784485\n",
                        "\n",
                        "\n",
                        "Epoch 124/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9139941930770874\n",
                        "\n",
                        "\n",
                        "Epoch 125/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9081685543060303\n",
                        "\n",
                        "\n",
                        "Epoch 126/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.9023779630661011\n",
                        "\n",
                        "\n",
                        "Epoch 127/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8966185450553894\n",
                        "\n",
                        "\n",
                        "Epoch 128/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8908936977386475\n",
                        "\n",
                        "\n",
                        "Epoch 129/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8851956725120544\n",
                        "\n",
                        "\n",
                        "Epoch 130/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8795363903045654\n",
                        "\n",
                        "\n",
                        "Epoch 131/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8739118576049805\n",
                        "\n",
                        "\n",
                        "Epoch 132/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8683201670646667\n",
                        "\n",
                        "\n",
                        "Epoch 133/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8627572655677795\n",
                        "\n",
                        "\n",
                        "Epoch 134/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8572329878807068\n",
                        "\n",
                        "\n",
                        "Epoch 135/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.851744532585144\n",
                        "\n",
                        "\n",
                        "Epoch 136/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8462867140769958\n",
                        "\n",
                        "\n",
                        "Epoch 137/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8408629298210144\n",
                        "\n",
                        "\n",
                        "Epoch 138/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8354734182357788\n",
                        "\n",
                        "\n",
                        "Epoch 139/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8301044702529907\n",
                        "\n",
                        "\n",
                        "Epoch 140/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8246479034423828\n",
                        "\n",
                        "\n",
                        "Epoch 141/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8191872835159302\n",
                        "\n",
                        "\n",
                        "Epoch 142/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8136862516403198\n",
                        "\n",
                        "\n",
                        "Epoch 143/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8082398772239685\n",
                        "\n",
                        "\n",
                        "Epoch 144/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.8028913736343384\n",
                        "\n",
                        "\n",
                        "Epoch 145/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7976459264755249\n",
                        "\n",
                        "\n",
                        "Epoch 146/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7926317453384399\n",
                        "\n",
                        "\n",
                        "Epoch 147/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7875166535377502\n",
                        "\n",
                        "\n",
                        "Epoch 148/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7820901870727539\n",
                        "\n",
                        "\n",
                        "Epoch 149/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7764450311660767\n",
                        "\n",
                        "\n",
                        "Epoch 150/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7720857858657837\n",
                        "\n",
                        "\n",
                        "Epoch 151/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7681368589401245\n",
                        "\n",
                        "\n",
                        "Epoch 152/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7641469240188599\n",
                        "\n",
                        "\n",
                        "Epoch 153/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7599934935569763\n",
                        "\n",
                        "\n",
                        "Epoch 154/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7575411796569824\n",
                        "\n",
                        "\n",
                        "Epoch 155/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7543836832046509\n",
                        "\n",
                        "\n",
                        "Epoch 156/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.750862181186676\n",
                        "\n",
                        "\n",
                        "Epoch 157/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7464492917060852\n",
                        "\n",
                        "\n",
                        "Epoch 158/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7428708076477051\n",
                        "\n",
                        "\n",
                        "Epoch 159/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7393106818199158\n",
                        "\n",
                        "\n",
                        "Epoch 160/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7357320785522461\n",
                        "\n",
                        "\n",
                        "Epoch 161/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7321682572364807\n",
                        "\n",
                        "\n",
                        "Epoch 162/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7286246418952942\n",
                        "\n",
                        "\n",
                        "Epoch 163/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7251020669937134\n",
                        "\n",
                        "\n",
                        "Epoch 164/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7215976119041443\n",
                        "\n",
                        "\n",
                        "Epoch 165/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7181096076965332\n",
                        "\n",
                        "\n",
                        "Epoch 166/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7146416902542114\n",
                        "\n",
                        "\n",
                        "Epoch 167/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7111889719963074\n",
                        "\n",
                        "\n",
                        "Epoch 168/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7077534794807434\n",
                        "\n",
                        "\n",
                        "Epoch 169/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.7043384909629822\n",
                        "\n",
                        "\n",
                        "Epoch 170/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.700940728187561\n",
                        "\n",
                        "\n",
                        "Epoch 171/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.697559118270874\n",
                        "\n",
                        "\n",
                        "Epoch 172/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6941943764686584\n",
                        "\n",
                        "\n",
                        "Epoch 173/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.690843939781189\n",
                        "\n",
                        "\n",
                        "Epoch 174/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6875168085098267\n",
                        "\n",
                        "\n",
                        "Epoch 175/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6842114329338074\n",
                        "\n",
                        "\n",
                        "Epoch 176/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6809183955192566\n",
                        "\n",
                        "\n",
                        "Epoch 177/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.677647054195404\n",
                        "\n",
                        "\n",
                        "Epoch 178/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6743900179862976\n",
                        "\n",
                        "\n",
                        "Epoch 179/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6711472868919373\n",
                        "\n",
                        "\n",
                        "Epoch 180/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.667924165725708\n",
                        "\n",
                        "\n",
                        "Epoch 181/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6647161245346069\n",
                        "\n",
                        "\n",
                        "Epoch 182/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6615290641784668\n",
                        "\n",
                        "\n",
                        "Epoch 183/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6583563685417175\n",
                        "\n",
                        "\n",
                        "Epoch 184/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.65520179271698\n",
                        "\n",
                        "\n",
                        "Epoch 185/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.652065098285675\n",
                        "\n",
                        "\n",
                        "Epoch 186/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6489408016204834\n",
                        "\n",
                        "\n",
                        "Epoch 187/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6458325982093811\n",
                        "\n",
                        "\n",
                        "Epoch 188/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6427431702613831\n",
                        "\n",
                        "\n",
                        "Epoch 189/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.63967365026474\n",
                        "\n",
                        "\n",
                        "Epoch 190/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6366192102432251\n",
                        "\n",
                        "\n",
                        "Epoch 191/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6335788369178772\n",
                        "\n",
                        "\n",
                        "Epoch 192/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6305543184280396\n",
                        "\n",
                        "\n",
                        "Epoch 193/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6275411248207092\n",
                        "\n",
                        "\n",
                        "Epoch 194/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6245429515838623\n",
                        "\n",
                        "\n",
                        "Epoch 195/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6215510964393616\n",
                        "\n",
                        "\n",
                        "Epoch 196/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6185734272003174\n",
                        "\n",
                        "\n",
                        "Epoch 197/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6156149506568909\n",
                        "\n",
                        "\n",
                        "Epoch 198/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6126720905303955\n",
                        "\n",
                        "\n",
                        "Epoch 199/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.6097452044487\n",
                        "\n",
                        "\n",
                        "Epoch 200/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  0.606839120388031\n"
                    ]
                }
            ],
            "source": [
                "Epoch = 200\n",
                "i = 0\n",
                "lr = 5e-5\n",
                "lam = 0.1\n",
                "temp = 1000\n",
                "RHS = [Zeta, Eta, Delta]\n",
                "while(i<=Epoch):\n",
                "    print(\"\\n\")\n",
                "    print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
                "    print(\"Learning rate : \", lr)\n",
                "    xi_L, prevxi_L, lossitem= training_loop(xi_L,prevxi_L,RHS,Tau,Xdot,128,lr=lr,lam=lam)\n",
                "    temp = lossitem\n",
                "    i+=1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result stage 1:  0.13*x0_t**2 + 0.13*x0_t*x1_t*cos(x0 - x1) + 2.3*cos(x0) + 1.09*cos(x1)\n"
                    ]
                }
            ],
            "source": [
                "## Thresholding\n",
                "threshold = 1e-1\n",
                "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
                "expr = np.array(expr)[surv_index].tolist()\n",
                "\n",
                "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "\n",
                "## obtaining analytical model\n",
                "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
                "L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-3)\n",
                "print(\"Result stage 1: \", simplify(L))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Next round selection ##\n",
                "for stage in range(4):\n",
                "\n",
                "    #Redefine computation after thresholding\n",
                "    expr.append(known_expr[0])\n",
                "    Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)\n",
                "\n",
                "    expr = np.array(expr)\n",
                "    i1 = np.where(expr == 'x0_t**2')[0]\n",
                "    i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "    i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "    i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "    idx = np.arange(0,len(expr))\n",
                "    idx = np.delete(idx,i1)\n",
                "    known_expr = expr[i1].tolist()\n",
                "    expr = np.delete(expr,i1).tolist()\n",
                "    nonpenaltyidx = [i4,i5,i6]\n",
                "\n",
                "    Zeta_ = Zeta[:,:,i1,:].clone().detach()\n",
                "    Eta_ = Eta[:,:,i1,:].clone().detach()\n",
                "    Delta_ = Delta[:,i1,:].clone().detach()\n",
                "\n",
                "    Zeta = Zeta[:,:,idx,:]\n",
                "    Eta = Eta[:,:,idx,:]\n",
                "    Delta = Delta[:,idx,:]\n",
                "\n",
                "    Zeta = Zeta.to(device)\n",
                "    Eta = Eta.to(device)\n",
                "    Delta = Delta.to(device)\n",
                "    Zeta_ = Zeta_.to(device)\n",
                "    Eta_ = Eta_.to(device)\n",
                "    Delta_ = Delta_.to(device)\n",
                "\n",
                "    Epoch = 100\n",
                "    i = 0\n",
                "    lr += 2e-6\n",
                "    if(stage==3):\n",
                "        lam = 0\n",
                "    else:\n",
                "        lam = 0.1\n",
                "    temp = 1000\n",
                "    RHS = [Zeta, Eta, Delta]\n",
                "    LHS = [Zeta_, Eta_, Delta_]\n",
                "    while(i<=Epoch):\n",
                "        print(\"\\n\")\n",
                "        print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
                "        print(\"Learning rate : \", lr)\n",
                "        xi_L, prevxi_L, lossitem= training_loop(c, xi_L,prevxi_L,RHS,LHS,Xdot,128,lr=lr,lam=lam)\n",
                "        i+=1\n",
                "        if(temp <= 1e-3):\n",
                "            break\n",
                "    \n",
                "    ## Thresholding\n",
                "    threshold = 1e-1\n",
                "    surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
                "    expr = np.array(expr)[surv_index].tolist()\n",
                "\n",
                "    xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
                "    prevxi_L = xi_L.clone().detach()\n",
                "\n",
                "    ## obtaining analytical model\n",
                "    xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=3)\n",
                "    L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-1)\n",
                "    print(\"Result stage \" + str(stage+2) + \":\" , simplify(L))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Adding known terms\n",
                "L = str(simplify(L)) + \" + \" + known_expr[0]\n",
                "print(L)\n",
                "\n",
                "expr = expr + known_expr\n",
                "xi_L = torch.cat((xi_L, c))\n",
                "mask = torch.ones(len(xi_L),device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if(save==True):\n",
                "    #Saving Equation in string\n",
                "    text_file = open(rootdir + \"lagrangian_\" + str(noiselevel)+ \"_noise.txt\", \"w\")\n",
                "    text_file.write(L)\n",
                "    text_file.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "## Computing Jacobian in the case of stiff-ODE\n",
                "states_dot_sym = sympy.Matrix([states_dot_sym[0], states_dot_sym[1], eom[0], eom[1]])\n",
                "Jac = states_dot_sym.jacobian(states_sym)\n",
                "\n",
                "## Please copy the string shown to the definition of equation in the function of Jacobian\n",
                "for i in range(Jac.shape[0]):\n",
                "    for j in range(Jac.shape[1]):\n",
                "        print('Equation ' + str(i) + ',' + str(j) + ': ' + str(Jac[i,j]))\n",
                "        print('\\n')\n",
                "\n",
                "def Jac(t, y):\n",
                "    from numpy import cos, sin, sign\n",
                "    x0, x1, x0_t, x1_t = y\n",
                "    tau0, tau1 = 0, 0 \n",
                "    J = np.zeros((4,4))\n",
                "    J[0,0] = 0\n",
                "    J[0,1] = 0\n",
                "    J[0,2] = 1\n",
                "    J[0,3] = 0\n",
                "\n",
                "    J[1,0] = 0\n",
                "    J[1,1] = 0\n",
                "    J[1,2] = 0\n",
                "    J[1,3] = 1\n",
                "\n",
                "    J[2,0] = 1.0*(2.0*sin(x0)*sin(x1) + 2.0*cos(x0)*cos(x1))*(x0_t**2*sin(x0)*sin(x1) + x0_t**2*cos(x0)*cos(x1))/(2.0*sin(x0)**2*sin(x1)**2 + 4.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 2.0*cos(x0)**2*cos(x1)**2 - 4.0) + 0.0625*(2.0*sin(x0)*sin(x1) + 2.0*cos(x0)*cos(x1))*(4.0*sin(x0)**2*sin(x1)*cos(x1) - 4.0*sin(x0)*sin(x1)**2*cos(x0) + 4.0*sin(x0)*cos(x0)*cos(x1)**2 - 4.0*sin(x1)*cos(x0)**2*cos(x1))*(tau1 + x0_t**2*sin(x0)*cos(x1) - x0_t**2*sin(x1)*cos(x0) + 0.5*x0_t - 0.5*x1_t - 9.8*sin(x1))/(0.5*sin(x0)**2*sin(x1)**2 + sin(x0)*sin(x1)*cos(x0)*cos(x1) + 0.5*cos(x0)**2*cos(x1)**2 - 1)**2 + 1.0*(-2.0*sin(x0)*cos(x1) + 2.0*sin(x1)*cos(x0))*(tau1 + x0_t**2*sin(x0)*cos(x1) - x0_t**2*sin(x1)*cos(x0) + 0.5*x0_t - 0.5*x1_t - 9.8*sin(x1))/(2.0*sin(x0)**2*sin(x1)**2 + 4.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 2.0*cos(x0)**2*cos(x1)**2 - 4.0) - 2.0*(-x1_t**2*sin(x0)*sin(x1) - x1_t**2*cos(x0)*cos(x1) - 19.6*cos(x0))/(2.0*sin(x0)**2*sin(x1)**2 + 4.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 2.0*cos(x0)**2*cos(x1)**2 - 4.0) - 0.125*(4.0*sin(x0)**2*sin(x1)*cos(x1) - 4.0*sin(x0)*sin(x1)**2*cos(x0) + 4.0*sin(x0)*cos(x0)*cos(x1)**2 - 4.0*sin(x1)*cos(x0)**2*cos(x1))*(tau0 - tau1 - 1.0*x0_t + 0.5*x1_t - x1_t**2*sin(x0)*cos(x1) + x1_t**2*sin(x1)*cos(x0) - 19.6*sin(x0))/(0.5*sin(x0)**2*sin(x1)**2 + sin(x0)*sin(x1)*cos(x0)*cos(x1) + 0.5*cos(x0)**2*cos(x1)**2 - 1)**2\n",
                "    J[2,1] = 1.0*(2.0*sin(x0)*sin(x1) + 2.0*cos(x0)*cos(x1))*(-x0_t**2*sin(x0)*sin(x1) - x0_t**2*cos(x0)*cos(x1) - 9.8*cos(x1))/(2.0*sin(x0)**2*sin(x1)**2 + 4.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 2.0*cos(x0)**2*cos(x1)**2 - 4.0) + 0.0625*(2.0*sin(x0)*sin(x1) + 2.0*cos(x0)*cos(x1))*(-4.0*sin(x0)**2*sin(x1)*cos(x1) + 4.0*sin(x0)*sin(x1)**2*cos(x0) - 4.0*sin(x0)*cos(x0)*cos(x1)**2 + 4.0*sin(x1)*cos(x0)**2*cos(x1))*(tau1 + x0_t**2*sin(x0)*cos(x1) - x0_t**2*sin(x1)*cos(x0) + 0.5*x0_t - 0.5*x1_t - 9.8*sin(x1))/(0.5*sin(x0)**2*sin(x1)**2 + sin(x0)*sin(x1)*cos(x0)*cos(x1) + 0.5*cos(x0)**2*cos(x1)**2 - 1)**2 + 1.0*(2.0*sin(x0)*cos(x1) - 2.0*sin(x1)*cos(x0))*(tau1 + x0_t**2*sin(x0)*cos(x1) - x0_t**2*sin(x1)*cos(x0) + 0.5*x0_t - 0.5*x1_t - 9.8*sin(x1))/(2.0*sin(x0)**2*sin(x1)**2 + 4.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 2.0*cos(x0)**2*cos(x1)**2 - 4.0) - 2.0*(x1_t**2*sin(x0)*sin(x1) + x1_t**2*cos(x0)*cos(x1))/(2.0*sin(x0)**2*sin(x1)**2 + 4.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 2.0*cos(x0)**2*cos(x1)**2 - 4.0) - 0.125*(-4.0*sin(x0)**2*sin(x1)*cos(x1) + 4.0*sin(x0)*sin(x1)**2*cos(x0) - 4.0*sin(x0)*cos(x0)*cos(x1)**2 + 4.0*sin(x1)*cos(x0)**2*cos(x1))*(tau0 - tau1 - 1.0*x0_t + 0.5*x1_t - x1_t**2*sin(x0)*cos(x1) + x1_t**2*sin(x1)*cos(x0) - 19.6*sin(x0))/(0.5*sin(x0)**2*sin(x1)**2 + sin(x0)*sin(x1)*cos(x0)*cos(x1) + 0.5*cos(x0)**2*cos(x1)**2 - 1)**2\n",
                "    J[2,2] = 1.0*(2.0*sin(x0)*sin(x1) + 2.0*cos(x0)*cos(x1))*(2*x0_t*sin(x0)*cos(x1) - 2*x0_t*sin(x1)*cos(x0))/(2.0*sin(x0)**2*sin(x1)**2 + 4.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 2.0*cos(x0)**2*cos(x1)**2 - 4.0)\n",
                "    J[2,3] = -2.0*(-2*x1_t*sin(x0)*cos(x1) + 2*x1_t*sin(x1)*cos(x0))/(2.0*sin(x0)**2*sin(x1)**2 + 4.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 2.0*cos(x0)**2*cos(x1)**2 - 4.0)\n",
                "\n",
                "    J[3,0] = 1.0*(1.0*sin(x0)*sin(x1) + 1.0*cos(x0)*cos(x1))*(-x1_t**2*sin(x0)*sin(x1) - x1_t**2*cos(x0)*cos(x1) - 19.6*cos(x0))/(1.0*sin(x0)**2*sin(x1)**2 + 2.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 1.0*cos(x0)**2*cos(x1)**2 - 2.0) + 0.25*(1.0*sin(x0)*sin(x1) + 1.0*cos(x0)*cos(x1))*(2.0*sin(x0)**2*sin(x1)*cos(x1) - 2.0*sin(x0)*sin(x1)**2*cos(x0) + 2.0*sin(x0)*cos(x0)*cos(x1)**2 - 2.0*sin(x1)*cos(x0)**2*cos(x1))*(tau0 - tau1 - 1.0*x0_t + 0.5*x1_t - x1_t**2*sin(x0)*cos(x1) + x1_t**2*sin(x1)*cos(x0) - 19.6*sin(x0))/(0.5*sin(x0)**2*sin(x1)**2 + sin(x0)*sin(x1)*cos(x0)*cos(x1) + 0.5*cos(x0)**2*cos(x1)**2 - 1)**2 + 1.0*(-1.0*sin(x0)*cos(x1) + 1.0*sin(x1)*cos(x0))*(tau0 - tau1 - 1.0*x0_t + 0.5*x1_t - x1_t**2*sin(x0)*cos(x1) + x1_t**2*sin(x1)*cos(x0) - 19.6*sin(x0))/(1.0*sin(x0)**2*sin(x1)**2 + 2.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 1.0*cos(x0)**2*cos(x1)**2 - 2.0) - 2.0*(x0_t**2*sin(x0)*sin(x1) + x0_t**2*cos(x0)*cos(x1))/(1.0*sin(x0)**2*sin(x1)**2 + 2.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 1.0*cos(x0)**2*cos(x1)**2 - 2.0) - 0.5*(2.0*sin(x0)**2*sin(x1)*cos(x1) - 2.0*sin(x0)*sin(x1)**2*cos(x0) + 2.0*sin(x0)*cos(x0)*cos(x1)**2 - 2.0*sin(x1)*cos(x0)**2*cos(x1))*(tau1 + x0_t**2*sin(x0)*cos(x1) - x0_t**2*sin(x1)*cos(x0) + 0.5*x0_t - 0.5*x1_t - 9.8*sin(x1))/(0.5*sin(x0)**2*sin(x1)**2 + sin(x0)*sin(x1)*cos(x0)*cos(x1) + 0.5*cos(x0)**2*cos(x1)**2 - 1)**2\n",
                "    J[3,1] = 1.0*(1.0*sin(x0)*sin(x1) + 1.0*cos(x0)*cos(x1))*(x1_t**2*sin(x0)*sin(x1) + x1_t**2*cos(x0)*cos(x1))/(1.0*sin(x0)**2*sin(x1)**2 + 2.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 1.0*cos(x0)**2*cos(x1)**2 - 2.0) + 0.25*(1.0*sin(x0)*sin(x1) + 1.0*cos(x0)*cos(x1))*(-2.0*sin(x0)**2*sin(x1)*cos(x1) + 2.0*sin(x0)*sin(x1)**2*cos(x0) - 2.0*sin(x0)*cos(x0)*cos(x1)**2 + 2.0*sin(x1)*cos(x0)**2*cos(x1))*(tau0 - tau1 - 1.0*x0_t + 0.5*x1_t - x1_t**2*sin(x0)*cos(x1) + x1_t**2*sin(x1)*cos(x0) - 19.6*sin(x0))/(0.5*sin(x0)**2*sin(x1)**2 + sin(x0)*sin(x1)*cos(x0)*cos(x1) + 0.5*cos(x0)**2*cos(x1)**2 - 1)**2 + 1.0*(1.0*sin(x0)*cos(x1) - 1.0*sin(x1)*cos(x0))*(tau0 - tau1 - 1.0*x0_t + 0.5*x1_t - x1_t**2*sin(x0)*cos(x1) + x1_t**2*sin(x1)*cos(x0) - 19.6*sin(x0))/(1.0*sin(x0)**2*sin(x1)**2 + 2.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 1.0*cos(x0)**2*cos(x1)**2 - 2.0) - 2.0*(-x0_t**2*sin(x0)*sin(x1) - x0_t**2*cos(x0)*cos(x1) - 9.8*cos(x1))/(1.0*sin(x0)**2*sin(x1)**2 + 2.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 1.0*cos(x0)**2*cos(x1)**2 - 2.0) - 0.5*(-2.0*sin(x0)**2*sin(x1)*cos(x1) + 2.0*sin(x0)*sin(x1)**2*cos(x0) - 2.0*sin(x0)*cos(x0)*cos(x1)**2 + 2.0*sin(x1)*cos(x0)**2*cos(x1))*(tau1 + x0_t**2*sin(x0)*cos(x1) - x0_t**2*sin(x1)*cos(x0) + 0.5*x0_t - 0.5*x1_t - 9.8*sin(x1))/(0.5*sin(x0)**2*sin(x1)**2 + sin(x0)*sin(x1)*cos(x0)*cos(x1) + 0.5*cos(x0)**2*cos(x1)**2 - 1)**2\n",
                "    J[3,2] = -2.0*(2*x0_t*sin(x0)*cos(x1) - 2*x0_t*sin(x1)*cos(x0))/(1.0*sin(x0)**2*sin(x1)**2 + 2.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 1.0*cos(x0)**2*cos(x1)**2 - 2.0)\n",
                "    J[3,3] = 1.0*(1.0*sin(x0)*sin(x1) + 1.0*cos(x0)*cos(x1))*(-2*x1_t*sin(x0)*cos(x1) + 2*x1_t*sin(x1)*cos(x0))/(1.0*sin(x0)**2*sin(x1)**2 + 2.0*sin(x0)*sin(x1)*cos(x0)*cos(x1) + 1.0*cos(x0)**2*cos(x1)**2 - 2.0)\n",
                "    return J\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Debugging odeint vs solve_ivp\n",
                "\n",
                "t = np.arange(0,5,0.01)\n",
                "theta1 = np.random.uniform(-np.pi, np.pi)\n",
                "theta2 = np.random.uniform(-np.pi, np.pi)\n",
                "thetadot = np.random.uniform(0,0)\n",
                "omega = np.random.uniform(-np.pi, np.pi)\n",
                "  \n",
                "y0=np.array([theta1, theta2, thetadot, thetadot])\n",
                "\n",
                "from scipy.integrate import odeint\n",
                "X = odeint(doublePendulum, y0, t, args=(omega,))\n",
                "Xdot = doublePendulum(X,t,omega)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Debugging\n",
                "device = 'cuda:0'\n",
                "expr = np.array(expr)\n",
                "\n",
                "i1 = np.where(expr == 'x0_t**2')[0][0]\n",
                "i2 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i3 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i4 = np.where(expr == 'cos(x1)')[0][0]\n",
                "i5 = np.where(expr == 'x0_t*x1_t*cos(x0)*cos(x1)')[0][0]\n",
                "i6 = np.where(expr == 'x0_t*x1_t*sin(x0)*sin(x1)')[0][0]\n",
                "\n",
                "xi_L = torch.zeros(len(expr), device=device)\n",
                "\n",
                "#Creating library tensor\n",
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n",
                "\n",
                "xi_L[i1] = 1.0\n",
                "xi_L[i2] = 0.5\n",
                "xi_L[i3] = 19.62\n",
                "xi_L[i4] = 9.81\n",
                "xi_L[i5] = 1.0\n",
                "xi_L[i6] = 1.0\n",
                "\n",
                "xdot = torch.from_numpy(Xdot).to(device).float()\n",
                "TauPred = ELforward(xi_L,Zeta,Eta,Delta,xdot,device).detach().cpu().numpy()\n",
                "q_tt_pred = lagrangianforward(xi_L,Zeta,Eta,Delta,xdot,device).detach().cpu().numpy()\n",
                "q_tt = Xdot[:,2:4]\n",
                "\n",
                "t = np.arange(0,2000)\n",
                "plt.plot(t,Tau[:2000,0])\n",
                "plt.plot(t,TauPred[0,:2000])\n",
                "plt.show()\n",
                "\n",
                "plt.plot(t,Tau[:2000,1])\n",
                "plt.plot(t,TauPred[1,:2000])\n",
                "plt.show()\n",
                "\n",
                "t = np.arange(0,2000)\n",
                "plt.plot(t,q_tt[:2000,0])\n",
                "plt.plot(t,q_tt_pred[0,:2000])\n",
                "plt.show()\n",
                "\n",
                "plt.plot(t,q_tt[:2000,1])\n",
                "plt.plot(t,q_tt_pred[1,:2000])\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "f1d67c02906d6c1ff3c3070626bd4101068c113474fc7d697a1e8253903ce81f"
        },
        "kernelspec": {
            "display_name": "Python 3.8.5 64-bit ('base': conda)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
