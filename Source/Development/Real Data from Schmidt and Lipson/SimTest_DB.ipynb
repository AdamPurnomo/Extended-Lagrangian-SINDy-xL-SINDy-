{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import sys \n",
                "sys.path.append(r'../Python Script/')\n",
                "\n",
                "from sympy import symbols, simplify, derive_by_array\n",
                "from scipy.integrate import solve_ivp\n",
                "from xLSINDy import *\n",
                "from sympy.physics.mechanics import *\n",
                "from sympy import *\n",
                "import sympy\n",
                "import torch\n",
                "import HLsearch as HL\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "def generate_data(func, time, init_values):\n",
                "    sol = solve_ivp(func,[time[0],time[-1]],init_values,t_eval=time, method='RK45',rtol=1e-10,atol=1e-10)\n",
                "    return sol.y.T, np.array([func(0,sol.y.T[i,:]) for i in range(sol.y.T.shape[0])],dtype=np.float64)\n",
                "\n",
                "def pendulum(t,x):\n",
                "    return x[1],-9.81*np.sin(x[0])\n",
                "\n",
                "# Pendulum rod lengths (m), bob masses (kg).\n",
                "L1, L2 = 1, 1\n",
                "m1, m2 = 1, 1\n",
                "# The gravitational acceleration (m.s-2).\n",
                "g = 9.81\n",
                "tau = 0\n",
                "\n",
                "def doublePendulum(t,y,M=0.0):\n",
                "    q1,q2,q1_t,q2_t = y\n",
                "    q1_2t = (-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L2*g*m2*np.sin(q2) + m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t)/2 - m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t + 2*L2*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q2) - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t + 2*L2*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q2))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2)/(2*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)))/(2*(-m2**2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))**2/(4*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)) + m2*(2*L2**2*np.sin(q2)**2 + 2*L2**2*np.cos(q2)**2)/2)))/(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)\n",
                "    q2_2t = (-L2*g*m2*np.sin(q2) + m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t)/2 - m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t + 2*L2*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q2) - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t + 2*L2*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q2))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2)/(2*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)))/(-m2**2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))**2/(4*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)) + m2*(2*L2**2*np.sin(q2)**2 + 2*L2**2*np.cos(q2)**2)/2)\n",
                "    return q1_t,q2_t,q1_2t,q2_2t"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Saving Directory\n",
                "rootdir = \"../Double Pendulum/Data/\"\n",
                "\n",
                "num_sample = 100\n",
                "create_data = False\n",
                "training = True\n",
                "save = False\n",
                "noiselevel = 2e-2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "if(create_data):\n",
                "    print(\"Creating Data\")\n",
                "    num_sample = 100\n",
                "    X, Xdot = [], []\n",
                "    for i in range(num_sample):\n",
                "        t = np.arange(0,5,0.01)\n",
                "        theta1 = np.random.uniform(-np.pi, np.pi)\n",
                "        thetadot = np.random.uniform(0,0)\n",
                "        theta2 = np.random.uniform(-np.pi, np.pi)\n",
                "        \n",
                "        y0=np.array([theta1, theta2, thetadot, thetadot])\n",
                "        x,xdot = generate_data(doublePendulum,t,y0)\n",
                "        X.append(x)\n",
                "        Xdot.append(xdot)\n",
                "    X = np.vstack(X)\n",
                "    Xdot = np.vstack(Xdot)\n",
                "    if(save==True):\n",
                "        np.save(rootdir + \"X.npy\", X)\n",
                "        np.save(rootdir + \"Xdot.npy\",Xdot)\n",
                "else:\n",
                "    X = np.load(rootdir + \"X.npy\")\n",
                "    Xdot = np.load(rootdir + \"Xdot.npy\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "#adding noise\n",
                "mu, sigma = 0, noiselevel\n",
                "noise = np.random.normal(mu, sigma, X.shape[0])\n",
                "for i in range(X.shape[1]):\n",
                "    X[:,i] = X[:,i]+noise\n",
                "    Xdot[:,i] = Xdot[:,i]+noise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "states are: (x0, x1, x0_t, x1_t)\n",
                        "states derivatives are:  (x0_t, x1_t, x0_tt, x1_tt)\n"
                    ]
                }
            ],
            "source": [
                "states_dim = 4\n",
                "states = ()\n",
                "states_dot = ()\n",
                "for i in range(states_dim):\n",
                "    if(i<states_dim//2):\n",
                "        states = states + (symbols('x{}'.format(i)),)\n",
                "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
                "    else:\n",
                "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
                "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
                "print('states are:',states)\n",
                "print('states derivatives are: ', states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Turn from sympy to str\n",
                "states_sym = states\n",
                "states_dot_sym = states_dot\n",
                "states = list(str(descr) for descr in states)\n",
                "states_dot = list(str(descr) for descr in states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "#build function expression for the library in str\n",
                "exprdummy = HL.buildFunctionExpressions(1,states_dim,states,use_sine=True)\n",
                "polynom = exprdummy[2:4]\n",
                "trig = exprdummy[4:]\n",
                "polynom = HL.buildFunctionExpressions(2,len(polynom),polynom)\n",
                "trig = HL.buildFunctionExpressions(2, len(trig),trig)\n",
                "product = []\n",
                "for p in polynom:\n",
                "    for t in trig:\n",
                "        product.append(p + '*' + t)\n",
                "expr = polynom + trig + product"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Creating library tensor\n",
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "## separating known and unknown terms ##\n",
                "expr = np.array(expr)\n",
                "i1 = np.where(expr == 'x0_t**2')[0]\n",
                "\n",
                "## Garbage terms ##\n",
                "\n",
                "'''\n",
                "Explanation :\n",
                "x0_t, x1_t terms are not needed and will always satisfy EL's equation.\n",
                "Since x0_t, x1_t are garbages, we want to avoid (x0_t*sin()**2 + x0_t*cos()**2), thus we remove\n",
                "one of them, either  x0_t*sin()**2 or x0_t*cos()**2. \n",
                "Since the known term is x0_t**2, we also want to avoid the solution of (x0_t**2*sin()**2 + x0_t**2*cos()**2),\n",
                "so we remove either one of x0_t**2*sin()**2 or x0_t**2*cos()**2.\n",
                "'''\n",
                "\n",
                "i2 = np.where(expr == 'x0_t**2*cos(x0)**2')[0]\n",
                "i3 = np.where(expr == 'x0_t**2*cos(x1)**2')[0]\n",
                "i7 = np.where(expr == 'x1_t*cos(x0)**2')[0]\n",
                "i8 = np.where(expr == 'x1_t*cos(x1)**2')[0]\n",
                "i9 = np.where(expr == 'x1_t')[0]\n",
                "i10 = np.where(expr == 'x0_t*cos(x0)**2')[0]\n",
                "i11 = np.where(expr == 'x0_t*cos(x1)**2')[0]\n",
                "i12 = np.where(expr == 'x0_t')[0]\n",
                "i13 = np.where(expr == 'cos(x0)**2')[0]\n",
                "i14 = np.where(expr == 'cos(x1)**2')[0]\n",
                "\n",
                "#Deleting unused terms \n",
                "idx = np.arange(0,len(expr))\n",
                "idx = np.delete(idx,[i1,i2,i3,i7,i8,i9,i10,i11,i12,i13,i14])\n",
                "known_expr = expr[i1].tolist()\n",
                "expr = np.delete(expr,[i1,i2,i3,i7,i8,i9,i10,i11,i12,i13,i14])\n",
                "\n",
                "#non-penalty index from prev knowledge\n",
                "i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "nonpenaltyidx = [i4, i5, i6]\n",
                "\n",
                "expr = expr.tolist()\n",
                "\n",
                "Zeta_ = Zeta[:,:,i1,:].clone().detach()\n",
                "Eta_ = Eta[:,:,i1,:].clone().detach()\n",
                "Delta_ = Delta[:,i1,:].clone().detach()\n",
                "\n",
                "Zeta = Zeta[:,:,idx,:]\n",
                "Eta = Eta[:,:,idx,:]\n",
                "Delta = Delta[:,idx,:]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Moving to Cuda\n",
                "device = 'cuda:0'\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n",
                "\n",
                "Zeta_ = Zeta_.to(device)\n",
                "Eta_ = Eta_.to(device)\n",
                "Delta_ = Delta_.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "xi_L = torch.ones(len(expr), device=device).data.uniform_(-20,20)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "c = torch.ones(len(known_expr), device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "def loss(pred, targ):\n",
                "    loss = torch.mean((pred - targ)**2) \n",
                "    return loss "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clip(w, alpha):\n",
                "    clipped = torch.minimum(w,alpha)\n",
                "    clipped = torch.maximum(clipped,-alpha)\n",
                "    return clipped\n",
                "\n",
                "def proxL1norm(w_hat, alpha, nonpenaltyidx):\n",
                "    if(torch.is_tensor(alpha)==False):\n",
                "        alpha = torch.tensor(alpha)\n",
                "    w = w_hat - clip(w_hat,alpha)\n",
                "    for idx in nonpenaltyidx:\n",
                "        w[idx] = w_hat[idx]\n",
                "    return w"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "def training_loop(c,coef, prevcoef, RHS, LHS, xdot, bs, lr, lam, momentum=True):\n",
                "    loss_list = []\n",
                "    tl = xdot.shape[0]\n",
                "    n = xdot.shape[1]\n",
                "\n",
                "    Zeta_, Eta_, Delta_ = LHS\n",
                "    Zeta, Eta, Delta = RHS\n",
                "\n",
                "    if(torch.is_tensor(xdot)==False):\n",
                "        xdot = torch.from_numpy(xdot).to(device).float()\n",
                "    \n",
                "    v = coef.clone().detach().requires_grad_(True)\n",
                "    prev = v\n",
                "    \n",
                "    for i in range(tl//bs):\n",
                "                \n",
                "        #computing acceleration with momentum\n",
                "        if(momentum==True):\n",
                "            vhat = (v + ((i-1)/(i+2))*(v - prev)).clone().detach().requires_grad_(True)\n",
                "        else:\n",
                "            vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
                "   \n",
                "        prev = v\n",
                "\n",
                "        #Computing loss\n",
                "        zeta = Zeta[:,:,:,i*bs:(i+1)*bs]\n",
                "        eta = Eta[:,:,:,i*bs:(i+1)*bs]\n",
                "        delta = Delta[:,:,i*bs:(i+1)*bs]\n",
                "\n",
                "        zeta_ = Zeta_[:,:,:,i*bs:(i+1)*bs]\n",
                "        eta_ = Eta_[:,:,:,i*bs:(i+1)*bs]\n",
                "        delta_ = Delta_[:,:,i*bs:(i+1)*bs]\n",
                "        \n",
                "        x_t = xdot[i*bs:(i+1)*bs,:]\n",
                "\n",
                "        #forward\n",
                "        pred = -ELforward(vhat,zeta,eta,delta,x_t,device)\n",
                "        targ = ELforward(c,zeta_,eta_,delta_,x_t,device)\n",
                "        \n",
                "        lossval = loss(pred, targ)\n",
                "        \n",
                "        #Backpropagation\n",
                "        lossval.backward()\n",
                "\n",
                "        with torch.no_grad():\n",
                "            v = vhat - lr*vhat.grad\n",
                "            v = (proxL1norm(v,lr*lam,nonpenaltyidx))\n",
                "            \n",
                "            # Manually zero the gradients after updating weights\n",
                "            vhat.grad = None\n",
                "        \n",
                "        \n",
                "    \n",
                "        \n",
                "        loss_list.append(lossval.item())\n",
                "    print(\"Average loss : \" , torch.tensor(loss_list).mean().item())\n",
                "    return v, prevcoef, torch.tensor(loss_list).mean().item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "Epoch 0/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  31756.845703125\n",
                        "\n",
                        "\n",
                        "Epoch 1/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  2030.63916015625\n",
                        "\n",
                        "\n",
                        "Epoch 2/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1264.8812255859375\n",
                        "\n",
                        "\n",
                        "Epoch 3/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  901.633544921875\n",
                        "\n",
                        "\n",
                        "Epoch 4/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  683.1854248046875\n",
                        "\n",
                        "\n",
                        "Epoch 5/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  539.0038452148438\n",
                        "\n",
                        "\n",
                        "Epoch 6/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  437.5927734375\n",
                        "\n",
                        "\n",
                        "Epoch 7/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  362.76690673828125\n",
                        "\n",
                        "\n",
                        "Epoch 8/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  305.352294921875\n",
                        "\n",
                        "\n",
                        "Epoch 9/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  260.6390686035156\n",
                        "\n",
                        "\n",
                        "Epoch 10/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  224.6626739501953\n",
                        "\n",
                        "\n",
                        "Epoch 11/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  195.5170135498047\n",
                        "\n",
                        "\n",
                        "Epoch 12/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  171.52716064453125\n",
                        "\n",
                        "\n",
                        "Epoch 13/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  151.5218505859375\n",
                        "\n",
                        "\n",
                        "Epoch 14/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  134.96286010742188\n",
                        "\n",
                        "\n",
                        "Epoch 15/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  121.01837158203125\n",
                        "\n",
                        "\n",
                        "Epoch 16/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  109.22575378417969\n",
                        "\n",
                        "\n",
                        "Epoch 17/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  98.99443817138672\n",
                        "\n",
                        "\n",
                        "Epoch 18/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  90.11734771728516\n",
                        "\n",
                        "\n",
                        "Epoch 19/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  82.35387420654297\n",
                        "\n",
                        "\n",
                        "Epoch 20/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  75.46810150146484\n",
                        "\n",
                        "\n",
                        "Epoch 21/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  69.5064697265625\n",
                        "\n",
                        "\n",
                        "Epoch 22/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  64.1852798461914\n",
                        "\n",
                        "\n",
                        "Epoch 23/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  59.41996383666992\n",
                        "\n",
                        "\n",
                        "Epoch 24/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  55.18778610229492\n",
                        "\n",
                        "\n",
                        "Epoch 25/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  51.412925720214844\n",
                        "\n",
                        "\n",
                        "Epoch 26/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  47.99473190307617\n",
                        "\n",
                        "\n",
                        "Epoch 27/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  44.926551818847656\n",
                        "\n",
                        "\n",
                        "Epoch 28/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  42.21216583251953\n",
                        "\n",
                        "\n",
                        "Epoch 29/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  39.72480392456055\n",
                        "\n",
                        "\n",
                        "Epoch 30/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  37.48004150390625\n",
                        "\n",
                        "\n",
                        "Epoch 31/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  35.43135452270508\n",
                        "\n",
                        "\n",
                        "Epoch 32/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  33.52842330932617\n",
                        "\n",
                        "\n",
                        "Epoch 33/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  31.757905960083008\n",
                        "\n",
                        "\n",
                        "Epoch 34/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  30.13874626159668\n",
                        "\n",
                        "\n",
                        "Epoch 35/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  28.670684814453125\n",
                        "\n",
                        "\n",
                        "Epoch 36/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  27.29098129272461\n",
                        "\n",
                        "\n",
                        "Epoch 37/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  26.024320602416992\n",
                        "\n",
                        "\n",
                        "Epoch 38/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  24.84657859802246\n",
                        "\n",
                        "\n",
                        "Epoch 39/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  23.788320541381836\n",
                        "\n",
                        "\n",
                        "Epoch 40/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  22.82207489013672\n",
                        "\n",
                        "\n",
                        "Epoch 41/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  21.896089553833008\n",
                        "\n",
                        "\n",
                        "Epoch 42/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  21.0463924407959\n",
                        "\n",
                        "\n",
                        "Epoch 43/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.314199447631836\n",
                        "\n",
                        "\n",
                        "Epoch 44/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.607006072998047\n",
                        "\n",
                        "\n",
                        "Epoch 45/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  18.9167423248291\n",
                        "\n",
                        "\n",
                        "Epoch 46/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  18.35346221923828\n",
                        "\n",
                        "\n",
                        "Epoch 47/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  17.75790786743164\n",
                        "\n",
                        "\n",
                        "Epoch 48/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  17.25101089477539\n",
                        "\n",
                        "\n",
                        "Epoch 49/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  16.76485824584961\n",
                        "\n",
                        "\n",
                        "Epoch 50/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  16.349916458129883\n",
                        "\n",
                        "\n",
                        "Epoch 51/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  15.97226333618164\n",
                        "\n",
                        "\n",
                        "Epoch 52/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  15.630924224853516\n",
                        "\n",
                        "\n",
                        "Epoch 53/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  15.32298469543457\n",
                        "\n",
                        "\n",
                        "Epoch 54/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  15.0614595413208\n",
                        "\n",
                        "\n",
                        "Epoch 55/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  14.816975593566895\n",
                        "\n",
                        "\n",
                        "Epoch 56/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  14.600561141967773\n",
                        "\n",
                        "\n",
                        "Epoch 57/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  14.421286582946777\n",
                        "\n",
                        "\n",
                        "Epoch 58/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  14.260863304138184\n",
                        "\n",
                        "\n",
                        "Epoch 59/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  14.122445106506348\n",
                        "\n",
                        "\n",
                        "Epoch 60/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13.985480308532715\n",
                        "\n",
                        "\n",
                        "Epoch 61/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13.85542106628418\n",
                        "\n",
                        "\n",
                        "Epoch 62/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13.713007926940918\n",
                        "\n",
                        "\n",
                        "Epoch 63/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13.581991195678711\n",
                        "\n",
                        "\n",
                        "Epoch 64/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13.4527006149292\n",
                        "\n",
                        "\n",
                        "Epoch 65/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13.335345268249512\n",
                        "\n",
                        "\n",
                        "Epoch 66/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13.234383583068848\n",
                        "\n",
                        "\n",
                        "Epoch 67/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13.141220092773438\n",
                        "\n",
                        "\n",
                        "Epoch 68/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  13.039573669433594\n",
                        "\n",
                        "\n",
                        "Epoch 69/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.964866638183594\n",
                        "\n",
                        "\n",
                        "Epoch 70/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.885902404785156\n",
                        "\n",
                        "\n",
                        "Epoch 71/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.81175708770752\n",
                        "\n",
                        "\n",
                        "Epoch 72/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.74052619934082\n",
                        "\n",
                        "\n",
                        "Epoch 73/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.670401573181152\n",
                        "\n",
                        "\n",
                        "Epoch 74/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.596190452575684\n",
                        "\n",
                        "\n",
                        "Epoch 75/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.525808334350586\n",
                        "\n",
                        "\n",
                        "Epoch 76/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.45989990234375\n",
                        "\n",
                        "\n",
                        "Epoch 77/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.394255638122559\n",
                        "\n",
                        "\n",
                        "Epoch 78/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.32750415802002\n",
                        "\n",
                        "\n",
                        "Epoch 79/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.267898559570312\n",
                        "\n",
                        "\n",
                        "Epoch 80/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.208098411560059\n",
                        "\n",
                        "\n",
                        "Epoch 81/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.149036407470703\n",
                        "\n",
                        "\n",
                        "Epoch 82/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.092477798461914\n",
                        "\n",
                        "\n",
                        "Epoch 83/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  12.037309646606445\n",
                        "\n",
                        "\n",
                        "Epoch 84/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.976767539978027\n",
                        "\n",
                        "\n",
                        "Epoch 85/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.923359870910645\n",
                        "\n",
                        "\n",
                        "Epoch 86/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.870491981506348\n",
                        "\n",
                        "\n",
                        "Epoch 87/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.817803382873535\n",
                        "\n",
                        "\n",
                        "Epoch 88/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.772941589355469\n",
                        "\n",
                        "\n",
                        "Epoch 89/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.715644836425781\n",
                        "\n",
                        "\n",
                        "Epoch 90/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.657758712768555\n",
                        "\n",
                        "\n",
                        "Epoch 91/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.61719036102295\n",
                        "\n",
                        "\n",
                        "Epoch 92/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.55671215057373\n",
                        "\n",
                        "\n",
                        "Epoch 93/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.550148963928223\n",
                        "\n",
                        "\n",
                        "Epoch 94/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.537564277648926\n",
                        "\n",
                        "\n",
                        "Epoch 95/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.504672050476074\n",
                        "\n",
                        "\n",
                        "Epoch 96/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.476259231567383\n",
                        "\n",
                        "\n",
                        "Epoch 97/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.438335418701172\n",
                        "\n",
                        "\n",
                        "Epoch 98/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.407591819763184\n",
                        "\n",
                        "\n",
                        "Epoch 99/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.366862297058105\n",
                        "\n",
                        "\n",
                        "Epoch 100/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  11.331759452819824\n"
                    ]
                }
            ],
            "source": [
                "Epoch = 100\n",
                "i = 0\n",
                "lr = 5e-6\n",
                "lam = 1\n",
                "temp = 1000\n",
                "RHS = [Zeta, Eta, Delta]\n",
                "LHS = [Zeta_, Eta_, Delta_]\n",
                "while(i<=Epoch):\n",
                "    print(\"\\n\")\n",
                "    print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
                "    print(\"Learning rate : \", lr)\n",
                "    xi_L, prevxi_L, lossitem= training_loop(c, xi_L,prevxi_L,RHS,LHS,Xdot,128,lr=lr,lam=lam)\n",
                "    temp = lossitem\n",
                "    i+=1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result stage 1:  -0.01*x0_t**2*sin(2*x1) - 1.82*x0_t**2*cos(x0) + 0.58*x0_t**2*cos(2*x0) - 1.32*x0_t**2*cos(x1) + 0.405*x0_t**2*cos(2*x1) + 0.435*x0_t**2*cos(x0 - x1) + 0.425*x0_t**2*cos(x0 + x1) - 0.985*x0_t**2 + 0.005*x0_t*x1_t*sin(2*x0) + 0.005*x0_t*x1_t*sin(2*x1) + 0.03*x0_t*x1_t*cos(x0) + 0.045*x0_t*x1_t*cos(2*x0) + 0.02*x0_t*x1_t*cos(x1) + 0.015*x0_t*x1_t*cos(2*x1) + 0.575*x0_t*x1_t*cos(x0 - x1) + 0.295*x0_t*x1_t*cos(x0 + x1) - 0.02*x0_t*x1_t - 0.01*x1_t**2*cos(x0) + 0.0900000000000007*x1_t**2*cos(2*x0) + 0.05*x1_t**2*cos(2*x1) + 0.045*x1_t**2*cos(x0 - x1) + 0.045*x1_t**2*cos(x0 + x1) - 1.99*x1_t**2 + 4.27*cos(x0) + 6.22*cos(x1) - 0.905*cos(x0 - x1) + 0.905*cos(x0 + x1)\n"
                    ]
                }
            ],
            "source": [
                "## Thresholding\n",
                "threshold = 1e-2\n",
                "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
                "expr = np.array(expr)[surv_index].tolist()\n",
                "\n",
                "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "\n",
                "## obtaining analytical model\n",
                "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
                "L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-3)\n",
                "print(\"Result stage 1: \", simplify(L))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "Epoch 0/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  172.33639526367188\n",
                        "\n",
                        "\n",
                        "Epoch 1/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  24.161298751831055\n",
                        "\n",
                        "\n",
                        "Epoch 2/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  21.461692810058594\n",
                        "\n",
                        "\n",
                        "Epoch 3/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  20.46820640563965\n",
                        "\n",
                        "\n",
                        "Epoch 4/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  20.338376998901367\n",
                        "\n",
                        "\n",
                        "Epoch 5/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  19.94996452331543\n",
                        "\n",
                        "\n",
                        "Epoch 6/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  19.43919563293457\n",
                        "\n",
                        "\n",
                        "Epoch 7/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  19.356395721435547\n",
                        "\n",
                        "\n",
                        "Epoch 8/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  18.883708953857422\n",
                        "\n",
                        "\n",
                        "Epoch 9/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  18.631200790405273\n",
                        "\n",
                        "\n",
                        "Epoch 10/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  18.03507423400879\n",
                        "\n",
                        "\n",
                        "Epoch 11/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  17.887752532958984\n",
                        "\n",
                        "\n",
                        "Epoch 12/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  17.784208297729492\n",
                        "\n",
                        "\n",
                        "Epoch 13/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  17.488672256469727\n",
                        "\n",
                        "\n",
                        "Epoch 14/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  17.04803466796875\n",
                        "\n",
                        "\n",
                        "Epoch 15/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  16.696807861328125\n",
                        "\n",
                        "\n",
                        "Epoch 16/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  16.292137145996094\n",
                        "\n",
                        "\n",
                        "Epoch 17/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  16.15880584716797\n",
                        "\n",
                        "\n",
                        "Epoch 18/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  15.92032241821289\n",
                        "\n",
                        "\n",
                        "Epoch 19/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  15.545782089233398\n",
                        "\n",
                        "\n",
                        "Epoch 20/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  15.478931427001953\n",
                        "\n",
                        "\n",
                        "Epoch 21/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  15.183342933654785\n",
                        "\n",
                        "\n",
                        "Epoch 22/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  14.594206809997559\n",
                        "\n",
                        "\n",
                        "Epoch 23/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  14.729458808898926\n",
                        "\n",
                        "\n",
                        "Epoch 24/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  14.44211196899414\n",
                        "\n",
                        "\n",
                        "Epoch 25/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  14.102775573730469\n",
                        "\n",
                        "\n",
                        "Epoch 26/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  14.05854606628418\n",
                        "\n",
                        "\n",
                        "Epoch 27/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  13.758508682250977\n",
                        "\n",
                        "\n",
                        "Epoch 28/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  13.229251861572266\n",
                        "\n",
                        "\n",
                        "Epoch 29/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  13.3582181930542\n",
                        "\n",
                        "\n",
                        "Epoch 30/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  13.016728401184082\n",
                        "\n",
                        "\n",
                        "Epoch 31/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  12.895084381103516\n",
                        "\n",
                        "\n",
                        "Epoch 32/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  12.941261291503906\n",
                        "\n",
                        "\n",
                        "Epoch 33/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  12.355576515197754\n",
                        "\n",
                        "\n",
                        "Epoch 34/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  12.036865234375\n",
                        "\n",
                        "\n",
                        "Epoch 35/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  12.041067123413086\n",
                        "\n",
                        "\n",
                        "Epoch 36/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  11.794854164123535\n",
                        "\n",
                        "\n",
                        "Epoch 37/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  11.680975914001465\n",
                        "\n",
                        "\n",
                        "Epoch 38/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  11.356940269470215\n",
                        "\n",
                        "\n",
                        "Epoch 39/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  11.163383483886719\n",
                        "\n",
                        "\n",
                        "Epoch 40/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  10.901796340942383\n",
                        "\n",
                        "\n",
                        "Epoch 41/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  11.046467781066895\n",
                        "\n",
                        "\n",
                        "Epoch 42/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  10.645464897155762\n",
                        "\n",
                        "\n",
                        "Epoch 43/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  10.467489242553711\n",
                        "\n",
                        "\n",
                        "Epoch 44/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  10.403518676757812\n",
                        "\n",
                        "\n",
                        "Epoch 45/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  10.145824432373047\n",
                        "\n",
                        "\n",
                        "Epoch 46/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  10.16561222076416\n",
                        "\n",
                        "\n",
                        "Epoch 47/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  9.911067008972168\n",
                        "\n",
                        "\n",
                        "Epoch 48/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  9.57691764831543\n",
                        "\n",
                        "\n",
                        "Epoch 49/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  9.242522239685059\n",
                        "\n",
                        "\n",
                        "Epoch 50/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  8.962362289428711\n",
                        "\n",
                        "\n",
                        "Epoch 51/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  9.127548217773438\n",
                        "\n",
                        "\n",
                        "Epoch 52/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  9.236298561096191\n",
                        "\n",
                        "\n",
                        "Epoch 53/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  9.072030067443848\n",
                        "\n",
                        "\n",
                        "Epoch 54/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  8.485756874084473\n",
                        "\n",
                        "\n",
                        "Epoch 55/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  8.421560287475586\n",
                        "\n",
                        "\n",
                        "Epoch 56/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  8.284685134887695\n",
                        "\n",
                        "\n",
                        "Epoch 57/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  8.231776237487793\n",
                        "\n",
                        "\n",
                        "Epoch 58/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  8.10007095336914\n",
                        "\n",
                        "\n",
                        "Epoch 59/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  7.8646745681762695\n",
                        "\n",
                        "\n",
                        "Epoch 60/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  7.823836803436279\n",
                        "\n",
                        "\n",
                        "Epoch 61/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  7.668022632598877\n",
                        "\n",
                        "\n",
                        "Epoch 62/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  7.511190414428711\n",
                        "\n",
                        "\n",
                        "Epoch 63/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  7.286767482757568\n",
                        "\n",
                        "\n",
                        "Epoch 64/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  7.223907470703125\n",
                        "\n",
                        "\n",
                        "Epoch 65/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  7.109128475189209\n",
                        "\n",
                        "\n",
                        "Epoch 66/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  7.0259809494018555\n",
                        "\n",
                        "\n",
                        "Epoch 67/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  6.999978065490723\n",
                        "\n",
                        "\n",
                        "Epoch 68/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  6.8353447914123535\n",
                        "\n",
                        "\n",
                        "Epoch 69/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  6.661683559417725\n",
                        "\n",
                        "\n",
                        "Epoch 70/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  6.664107799530029\n",
                        "\n",
                        "\n",
                        "Epoch 71/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  6.430043697357178\n",
                        "\n",
                        "\n",
                        "Epoch 72/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  6.39494514465332\n",
                        "\n",
                        "\n",
                        "Epoch 73/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  6.28471040725708\n",
                        "\n",
                        "\n",
                        "Epoch 74/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  6.263354301452637\n",
                        "\n",
                        "\n",
                        "Epoch 75/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  6.055864334106445\n",
                        "\n",
                        "\n",
                        "Epoch 76/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  5.8819193840026855\n",
                        "\n",
                        "\n",
                        "Epoch 77/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  5.676287651062012\n",
                        "\n",
                        "\n",
                        "Epoch 78/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  5.835681915283203\n",
                        "\n",
                        "\n",
                        "Epoch 79/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  5.611804485321045\n",
                        "\n",
                        "\n",
                        "Epoch 80/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  5.361355781555176\n",
                        "\n",
                        "\n",
                        "Epoch 81/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  5.361739635467529\n",
                        "\n",
                        "\n",
                        "Epoch 82/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  5.29124116897583\n",
                        "\n",
                        "\n",
                        "Epoch 83/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  5.257602214813232\n",
                        "\n",
                        "\n",
                        "Epoch 84/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  5.213590621948242\n",
                        "\n",
                        "\n",
                        "Epoch 85/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  5.092316150665283\n",
                        "\n",
                        "\n",
                        "Epoch 86/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  5.1019439697265625\n",
                        "\n",
                        "\n",
                        "Epoch 87/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  4.805622100830078\n",
                        "\n",
                        "\n",
                        "Epoch 88/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  4.808207988739014\n",
                        "\n",
                        "\n",
                        "Epoch 89/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  4.661753177642822\n",
                        "\n",
                        "\n",
                        "Epoch 90/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  4.572879314422607\n",
                        "\n",
                        "\n",
                        "Epoch 91/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  4.4372029304504395\n",
                        "\n",
                        "\n",
                        "Epoch 92/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  4.370502471923828\n",
                        "\n",
                        "\n",
                        "Epoch 93/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  4.3909101486206055\n",
                        "\n",
                        "\n",
                        "Epoch 94/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  4.102230072021484\n",
                        "\n",
                        "\n",
                        "Epoch 95/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  4.222778797149658\n",
                        "\n",
                        "\n",
                        "Epoch 96/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  4.236488342285156\n",
                        "\n",
                        "\n",
                        "Epoch 97/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  4.1949143409729\n",
                        "\n",
                        "\n",
                        "Epoch 98/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  3.909183979034424\n",
                        "\n",
                        "\n",
                        "Epoch 99/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  3.8805770874023438\n",
                        "\n",
                        "\n",
                        "Epoch 100/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  3.882068395614624\n",
                        "Result stage 2: -0.203*x0_t**2*sin(x0)**2 + 0.168*x0_t**2*cos(x0)*cos(x1) - 0.219*x0_t**2*cos(x0) - 0.112*x0_t**2*cos(x1) + 0.726*x0_t*x1_t*sin(x0)*sin(x1) + 0.839*x0_t*x1_t*cos(x0)*cos(x1) + 0.028*x1_t**2*sin(x0)**2 + 0.407*x1_t**2 + 13.949*cos(x0) + 7.148*cos(x1)\n",
                        "\n",
                        "\n",
                        "Epoch 0/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  6.846584320068359\n",
                        "\n",
                        "\n",
                        "Epoch 1/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  6.0111308097839355\n",
                        "\n",
                        "\n",
                        "Epoch 2/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  5.927854061126709\n",
                        "\n",
                        "\n",
                        "Epoch 3/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  5.511862754821777\n",
                        "\n",
                        "\n",
                        "Epoch 4/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  5.5956034660339355\n",
                        "\n",
                        "\n",
                        "Epoch 5/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  5.40199089050293\n",
                        "\n",
                        "\n",
                        "Epoch 6/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  5.1855692863464355\n",
                        "\n",
                        "\n",
                        "Epoch 7/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.948205947875977\n",
                        "\n",
                        "\n",
                        "Epoch 8/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.699123382568359\n",
                        "\n",
                        "\n",
                        "Epoch 9/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.865850925445557\n",
                        "\n",
                        "\n",
                        "Epoch 10/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.494221210479736\n",
                        "\n",
                        "\n",
                        "Epoch 11/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.503915786743164\n",
                        "\n",
                        "\n",
                        "Epoch 12/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.161811828613281\n",
                        "\n",
                        "\n",
                        "Epoch 13/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.237937927246094\n",
                        "\n",
                        "\n",
                        "Epoch 14/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.942427635192871\n",
                        "\n",
                        "\n",
                        "Epoch 15/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.142708778381348\n",
                        "\n",
                        "\n",
                        "Epoch 16/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.866015911102295\n",
                        "\n",
                        "\n",
                        "Epoch 17/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.550724506378174\n",
                        "\n",
                        "\n",
                        "Epoch 18/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.7390077114105225\n",
                        "\n",
                        "\n",
                        "Epoch 19/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.543612003326416\n",
                        "\n",
                        "\n",
                        "Epoch 20/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.590426206588745\n",
                        "\n",
                        "\n",
                        "Epoch 21/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.220263719558716\n",
                        "\n",
                        "\n",
                        "Epoch 22/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.1124587059020996\n",
                        "\n",
                        "\n",
                        "Epoch 23/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.9285635948181152\n",
                        "\n",
                        "\n",
                        "Epoch 24/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.108658790588379\n",
                        "\n",
                        "\n",
                        "Epoch 25/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.863406181335449\n",
                        "\n",
                        "\n",
                        "Epoch 26/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.940380573272705\n",
                        "\n",
                        "\n",
                        "Epoch 27/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.5726895332336426\n",
                        "\n",
                        "\n",
                        "Epoch 28/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.7048211097717285\n",
                        "\n",
                        "\n",
                        "Epoch 29/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.536226749420166\n",
                        "\n",
                        "\n",
                        "Epoch 30/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.4164445400238037\n",
                        "\n",
                        "\n",
                        "Epoch 31/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.494966506958008\n",
                        "\n",
                        "\n",
                        "Epoch 32/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.2982981204986572\n",
                        "\n",
                        "\n",
                        "Epoch 33/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.20074725151062\n",
                        "\n",
                        "\n",
                        "Epoch 34/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.230790138244629\n",
                        "\n",
                        "\n",
                        "Epoch 35/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.0746443271636963\n",
                        "\n",
                        "\n",
                        "Epoch 36/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.0948920249938965\n",
                        "\n",
                        "\n",
                        "Epoch 37/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.0296132564544678\n",
                        "\n",
                        "\n",
                        "Epoch 38/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.9103299379348755\n",
                        "\n",
                        "\n",
                        "Epoch 39/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.9080051183700562\n",
                        "\n",
                        "\n",
                        "Epoch 40/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.7726290225982666\n",
                        "\n",
                        "\n",
                        "Epoch 41/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.7348182201385498\n",
                        "\n",
                        "\n",
                        "Epoch 42/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.757308840751648\n",
                        "\n",
                        "\n",
                        "Epoch 43/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.660691738128662\n",
                        "\n",
                        "\n",
                        "Epoch 44/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.7459913492202759\n",
                        "\n",
                        "\n",
                        "Epoch 45/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.6180964708328247\n",
                        "\n",
                        "\n",
                        "Epoch 46/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.543319582939148\n",
                        "\n",
                        "\n",
                        "Epoch 47/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.5181407928466797\n",
                        "\n",
                        "\n",
                        "Epoch 48/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.4454517364501953\n",
                        "\n",
                        "\n",
                        "Epoch 49/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.400630235671997\n",
                        "\n",
                        "\n",
                        "Epoch 50/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.395820140838623\n",
                        "\n",
                        "\n",
                        "Epoch 51/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.335800290107727\n",
                        "\n",
                        "\n",
                        "Epoch 52/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.3043506145477295\n",
                        "\n",
                        "\n",
                        "Epoch 53/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.2971397638320923\n",
                        "\n",
                        "\n",
                        "Epoch 54/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.3473751544952393\n",
                        "\n",
                        "\n",
                        "Epoch 55/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.1831248998641968\n",
                        "\n",
                        "\n",
                        "Epoch 56/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.2036432027816772\n",
                        "\n",
                        "\n",
                        "Epoch 57/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.1827547550201416\n",
                        "\n",
                        "\n",
                        "Epoch 58/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.0179308652877808\n",
                        "\n",
                        "\n",
                        "Epoch 59/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.1233139038085938\n",
                        "\n",
                        "\n",
                        "Epoch 60/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.0751492977142334\n",
                        "\n",
                        "\n",
                        "Epoch 61/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.9653459787368774\n",
                        "\n",
                        "\n",
                        "Epoch 62/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.9808467030525208\n",
                        "\n",
                        "\n",
                        "Epoch 63/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.9874887466430664\n",
                        "\n",
                        "\n",
                        "Epoch 64/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.9610995650291443\n",
                        "\n",
                        "\n",
                        "Epoch 65/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.9900824427604675\n",
                        "\n",
                        "\n",
                        "Epoch 66/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.9682103991508484\n",
                        "\n",
                        "\n",
                        "Epoch 67/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.8991876244544983\n",
                        "\n",
                        "\n",
                        "Epoch 68/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.9059043526649475\n",
                        "\n",
                        "\n",
                        "Epoch 69/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.8749535083770752\n",
                        "\n",
                        "\n",
                        "Epoch 70/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.8571399450302124\n",
                        "\n",
                        "\n",
                        "Epoch 71/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.8369017839431763\n",
                        "\n",
                        "\n",
                        "Epoch 72/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.8001424074172974\n",
                        "\n",
                        "\n",
                        "Epoch 73/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.7706956267356873\n",
                        "\n",
                        "\n",
                        "Epoch 74/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.7615156173706055\n",
                        "\n",
                        "\n",
                        "Epoch 75/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.789839506149292\n",
                        "\n",
                        "\n",
                        "Epoch 76/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.7281132340431213\n",
                        "\n",
                        "\n",
                        "Epoch 77/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.6939234137535095\n",
                        "\n",
                        "\n",
                        "Epoch 78/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.7422152161598206\n",
                        "\n",
                        "\n",
                        "Epoch 79/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.6843199133872986\n",
                        "\n",
                        "\n",
                        "Epoch 80/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.6339049339294434\n",
                        "\n",
                        "\n",
                        "Epoch 81/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.616431713104248\n",
                        "\n",
                        "\n",
                        "Epoch 82/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.6275617480278015\n",
                        "\n",
                        "\n",
                        "Epoch 83/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.5653985142707825\n",
                        "\n",
                        "\n",
                        "Epoch 84/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.609526515007019\n",
                        "\n",
                        "\n",
                        "Epoch 85/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.5919198989868164\n",
                        "\n",
                        "\n",
                        "Epoch 86/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.5438030362129211\n",
                        "\n",
                        "\n",
                        "Epoch 87/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.5803454518318176\n",
                        "\n",
                        "\n",
                        "Epoch 88/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.5266348719596863\n",
                        "\n",
                        "\n",
                        "Epoch 89/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.5203832387924194\n",
                        "\n",
                        "\n",
                        "Epoch 90/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.4986407458782196\n",
                        "\n",
                        "\n",
                        "Epoch 91/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.5034000277519226\n",
                        "\n",
                        "\n",
                        "Epoch 92/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.47599419951438904\n",
                        "\n",
                        "\n",
                        "Epoch 93/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.4874696433544159\n",
                        "\n",
                        "\n",
                        "Epoch 94/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.4301392734050751\n",
                        "\n",
                        "\n",
                        "Epoch 95/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.43598148226737976\n",
                        "\n",
                        "\n",
                        "Epoch 96/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.4378420412540436\n",
                        "\n",
                        "\n",
                        "Epoch 97/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.41515791416168213\n",
                        "\n",
                        "\n",
                        "Epoch 98/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.3986454904079437\n",
                        "\n",
                        "\n",
                        "Epoch 99/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.3757903277873993\n",
                        "\n",
                        "\n",
                        "Epoch 100/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.4231596291065216\n",
                        "Result stage 3: 0.933*x0_t*x1_t*sin(x0)*sin(x1) + 0.949*x0_t*x1_t*cos(x0)*cos(x1) + 0.02*x1_t**2*sin(x0)**2 + 0.476*x1_t**2 + 18.263*cos(x0) + 9.211*cos(x1)\n",
                        "\n",
                        "\n",
                        "Epoch 0/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  1.144033432006836\n",
                        "\n",
                        "\n",
                        "Epoch 1/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.9765839576721191\n",
                        "\n",
                        "\n",
                        "Epoch 2/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.8097808957099915\n",
                        "\n",
                        "\n",
                        "Epoch 3/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.7164745330810547\n",
                        "\n",
                        "\n",
                        "Epoch 4/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.6049741506576538\n",
                        "\n",
                        "\n",
                        "Epoch 5/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.5549448132514954\n",
                        "\n",
                        "\n",
                        "Epoch 6/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.5045486688613892\n",
                        "\n",
                        "\n",
                        "Epoch 7/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.44162821769714355\n",
                        "\n",
                        "\n",
                        "Epoch 8/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.33829402923583984\n",
                        "\n",
                        "\n",
                        "Epoch 9/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.2840276062488556\n",
                        "\n",
                        "\n",
                        "Epoch 10/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.27709877490997314\n",
                        "\n",
                        "\n",
                        "Epoch 11/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.24927540123462677\n",
                        "\n",
                        "\n",
                        "Epoch 12/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.2405206561088562\n",
                        "\n",
                        "\n",
                        "Epoch 13/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.21597491204738617\n",
                        "\n",
                        "\n",
                        "Epoch 14/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.2009403258562088\n",
                        "\n",
                        "\n",
                        "Epoch 15/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.18132856488227844\n",
                        "\n",
                        "\n",
                        "Epoch 16/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.17047080397605896\n",
                        "\n",
                        "\n",
                        "Epoch 17/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.1435553878545761\n",
                        "\n",
                        "\n",
                        "Epoch 18/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.13060545921325684\n",
                        "\n",
                        "\n",
                        "Epoch 19/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.13779832422733307\n",
                        "\n",
                        "\n",
                        "Epoch 20/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.12001358717679977\n",
                        "\n",
                        "\n",
                        "Epoch 21/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.11526098847389221\n",
                        "\n",
                        "\n",
                        "Epoch 22/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.10942378640174866\n",
                        "\n",
                        "\n",
                        "Epoch 23/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.10809215903282166\n",
                        "\n",
                        "\n",
                        "Epoch 24/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.10285559296607971\n",
                        "\n",
                        "\n",
                        "Epoch 25/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.0945754200220108\n",
                        "\n",
                        "\n",
                        "Epoch 26/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08919499069452286\n",
                        "\n",
                        "\n",
                        "Epoch 27/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.09155396372079849\n",
                        "\n",
                        "\n",
                        "Epoch 28/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08683743327856064\n",
                        "\n",
                        "\n",
                        "Epoch 29/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.0829225704073906\n",
                        "\n",
                        "\n",
                        "Epoch 30/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08418156206607819\n",
                        "\n",
                        "\n",
                        "Epoch 31/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08349431306123734\n",
                        "\n",
                        "\n",
                        "Epoch 32/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.083744116127491\n",
                        "\n",
                        "\n",
                        "Epoch 33/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08192860335111618\n",
                        "\n",
                        "\n",
                        "Epoch 34/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08219433575868607\n",
                        "\n",
                        "\n",
                        "Epoch 35/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.0817769393324852\n",
                        "\n",
                        "\n",
                        "Epoch 36/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08204072713851929\n",
                        "\n",
                        "\n",
                        "Epoch 37/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08168508857488632\n",
                        "\n",
                        "\n",
                        "Epoch 38/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08223295211791992\n",
                        "\n",
                        "\n",
                        "Epoch 39/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.0817050114274025\n",
                        "\n",
                        "\n",
                        "Epoch 40/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08138104528188705\n",
                        "\n",
                        "\n",
                        "Epoch 41/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.0814599096775055\n",
                        "\n",
                        "\n",
                        "Epoch 42/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08148078620433807\n",
                        "\n",
                        "\n",
                        "Epoch 43/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08245554566383362\n",
                        "\n",
                        "\n",
                        "Epoch 44/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08186066895723343\n",
                        "\n",
                        "\n",
                        "Epoch 45/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08148100972175598\n",
                        "\n",
                        "\n",
                        "Epoch 46/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08143757283687592\n",
                        "\n",
                        "\n",
                        "Epoch 47/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08191807568073273\n",
                        "\n",
                        "\n",
                        "Epoch 48/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08162505179643631\n",
                        "\n",
                        "\n",
                        "Epoch 49/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08132731169462204\n",
                        "\n",
                        "\n",
                        "Epoch 50/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08118806034326553\n",
                        "\n",
                        "\n",
                        "Epoch 51/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08130752295255661\n",
                        "\n",
                        "\n",
                        "Epoch 52/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08095833659172058\n",
                        "\n",
                        "\n",
                        "Epoch 53/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08090390264987946\n",
                        "\n",
                        "\n",
                        "Epoch 54/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08107645064592361\n",
                        "\n",
                        "\n",
                        "Epoch 55/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08101627975702286\n",
                        "\n",
                        "\n",
                        "Epoch 56/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08098327368497849\n",
                        "\n",
                        "\n",
                        "Epoch 57/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08130626380443573\n",
                        "\n",
                        "\n",
                        "Epoch 58/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08112958818674088\n",
                        "\n",
                        "\n",
                        "Epoch 59/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08109681308269501\n",
                        "\n",
                        "\n",
                        "Epoch 60/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08111714571714401\n",
                        "\n",
                        "\n",
                        "Epoch 61/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08097603917121887\n",
                        "\n",
                        "\n",
                        "Epoch 62/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08105405420064926\n",
                        "\n",
                        "\n",
                        "Epoch 63/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08122231811285019\n",
                        "\n",
                        "\n",
                        "Epoch 64/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08105011284351349\n",
                        "\n",
                        "\n",
                        "Epoch 65/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08096198737621307\n",
                        "\n",
                        "\n",
                        "Epoch 66/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08111034333705902\n",
                        "\n",
                        "\n",
                        "Epoch 67/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08108177036046982\n",
                        "\n",
                        "\n",
                        "Epoch 68/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08103679120540619\n",
                        "\n",
                        "\n",
                        "Epoch 69/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08101176470518112\n",
                        "\n",
                        "\n",
                        "Epoch 70/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08119818568229675\n",
                        "\n",
                        "\n",
                        "Epoch 71/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08106300979852676\n",
                        "\n",
                        "\n",
                        "Epoch 72/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08102104812860489\n",
                        "\n",
                        "\n",
                        "Epoch 73/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08102717995643616\n",
                        "\n",
                        "\n",
                        "Epoch 74/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08101467043161392\n",
                        "\n",
                        "\n",
                        "Epoch 75/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08112678676843643\n",
                        "\n",
                        "\n",
                        "Epoch 76/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08122120052576065\n",
                        "\n",
                        "\n",
                        "Epoch 77/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08112923055887222\n",
                        "\n",
                        "\n",
                        "Epoch 78/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08104968070983887\n",
                        "\n",
                        "\n",
                        "Epoch 79/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08116715401411057\n",
                        "\n",
                        "\n",
                        "Epoch 80/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08163266628980637\n",
                        "\n",
                        "\n",
                        "Epoch 81/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08112891763448715\n",
                        "\n",
                        "\n",
                        "Epoch 82/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08092959970235825\n",
                        "\n",
                        "\n",
                        "Epoch 83/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08092465251684189\n",
                        "\n",
                        "\n",
                        "Epoch 84/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08092940598726273\n",
                        "\n",
                        "\n",
                        "Epoch 85/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08079700917005539\n",
                        "\n",
                        "\n",
                        "Epoch 86/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08078866451978683\n",
                        "\n",
                        "\n",
                        "Epoch 87/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08087248355150223\n",
                        "\n",
                        "\n",
                        "Epoch 88/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08088462799787521\n",
                        "\n",
                        "\n",
                        "Epoch 89/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08090396970510483\n",
                        "\n",
                        "\n",
                        "Epoch 90/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08080738037824631\n",
                        "\n",
                        "\n",
                        "Epoch 91/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08080800622701645\n",
                        "\n",
                        "\n",
                        "Epoch 92/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08087246865034103\n",
                        "\n",
                        "\n",
                        "Epoch 93/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08081962913274765\n",
                        "\n",
                        "\n",
                        "Epoch 94/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08086469769477844\n",
                        "\n",
                        "\n",
                        "Epoch 95/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08082297444343567\n",
                        "\n",
                        "\n",
                        "Epoch 96/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08079871535301208\n",
                        "\n",
                        "\n",
                        "Epoch 97/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08084297925233841\n",
                        "\n",
                        "\n",
                        "Epoch 98/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08080849796533585\n",
                        "\n",
                        "\n",
                        "Epoch 99/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08084309101104736\n",
                        "\n",
                        "\n",
                        "Epoch 100/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08086008578538895\n",
                        "Result stage 4: 0.998*x0_t*x1_t*sin(x0)*sin(x1) + 1.001*x0_t*x1_t*cos(x0)*cos(x1) + 0.499*x1_t**2 + 19.559*cos(x0) + 9.796*cos(x1)\n"
                    ]
                }
            ],
            "source": [
                "## Next round selection ##\n",
                "for stage in range(4):\n",
                "\n",
                "    #Redefine computation after thresholding\n",
                "    expr.append(known_expr[0])\n",
                "    Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)\n",
                "\n",
                "    expr = np.array(expr)\n",
                "    i1 = np.where(expr == 'x0_t**2')[0]\n",
                "    i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "    i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "    i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "    idx = np.arange(0,len(expr))\n",
                "    idx = np.delete(idx,i1)\n",
                "    known_expr = expr[i1].tolist()\n",
                "    expr = np.delete(expr,i1).tolist()\n",
                "    nonpenaltyidx = [i4,i5,i6]\n",
                "\n",
                "    Zeta_ = Zeta[:,:,i1,:].clone().detach()\n",
                "    Eta_ = Eta[:,:,i1,:].clone().detach()\n",
                "    Delta_ = Delta[:,i1,:].clone().detach()\n",
                "\n",
                "    Zeta = Zeta[:,:,idx,:]\n",
                "    Eta = Eta[:,:,idx,:]\n",
                "    Delta = Delta[:,idx,:]\n",
                "\n",
                "    Zeta = Zeta.to(device)\n",
                "    Eta = Eta.to(device)\n",
                "    Delta = Delta.to(device)\n",
                "    Zeta_ = Zeta_.to(device)\n",
                "    Eta_ = Eta_.to(device)\n",
                "    Delta_ = Delta_.to(device)\n",
                "\n",
                "    Epoch = 100\n",
                "    i = 0\n",
                "    lr += 2e-6\n",
                "    if(stage==3):\n",
                "        lam = 0\n",
                "    else:\n",
                "        lam = 0.1\n",
                "    temp = 1000\n",
                "    RHS = [Zeta, Eta, Delta]\n",
                "    LHS = [Zeta_, Eta_, Delta_]\n",
                "    while(i<=Epoch):\n",
                "        print(\"\\n\")\n",
                "        print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
                "        print(\"Learning rate : \", lr)\n",
                "        xi_L, prevxi_L, lossitem= training_loop(c, xi_L,prevxi_L,RHS,LHS,Xdot,128,lr=lr,lam=lam)\n",
                "        i+=1\n",
                "        if(temp <= 1e-3):\n",
                "            break\n",
                "    \n",
                "    ## Thresholding\n",
                "    threshold = 1e-1\n",
                "    surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
                "    expr = np.array(expr)[surv_index].tolist()\n",
                "\n",
                "    xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
                "    prevxi_L = xi_L.clone().detach()\n",
                "\n",
                "    ## obtaining analytical model\n",
                "    xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=3)\n",
                "    L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-1)\n",
                "    print(\"Result stage \" + str(stage+2) + \":\" , simplify(L))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0.998*x0_t*x1_t*sin(x0)*sin(x1) + 1.001*x0_t*x1_t*cos(x0)*cos(x1) + 0.499*x1_t**2 + 19.559*cos(x0) + 9.796*cos(x1) + x0_t**2\n"
                    ]
                }
            ],
            "source": [
                "## Adding known terms\n",
                "L = str(simplify(L)) + \" + \" + known_expr[0]\n",
                "print(L)\n",
                "\n",
                "expr = expr + known_expr\n",
                "xi_L = torch.cat((xi_L, c))\n",
                "mask = torch.ones(len(xi_L),device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "if(save==True):\n",
                "    #Saving Equation in string\n",
                "    text_file = open(rootdir + \"lagrangian_\" + str(noiselevel)+ \"_noise.txt\", \"w\")\n",
                "    text_file.write(L)\n",
                "    text_file.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "d5784be8b0ed123205c521437a438df309f2d2f16cb6cf8124a1b3e0f87bfce1"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 64-bit ('SystemIdentification': conda)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
