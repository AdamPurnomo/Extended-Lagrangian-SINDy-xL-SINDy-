{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import sys \n",
                "sys.path.append(r'../../Python Script/')\n",
                "\n",
                "from sympy import symbols, simplify, derive_by_array\n",
                "from scipy.integrate import solve_ivp\n",
                "from xLSINDy import *\n",
                "from sympy.physics.mechanics import *\n",
                "from sympy import *\n",
                "import sympy\n",
                "import torch\n",
                "import HLsearch as HL\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "def generate_data(func, time, init_values):\n",
                "    sol = solve_ivp(func,[time[0],time[-1]],init_values,t_eval=time, method='RK45',rtol=1e-10,atol=1e-10)\n",
                "    return sol.y.T, np.array([func(0,sol.y.T[i,:]) for i in range(sol.y.T.shape[0])],dtype=np.float64)\n",
                "\n",
                "def pendulum(t,x):\n",
                "    return x[1],-9.81*np.sin(x[0])\n",
                "\n",
                "# Pendulum rod lengths (m), bob masses (kg).\n",
                "L1, L2 = 1, 1\n",
                "m1, m2 = 1, 1\n",
                "# The gravitational acceleration (m.s-2).\n",
                "g = 9.81\n",
                "tau = 0\n",
                "\n",
                "def doublePendulum(t,y,M=0.0):\n",
                "    q1,q2,q1_t,q2_t = y\n",
                "    q1_2t = (-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L2*g*m2*np.sin(q2) + m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t)/2 - m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t + 2*L2*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q2) - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t + 2*L2*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q2))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2)/(2*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)))/(2*(-m2**2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))**2/(4*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)) + m2*(2*L2**2*np.sin(q2)**2 + 2*L2**2*np.cos(q2)**2)/2)))/(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)\n",
                "    q2_2t = (-L2*g*m2*np.sin(q2) + m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t)/2 - m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t + 2*L2*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q2) - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t + 2*L2*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q2))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2)/(2*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)))/(-m2**2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))**2/(4*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)) + m2*(2*L2**2*np.sin(q2)**2 + 2*L2**2*np.cos(q2)**2)/2)\n",
                "    return q1_t,q2_t,q1_2t,q2_2t"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Saving Directory\n",
                "rootdir = \"../../Double Pendulum/Data/\"\n",
                "\n",
                "num_sample = 100\n",
                "create_data = False\n",
                "training = True\n",
                "save = False\n",
                "noiselevel = 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if(create_data):\n",
                "    print(\"Creating Data\")\n",
                "    num_sample = 100\n",
                "    X, Xdot = [], []\n",
                "    for i in range(num_sample):\n",
                "        t = np.arange(0,5,0.01)\n",
                "        theta1 = np.random.uniform(-np.pi, np.pi)\n",
                "        thetadot = np.random.uniform(0,0)\n",
                "        theta2 = np.random.uniform(-np.pi, np.pi)\n",
                "        \n",
                "        y0=np.array([theta1, theta2, thetadot, thetadot])\n",
                "        x,xdot = generate_data(doublePendulum,t,y0)\n",
                "        X.append(x)\n",
                "        Xdot.append(xdot)\n",
                "    X = np.vstack(X)\n",
                "    Xdot = np.vstack(Xdot)\n",
                "    if(save==True):\n",
                "        np.save(rootdir + \"X.npy\", X)\n",
                "        np.save(rootdir + \"Xdot.npy\",Xdot)\n",
                "else:\n",
                "    X = np.load(rootdir + \"X.npy\")\n",
                "    Xdot = np.load(rootdir + \"Xdot.npy\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#adding noise\n",
                "mu, sigma = 0, noiselevel\n",
                "noise = np.random.normal(mu, sigma, X.shape[0])\n",
                "for i in range(X.shape[1]):\n",
                "    X[:,i] = X[:,i]+noise\n",
                "    Xdot[:,i] = Xdot[:,i]+noise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "states_dim = 4\n",
                "states = ()\n",
                "states_dot = ()\n",
                "for i in range(states_dim):\n",
                "    if(i<states_dim//2):\n",
                "        states = states + (symbols('x{}'.format(i)),)\n",
                "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
                "    else:\n",
                "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
                "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
                "print('states are:',states)\n",
                "print('states derivatives are: ', states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Turn from sympy to str\n",
                "states_sym = states\n",
                "states_dot_sym = states_dot\n",
                "states = list(str(descr) for descr in states)\n",
                "states_dot = list(str(descr) for descr in states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#build function expression for the library in str\n",
                "exprdummy = HL.buildFunctionExpressions(1,states_dim,states,use_sine=True)\n",
                "polynom = exprdummy[2:4]\n",
                "trig = exprdummy[4:]\n",
                "polynom = HL.buildFunctionExpressions(2,len(polynom),polynom)\n",
                "trig = HL.buildFunctionExpressions(2, len(trig),trig)\n",
                "product = []\n",
                "for p in polynom:\n",
                "    for t in trig:\n",
                "        product.append(p + '*' + t)\n",
                "expr = polynom + trig + product"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Creating library tensor\n",
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Debugging for only known terms\n",
                "\n",
                "expr = np.array(expr)\n",
                "i0 = np.where(expr == 'x0_t**2')[0]\n",
                "\n",
                "## Separating known term and uknown term\n",
                "idx = np.arange(0,len(expr))\n",
                "idx = np.delete(idx,[i0])\n",
                "known_expr = expr[i0].tolist()\n",
                "expr = np.delete(expr,[i0])\n",
                "\n",
                "#expr = expr.tolist()\n",
                "\n",
                "Zeta_ = Zeta[:,:,i0,:].clone().detach()\n",
                "Eta_ = Eta[:,:,i0,:].clone().detach()\n",
                "Delta_ = Delta[:,i0,:].clone().detach()\n",
                "\n",
                "Zeta = Zeta[:,:,idx,:]\n",
                "Eta = Eta[:,:,idx,:]\n",
                "Delta = Delta[:,idx,:]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Moving to Cuda\n",
                "device = 'cuda:0'\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n",
                "\n",
                "Zeta_ = Zeta_.to(device)\n",
                "Eta_ = Eta_.to(device)\n",
                "Delta_ = Delta_.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Initialize coefficient\n",
                "\n",
                "xi_L = torch.zeros(len(expr), device=device)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "c = torch.ones(len(known_expr), device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Assigning True Coefficient\n",
                "#xi_L[i0] = 1\n",
                "\n",
                "i1 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i2 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i3 = np.where(expr == 'cos(x1)')[0][0]\n",
                "i4 = np.where(expr == 'x0_t*x1_t*cos(x0)*cos(x1)')[0][0]\n",
                "i5 = np.where(expr == 'x0_t*x1_t*sin(x0)*sin(x1)')[0][0]\n",
                "\n",
                "xi_L[i1] = 0.5\n",
                "xi_L[i2] = 19.620\n",
                "xi_L[i3] = 9.810\n",
                "xi_L[i4] = 1.0\n",
                "xi_L[i5] = 1.0\n",
                "\n",
                "## Checking for forward\n",
                "pred = ELforward(xi_L, Zeta, Eta, Delta, Xdot, device)\n",
                "targ = -ELforward(c, Zeta_, Eta_, Delta_, Xdot, device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Checking shape\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "t = np.arange(500)\n",
                "\n",
                "plt.plot(t,pred[0,:500].detach().cpu().numpy())\n",
                "plt.show()\n",
                "plt.plot(t,targ[0,:500].detach().cpu().numpy())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ELforward2(coef, Zeta, Eta, Delta, xdot, device):\n",
                "    \"\"\"\n",
                "    Computing time series of total sum of Euler-Lagrange equation\n",
                "    #Params:\n",
                "    coef        : Coefficient corresponding to each basis function\n",
                "    Zeta        : time-series of double derivative of basis functions w.r.t qdot and qdot  \n",
                "    Eta         : time-series of double derivative of basis functions w.r.t qdot and q \n",
                "    Delta       : time-series of derivative of basis functions w.r.t q \n",
                "    xdot        : Time-series of states_dot data  \n",
                "\n",
                "    #Returns:\n",
                "    El          : Time series of the left hand side of Euler's Lagranges equation (n, time-series)\n",
                "    \"\"\"\n",
                "    weight = xi_L\n",
                "    n = Xdot.shape[1]\n",
                "    \n",
                "    if(torch.is_tensor(Xdot)==False):\n",
                "        xdot = torch.from_numpy(Xdot).to(device).float()\n",
                "        q_t = xdot[:,:n//2].T\n",
                "        q_tt = xdot[:,n//2:].T\n",
                "\n",
                "    A = torch.einsum('ijkl,il->jkl',Zeta,q_tt)\n",
                "    B = torch.einsum('ijkl,il->jkl',Eta, q_t)\n",
                "    C = Delta\n",
                "\n",
                "    Upsilon = A + B - C\n",
                "    EL = torch.einsum('jkl,k->jl',Upsilon, weight)\n",
                "    return EL, Upsilon"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Checking for forward\n",
                "(pred2, UpsilonL) = ELforward2(xi_L, Zeta, Eta, Delta, Xdot, device)\n",
                "(targ2, UpsilonR) = ELforward2(c, Zeta_, Eta_, Delta_, Xdot, device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(t,-pred2[0,:500].detach().cpu().numpy())\n",
                "plt.show()\n",
                "plt.plot(t,targ2[0,:500].detach().cpu().numpy())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "for i in range(88):\n",
                "    plt.plot(t,UpsilonL[0,i,:500].detach().cpu().numpy())\n",
                "    print(expr[i])\n",
                "    plt.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i in range(88):\n",
                "    plt.plot(t,UpsilonL[1,i,:500].detach().cpu().numpy())\n",
                "    print(expr[i])\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(t,UpsilonR[0,0,:500].detach().cpu().numpy())\n",
                "plt.show()\n",
                "\n",
                "plt.plot(t,UpsilonR[1,0,:500].detach().cpu().numpy())\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Training ###"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## separating known and unknown terms ##\n",
                "expr = np.array(expr)\n",
                "i1 = np.where(expr == 'x0_t**2')[0]\n",
                "\n",
                "## Garbage terms ##\n",
                "\n",
                "'''\n",
                "Explanation :\n",
                "x0_t, x1_t terms are not needed and will always satisfy EL's equation.\n",
                "Since x0_t, x1_t are garbages, we want to avoid (x0_t*sin()**2 + x0_t*cos()**2), thus we remove\n",
                "one of them, either  x0_t*sin()**2 or x0_t*cos()**2. \n",
                "Since the known term is x0_t**2, we also want to avoid the solution of (x0_t**2*sin()**2 + x0_t**2*cos()**2),\n",
                "so we remove either one of x0_t**2*sin()**2 or x0_t**2*cos()**2.\n",
                "'''\n",
                "\n",
                "i2 = np.where(expr == 'x0_t**2*cos(x0)**2')[0]\n",
                "i3 = np.where(expr == 'x0_t**2*cos(x1)**2')[0]\n",
                "i7 = np.where(expr == 'x1_t*cos(x0)**2')[0]\n",
                "i8 = np.where(expr == 'x1_t*cos(x1)**2')[0]\n",
                "i9 = np.where(expr == 'x1_t')[0]\n",
                "i10 = np.where(expr == 'x0_t*cos(x0)**2')[0]\n",
                "i11 = np.where(expr == 'x0_t*cos(x1)**2')[0]\n",
                "i12 = np.where(expr == 'x0_t')[0]\n",
                "i13 = np.where(expr == 'cos(x0)**2')[0]\n",
                "i14 = np.where(expr == 'cos(x1)**2')[0]\n",
                "\n",
                "#Deleting unused terms \n",
                "idx = np.arange(0,len(expr))\n",
                "idx = np.delete(idx,[i1,i2,i3,i7,i8,i9,i10,i11,i12,i13,i14])\n",
                "known_expr = expr[i1].tolist()\n",
                "expr = np.delete(expr,[i1,i2,i3,i7,i8,i9,i10,i11,i12,i13,i14])\n",
                "\n",
                "#non-penalty index from prev knowledge\n",
                "i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "nonpenaltyidx = [i4, i5, i6]\n",
                "\n",
                "expr = expr.tolist()\n",
                "\n",
                "Zeta_ = Zeta[:,:,i1,:].clone().detach()\n",
                "Eta_ = Eta[:,:,i1,:].clone().detach()\n",
                "Delta_ = Delta[:,i1,:].clone().detach()\n",
                "\n",
                "Zeta = Zeta[:,:,idx,:]\n",
                "Eta = Eta[:,:,idx,:]\n",
                "Delta = Delta[:,idx,:]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Moving to Cuda\n",
                "device = 'cuda:0'\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n",
                "\n",
                "Zeta_ = Zeta_.to(device)\n",
                "Eta_ = Eta_.to(device)\n",
                "Delta_ = Delta_.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "xi_L = torch.ones(len(expr), device=device).data.uniform_(-20,20)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "c = torch.ones(len(known_expr), device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def loss(pred, targ):\n",
                "    loss = torch.mean((pred - targ)**2) \n",
                "    return loss "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clip(w, alpha):\n",
                "    clipped = torch.minimum(w,alpha)\n",
                "    clipped = torch.maximum(clipped,-alpha)\n",
                "    return clipped\n",
                "\n",
                "def proxL1norm(w_hat, alpha, nonpenaltyidx):\n",
                "    if(torch.is_tensor(alpha)==False):\n",
                "        alpha = torch.tensor(alpha)\n",
                "    w = w_hat - clip(w_hat,alpha)\n",
                "    for idx in nonpenaltyidx:\n",
                "        w[idx] = w_hat[idx]\n",
                "    return w"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def training_loop(c,coef, prevcoef, RHS, LHS, xdot, bs, lr, lam, momentum=True):\n",
                "    loss_list = []\n",
                "    tl = xdot.shape[0]\n",
                "    n = xdot.shape[1]\n",
                "\n",
                "    Zeta_, Eta_, Delta_ = LHS\n",
                "    Zeta, Eta, Delta = RHS\n",
                "\n",
                "    if(torch.is_tensor(xdot)==False):\n",
                "        xdot = torch.from_numpy(xdot).to(device).float()\n",
                "    \n",
                "    v = coef.clone().detach().requires_grad_(True)\n",
                "    prev = v\n",
                "    \n",
                "    for i in range(tl//bs):\n",
                "                \n",
                "        #computing acceleration with momentum\n",
                "        if(momentum==True):\n",
                "            vhat = (v + ((i-1)/(i+2))*(v - prev)).clone().detach().requires_grad_(True)\n",
                "        else:\n",
                "            vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
                "   \n",
                "        prev = v\n",
                "\n",
                "        #Computing loss\n",
                "        zeta = Zeta[:,:,:,i*bs:(i+1)*bs]\n",
                "        eta = Eta[:,:,:,i*bs:(i+1)*bs]\n",
                "        delta = Delta[:,:,i*bs:(i+1)*bs]\n",
                "\n",
                "        zeta_ = Zeta_[:,:,:,i*bs:(i+1)*bs]\n",
                "        eta_ = Eta_[:,:,:,i*bs:(i+1)*bs]\n",
                "        delta_ = Delta_[:,:,i*bs:(i+1)*bs]\n",
                "        \n",
                "        x_t = xdot[i*bs:(i+1)*bs,:]\n",
                "\n",
                "        #forward\n",
                "        pred = -ELforward(vhat,zeta,eta,delta,x_t,device)\n",
                "        targ = ELforward(c,zeta_,eta_,delta_,x_t,device)\n",
                "        \n",
                "        lossval = loss(pred, targ)\n",
                "        \n",
                "        #Backpropagation\n",
                "        lossval.backward()\n",
                "\n",
                "        with torch.no_grad():\n",
                "            v = vhat - lr*vhat.grad\n",
                "            v = (proxL1norm(v,lr*lam,nonpenaltyidx))\n",
                "            \n",
                "            # Manually zero the gradients after updating weights\n",
                "            vhat.grad = None\n",
                "        \n",
                "        \n",
                "    \n",
                "        \n",
                "        loss_list.append(lossval.item())\n",
                "    print(\"Average loss : \" , torch.tensor(loss_list).mean().item())\n",
                "    return v, prevcoef, torch.tensor(loss_list).mean().item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Epoch = 100\n",
                "i = 0\n",
                "lr = 5e-6\n",
                "lam = 1\n",
                "temp = 1000\n",
                "RHS = [Zeta, Eta, Delta]\n",
                "LHS = [Zeta_, Eta_, Delta_]\n",
                "while(i<=Epoch):\n",
                "    print(\"\\n\")\n",
                "    print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
                "    print(\"Learning rate : \", lr)\n",
                "    xi_L, prevxi_L, lossitem= training_loop(c, xi_L,prevxi_L,RHS,LHS,Xdot,128,lr=lr,lam=lam)\n",
                "    temp = lossitem\n",
                "    i+=1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Thresholding\n",
                "threshold = 1e-2\n",
                "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
                "expr = np.array(expr)[surv_index].tolist()\n",
                "\n",
                "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "\n",
                "## obtaining analytical model\n",
                "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
                "L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-3)\n",
                "print(\"Result stage 1: \", simplify(L))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Next round selection ##\n",
                "for stage in range(4):\n",
                "\n",
                "    #Redefine computation after thresholding\n",
                "    expr.append(known_expr[0])\n",
                "    Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)\n",
                "\n",
                "    expr = np.array(expr)\n",
                "    i1 = np.where(expr == 'x0_t**2')[0]\n",
                "    i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "    i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "    i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "    idx = np.arange(0,len(expr))\n",
                "    idx = np.delete(idx,i1)\n",
                "    known_expr = expr[i1].tolist()\n",
                "    expr = np.delete(expr,i1).tolist()\n",
                "    nonpenaltyidx = [i4,i5,i6]\n",
                "\n",
                "    Zeta_ = Zeta[:,:,i1,:].clone().detach()\n",
                "    Eta_ = Eta[:,:,i1,:].clone().detach()\n",
                "    Delta_ = Delta[:,i1,:].clone().detach()\n",
                "\n",
                "    Zeta = Zeta[:,:,idx,:]\n",
                "    Eta = Eta[:,:,idx,:]\n",
                "    Delta = Delta[:,idx,:]\n",
                "\n",
                "    Zeta = Zeta.to(device)\n",
                "    Eta = Eta.to(device)\n",
                "    Delta = Delta.to(device)\n",
                "    Zeta_ = Zeta_.to(device)\n",
                "    Eta_ = Eta_.to(device)\n",
                "    Delta_ = Delta_.to(device)\n",
                "\n",
                "    Epoch = 100\n",
                "    i = 0\n",
                "    lr += 2e-6\n",
                "    if(stage==3):\n",
                "        lam = 0\n",
                "    else:\n",
                "        lam = 0.1\n",
                "    temp = 1000\n",
                "    RHS = [Zeta, Eta, Delta]\n",
                "    LHS = [Zeta_, Eta_, Delta_]\n",
                "    while(i<=Epoch):\n",
                "        print(\"\\n\")\n",
                "        print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
                "        print(\"Learning rate : \", lr)\n",
                "        xi_L, prevxi_L, lossitem= training_loop(c, xi_L,prevxi_L,RHS,LHS,Xdot,128,lr=lr,lam=lam)\n",
                "        i+=1\n",
                "        if(temp <= 1e-3):\n",
                "            break\n",
                "    \n",
                "    ## Thresholding\n",
                "    threshold = 1e-1\n",
                "    surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
                "    expr = np.array(expr)[surv_index].tolist()\n",
                "\n",
                "    xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
                "    prevxi_L = xi_L.clone().detach()\n",
                "\n",
                "    ## obtaining analytical model\n",
                "    xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=3)\n",
                "    L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-1)\n",
                "    print(\"Result stage \" + str(stage+2) + \":\" , simplify(L))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Adding known terms\n",
                "L = str(simplify(L)) + \" + \" + known_expr[0]\n",
                "print(L)\n",
                "\n",
                "expr = expr + known_expr\n",
                "xi_L = torch.cat((xi_L, c))\n",
                "mask = torch.ones(len(xi_L),device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if(save==True):\n",
                "    #Saving Equation in string\n",
                "    text_file = open(rootdir + \"lagrangian_\" + str(noiselevel)+ \"_noise.txt\", \"w\")\n",
                "    text_file.write(L)\n",
                "    text_file.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "d5784be8b0ed123205c521437a438df309f2d2f16cb6cf8124a1b3e0f87bfce1"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 64-bit ('SystemIdentification': conda)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
