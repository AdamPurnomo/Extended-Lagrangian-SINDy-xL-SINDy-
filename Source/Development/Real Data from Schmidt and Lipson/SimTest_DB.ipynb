{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import sys \n",
                "sys.path.append(r'../../Python Script/')\n",
                "\n",
                "from sympy import symbols, simplify, derive_by_array\n",
                "from scipy.integrate import solve_ivp\n",
                "from xLSINDy import *\n",
                "from sympy.physics.mechanics import *\n",
                "from sympy import *\n",
                "import sympy\n",
                "import torch\n",
                "import HLsearch as HL\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "def generate_data(func, time, init_values):\n",
                "    sol = solve_ivp(func,[time[0],time[-1]],init_values,t_eval=time, method='RK45',rtol=1e-10,atol=1e-10)\n",
                "    return sol.y.T, np.array([func(0,sol.y.T[i,:]) for i in range(sol.y.T.shape[0])],dtype=np.float64)\n",
                "\n",
                "def pendulum(t,x):\n",
                "    return x[1],-9.81*np.sin(x[0])\n",
                "\n",
                "# Pendulum rod lengths (m), bob masses (kg).\n",
                "L1, L2 = 1, 1\n",
                "m1, m2 = 1, 1\n",
                "# The gravitational acceleration (m.s-2).\n",
                "g = 9.81\n",
                "tau = 0\n",
                "\n",
                "def doublePendulum(t,y,M=0.0):\n",
                "    q1,q2,q1_t,q2_t = y\n",
                "    q1_2t = (-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L2*g*m2*np.sin(q2) + m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t)/2 - m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t + 2*L2*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q2) - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t + 2*L2*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q2))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2)/(2*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)))/(2*(-m2**2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))**2/(4*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)) + m2*(2*L2**2*np.sin(q2)**2 + 2*L2**2*np.cos(q2)**2)/2)))/(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)\n",
                "    q2_2t = (-L2*g*m2*np.sin(q2) + m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t)/2 - m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t + 2*L2*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q2) - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t + 2*L2*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q2))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2)/(2*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)))/(-m2**2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))**2/(4*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)) + m2*(2*L2**2*np.sin(q2)**2 + 2*L2**2*np.cos(q2)**2)/2)\n",
                "    return q1_t,q2_t,q1_2t,q2_2t"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Saving Directory\n",
                "rootdir = \"../../Double Pendulum/Data/\"\n",
                "\n",
                "num_sample = 100\n",
                "create_data = False\n",
                "training = True\n",
                "save = False\n",
                "noiselevel = 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "if(create_data):\n",
                "    print(\"Creating Data\")\n",
                "    num_sample = 100\n",
                "    X, Xdot = [], []\n",
                "    for i in range(num_sample):\n",
                "        t = np.arange(0,5,0.01)\n",
                "        theta1 = np.random.uniform(-np.pi, np.pi)\n",
                "        thetadot = np.random.uniform(0,0)\n",
                "        theta2 = np.random.uniform(-np.pi, np.pi)\n",
                "        \n",
                "        y0=np.array([theta1, theta2, thetadot, thetadot])\n",
                "        x,xdot = generate_data(doublePendulum,t,y0)\n",
                "        X.append(x)\n",
                "        Xdot.append(xdot)\n",
                "    X = np.vstack(X)\n",
                "    Xdot = np.vstack(Xdot)\n",
                "    if(save==True):\n",
                "        np.save(rootdir + \"X.npy\", X)\n",
                "        np.save(rootdir + \"Xdot.npy\",Xdot)\n",
                "else:\n",
                "    X = np.load(rootdir + \"X.npy\")\n",
                "    Xdot = np.load(rootdir + \"Xdot.npy\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "#adding noise\n",
                "mu, sigma = 0, noiselevel\n",
                "noise = np.random.normal(mu, sigma, X.shape[0])\n",
                "for i in range(X.shape[1]):\n",
                "    X[:,i] = X[:,i]+noise\n",
                "    Xdot[:,i] = Xdot[:,i]+noise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "states are: (x0, x1, x0_t, x1_t)\n",
                        "states derivatives are:  (x0_t, x1_t, x0_tt, x1_tt)\n"
                    ]
                }
            ],
            "source": [
                "states_dim = 4\n",
                "states = ()\n",
                "states_dot = ()\n",
                "for i in range(states_dim):\n",
                "    if(i<states_dim//2):\n",
                "        states = states + (symbols('x{}'.format(i)),)\n",
                "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
                "    else:\n",
                "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
                "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
                "print('states are:',states)\n",
                "print('states derivatives are: ', states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Turn from sympy to str\n",
                "states_sym = states\n",
                "states_dot_sym = states_dot\n",
                "states = list(str(descr) for descr in states)\n",
                "states_dot = list(str(descr) for descr in states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "#build function expression for the library in str\n",
                "exprdummy = HL.buildFunctionExpressions(1,states_dim,states,use_sine=True)\n",
                "polynom = exprdummy[2:4]\n",
                "trig = exprdummy[4:]\n",
                "polynom = HL.buildFunctionExpressions(2,len(polynom),polynom)\n",
                "trig = HL.buildFunctionExpressions(2, len(trig),trig)\n",
                "product = []\n",
                "for p in polynom:\n",
                "    for t in trig:\n",
                "        product.append(p + '*' + t)\n",
                "expr = polynom + trig + product"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Creating library tensor\n",
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Debugging for only known terms\n",
                "\n",
                "expr = np.array(expr)\n",
                "i1 = np.where(expr == 'x0_t**2')[0]\n",
                "\n",
                "## Garbage terms ##\n",
                "\n",
                "'''\n",
                "Explanation :\n",
                "x0_t, x1_t terms are not needed and will always satisfy EL's equation.\n",
                "Since x0_t, x1_t are garbages, we want to avoid (x0_t*sin()**2 + x0_t*cos()**2), thus we remove\n",
                "one of them, either  x0_t*sin()**2 or x0_t*cos()**2. \n",
                "Since the known term is x0_t**2, we also want to avoid the solution of (x0_t**2*sin()**2 + x0_t**2*cos()**2),\n",
                "so we remove either one of x0_t**2*sin()**2 or x0_t**2*cos()**2.\n",
                "'''\n",
                "\n",
                "\n",
                "i2 = np.where(expr == 'x0_t**2*cos(x0)**2')[0]\n",
                "i3 = np.where(expr == 'x0_t**2*cos(x1)**2')[0]\n",
                "i7 = np.where(expr == 'x1_t*cos(x0)**2')[0]\n",
                "i8 = np.where(expr == 'x1_t*cos(x1)**2')[0]\n",
                "i9 = np.where(expr == 'x1_t')[0]\n",
                "i10 = np.where(expr == 'x0_t*cos(x0)**2')[0]\n",
                "i11 = np.where(expr == 'x0_t*cos(x1)**2')[0]\n",
                "i12 = np.where(expr == 'x0_t')[0]\n",
                "i13 = np.where(expr == 'cos(x0)**2')[0]\n",
                "i14 = np.where(expr == 'cos(x1)**2')[0]\n",
                "\n",
                "#Deleting unused terms \n",
                "idx = np.arange(0,len(expr))\n",
                "idx = np.delete(idx,[i1,i2,i3,i7,i8,i9,i10,i11,i12,i13,i14])\n",
                "known_expr = expr[i1].tolist()\n",
                "expr = np.delete(expr,[i1,i2,i3,i7,i8,i9,i10,i11,i12,i13,i14])\n",
                "\n",
                "## Separating known term and uknown term\n",
                "idx = np.arange(0,len(expr))\n",
                "idx = np.delete(idx,[i1])\n",
                "known_expr = expr[i1].tolist()\n",
                "expr = np.delete(expr,[i1])\n",
                "\n",
                "#non-penalty index from prev knowledge\n",
                "i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "nonpenaltyidx = [i4, i5, i6]\n",
                "\n",
                "expr = expr.tolist()\n",
                "\n",
                "Zeta_ = Zeta[:,:,i1,:].clone().detach()\n",
                "Eta_ = Eta[:,:,i1,:].clone().detach()\n",
                "Delta_ = Delta[:,i1,:].clone().detach()\n",
                "\n",
                "Zeta = Zeta[:,:,idx,:]\n",
                "Eta = Eta[:,:,idx,:]\n",
                "Delta = Delta[:,idx,:]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Moving to Cuda\n",
                "device = 'cuda:0'\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n",
                "\n",
                "Zeta_ = Zeta_.to(device)\n",
                "Eta_ = Eta_.to(device)\n",
                "Delta_ = Delta_.to(device)\n",
                "\n",
                "UpsilonL = Upsilonforward(Zeta, Eta, Delta, Xdot, device)\n",
                "UpsilonR = Upsilonforward(Zeta_, Eta_, Delta_, Xdot, device)\n",
                "scale = torch.ones(UpsilonL.shape[1])\n",
                "\n",
                "## Scaling for upsilon\n",
                "for i in range(UpsilonL.shape[1]):\n",
                "    scale[i] = UpsilonL[:,i,:].max() - UpsilonL[:,i,:].min()\n",
                "    if(scale[i]!=0):\n",
                "        UpsilonL[:,i,:] = UpsilonL[:,i,:]/scale[i] \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Initialize coefficient\n",
                "\n",
                "xi_L = torch.zeros(len(expr), device=device)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "c = torch.ones(len(known_expr), device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "def loss(pred, targ):\n",
                "    loss = torch.mean((pred - targ)**2) \n",
                "    return loss "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clip(w, alpha):\n",
                "    clipped = torch.minimum(w,alpha)\n",
                "    clipped = torch.maximum(clipped,-alpha)\n",
                "    return clipped\n",
                "\n",
                "def proxL1norm(w_hat, alpha, nonpenaltyidx):\n",
                "    if(torch.is_tensor(alpha)==False):\n",
                "        alpha = torch.tensor(alpha)\n",
                "    w = w_hat - clip(w_hat,alpha)\n",
                "    for idx in nonpenaltyidx:\n",
                "        w[idx] = w_hat[idx]\n",
                "    return w"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "def training_loop(c,coef, prevcoef, UpsilonR, UpsilonL, xdot, bs, lr, lam, momentum=True):\n",
                "    loss_list = []\n",
                "    tl = xdot.shape[0]\n",
                "    n = xdot.shape[1]\n",
                "\n",
                "    if(torch.is_tensor(xdot)==False):\n",
                "        xdot = torch.from_numpy(xdot).to(device).float()\n",
                "    \n",
                "    v = coef.clone().detach().requires_grad_(True)\n",
                "    prev = v\n",
                "    \n",
                "    for i in range(tl//bs):\n",
                "                \n",
                "        #computing acceleration with momentum\n",
                "        if(momentum==True):\n",
                "            vhat = (v + ((i-1)/(i+2))*(v - prev)).clone().detach().requires_grad_(True)\n",
                "        else:\n",
                "            vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
                "   \n",
                "        prev = v\n",
                "\n",
                "        #Computing loss\n",
                "        upsilonL = -UpsilonL[:,:,i*bs:(i+1)*bs]\n",
                "        upsilonR = UpsilonR[:,:,i*bs:(i+1)*bs]\n",
                " \n",
                "        x_t = xdot[i*bs:(i+1)*bs,:]\n",
                "\n",
                "        #forward\n",
                "        pred = torch.einsum('jkl,k->jl', upsilonL, vhat)\n",
                "        targ = torch.einsum('jkl,k->jl', upsilonR, c)\n",
                "        \n",
                "        lossval = loss(pred, targ)\n",
                "        \n",
                "        #Backpropagation\n",
                "        lossval.backward()\n",
                "\n",
                "        with torch.no_grad():\n",
                "            v = vhat - lr*vhat.grad\n",
                "            v = (proxL1norm(v,lr*lam,nonpenaltyidx))\n",
                "            \n",
                "            # Manually zero the gradients after updating weights\n",
                "            vhat.grad = None\n",
                "        \n",
                "        \n",
                "    \n",
                "        \n",
                "        loss_list.append(lossval.item())\n",
                "    print(\"Average loss : \" , torch.tensor(loss_list).mean().item())\n",
                "    return v, prevcoef, torch.tensor(loss_list).mean().item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "Epoch 0/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  108.38902282714844\n",
                        "\n",
                        "\n",
                        "Epoch 1/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  13.439009666442871\n",
                        "\n",
                        "\n",
                        "Epoch 2/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  7.117554187774658\n",
                        "\n",
                        "\n",
                        "Epoch 3/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  5.103173732757568\n",
                        "\n",
                        "\n",
                        "Epoch 4/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  4.059601306915283\n",
                        "\n",
                        "\n",
                        "Epoch 5/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  3.3657286167144775\n",
                        "\n",
                        "\n",
                        "Epoch 6/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  2.8567605018615723\n",
                        "\n",
                        "\n",
                        "Epoch 7/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  2.463046073913574\n",
                        "\n",
                        "\n",
                        "Epoch 8/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  2.1387927532196045\n",
                        "\n",
                        "\n",
                        "Epoch 9/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  1.8678442239761353\n",
                        "\n",
                        "\n",
                        "Epoch 10/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  1.6393556594848633\n",
                        "\n",
                        "\n",
                        "Epoch 11/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  1.4451954364776611\n",
                        "\n",
                        "\n",
                        "Epoch 12/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  1.2794568538665771\n",
                        "\n",
                        "\n",
                        "Epoch 13/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  1.1353695392608643\n",
                        "\n",
                        "\n",
                        "Epoch 14/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  1.0104444026947021\n",
                        "\n",
                        "\n",
                        "Epoch 15/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.90419602394104\n",
                        "\n",
                        "\n",
                        "Epoch 16/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.8125414848327637\n",
                        "\n",
                        "\n",
                        "Epoch 17/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.732291042804718\n",
                        "\n",
                        "\n",
                        "Epoch 18/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.663092851638794\n",
                        "\n",
                        "\n",
                        "Epoch 19/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.6035950779914856\n",
                        "\n",
                        "\n",
                        "Epoch 20/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.5522337555885315\n",
                        "\n",
                        "\n",
                        "Epoch 21/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.5073184967041016\n",
                        "\n",
                        "\n",
                        "Epoch 22/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.46835923194885254\n",
                        "\n",
                        "\n",
                        "Epoch 23/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.4352430999279022\n",
                        "\n",
                        "\n",
                        "Epoch 24/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.4059198200702667\n",
                        "\n",
                        "\n",
                        "Epoch 25/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.3799940049648285\n",
                        "\n",
                        "\n",
                        "Epoch 26/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.3563125729560852\n",
                        "\n",
                        "\n",
                        "Epoch 27/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.33472877740859985\n",
                        "\n",
                        "\n",
                        "Epoch 28/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.31461015343666077\n",
                        "\n",
                        "\n",
                        "Epoch 29/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.2960362732410431\n",
                        "\n",
                        "\n",
                        "Epoch 30/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.27905189990997314\n",
                        "\n",
                        "\n",
                        "Epoch 31/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.2627696394920349\n",
                        "\n",
                        "\n",
                        "Epoch 32/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.24768491089344025\n",
                        "\n",
                        "\n",
                        "Epoch 33/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.23287531733512878\n",
                        "\n",
                        "\n",
                        "Epoch 34/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.21903260052204132\n",
                        "\n",
                        "\n",
                        "Epoch 35/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.20641547441482544\n",
                        "\n",
                        "\n",
                        "Epoch 36/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.19478870928287506\n",
                        "\n",
                        "\n",
                        "Epoch 37/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.18364441394805908\n",
                        "\n",
                        "\n",
                        "Epoch 38/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.1730566918849945\n",
                        "\n",
                        "\n",
                        "Epoch 39/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.16348174214363098\n",
                        "\n",
                        "\n",
                        "Epoch 40/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.1547599881887436\n",
                        "\n",
                        "\n",
                        "Epoch 41/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.14670367538928986\n",
                        "\n",
                        "\n",
                        "Epoch 42/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.1393568217754364\n",
                        "\n",
                        "\n",
                        "Epoch 43/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.13243821263313293\n",
                        "\n",
                        "\n",
                        "Epoch 44/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.1261635273694992\n",
                        "\n",
                        "\n",
                        "Epoch 45/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.12030217796564102\n",
                        "\n",
                        "\n",
                        "Epoch 46/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.11493717133998871\n",
                        "\n",
                        "\n",
                        "Epoch 47/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.10987972468137741\n",
                        "\n",
                        "\n",
                        "Epoch 48/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.1051667109131813\n",
                        "\n",
                        "\n",
                        "Epoch 49/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.09964042156934738\n",
                        "\n",
                        "\n",
                        "Epoch 50/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.09334051609039307\n",
                        "\n",
                        "\n",
                        "Epoch 51/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.08735930919647217\n",
                        "\n",
                        "\n",
                        "Epoch 52/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.0819595605134964\n",
                        "\n",
                        "\n",
                        "Epoch 53/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.07695334404706955\n",
                        "\n",
                        "\n",
                        "Epoch 54/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.07233987003564835\n",
                        "\n",
                        "\n",
                        "Epoch 55/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.06812440603971481\n",
                        "\n",
                        "\n",
                        "Epoch 56/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.06426645070314407\n",
                        "\n",
                        "\n",
                        "Epoch 57/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.06074526533484459\n",
                        "\n",
                        "\n",
                        "Epoch 58/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.057537831366062164\n",
                        "\n",
                        "\n",
                        "Epoch 59/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.054565075784921646\n",
                        "\n",
                        "\n",
                        "Epoch 60/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.05182569473981857\n",
                        "\n",
                        "\n",
                        "Epoch 61/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.04895588383078575\n",
                        "\n",
                        "\n",
                        "Epoch 62/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.046155400574207306\n",
                        "\n",
                        "\n",
                        "Epoch 63/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.0438656248152256\n",
                        "\n",
                        "\n",
                        "Epoch 64/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.04178044572472572\n",
                        "\n",
                        "\n",
                        "Epoch 65/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.039889145642519\n",
                        "\n",
                        "\n",
                        "Epoch 66/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.03813664987683296\n",
                        "\n",
                        "\n",
                        "Epoch 67/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.03653399646282196\n",
                        "\n",
                        "\n",
                        "Epoch 68/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.03507395461201668\n",
                        "\n",
                        "\n",
                        "Epoch 69/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.03371378779411316\n",
                        "\n",
                        "\n",
                        "Epoch 70/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.0324687696993351\n",
                        "\n",
                        "\n",
                        "Epoch 71/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.031308047473430634\n",
                        "\n",
                        "\n",
                        "Epoch 72/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.030259009450674057\n",
                        "\n",
                        "\n",
                        "Epoch 73/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.02925277128815651\n",
                        "\n",
                        "\n",
                        "Epoch 74/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.02833811193704605\n",
                        "\n",
                        "\n",
                        "Epoch 75/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.02750399336218834\n",
                        "\n",
                        "\n",
                        "Epoch 76/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.026681557297706604\n",
                        "\n",
                        "\n",
                        "Epoch 77/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.025925638154149055\n",
                        "\n",
                        "\n",
                        "Epoch 78/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.0252221766859293\n",
                        "\n",
                        "\n",
                        "Epoch 79/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.02456303872168064\n",
                        "\n",
                        "\n",
                        "Epoch 80/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.023931650444865227\n",
                        "\n",
                        "\n",
                        "Epoch 81/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.023400163277983665\n",
                        "\n",
                        "\n",
                        "Epoch 82/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.022917011752724648\n",
                        "\n",
                        "\n",
                        "Epoch 83/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.022415200248360634\n",
                        "\n",
                        "\n",
                        "Epoch 84/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.021999647840857506\n",
                        "\n",
                        "\n",
                        "Epoch 85/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.021546276286244392\n",
                        "\n",
                        "\n",
                        "Epoch 86/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.021215327084064484\n",
                        "\n",
                        "\n",
                        "Epoch 87/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.020815908908843994\n",
                        "\n",
                        "\n",
                        "Epoch 88/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.02049999311566353\n",
                        "\n",
                        "\n",
                        "Epoch 89/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.02015957236289978\n",
                        "\n",
                        "\n",
                        "Epoch 90/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01986526884138584\n",
                        "\n",
                        "\n",
                        "Epoch 91/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.019588535651564598\n",
                        "\n",
                        "\n",
                        "Epoch 92/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01935720257461071\n",
                        "\n",
                        "\n",
                        "Epoch 93/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.019128946587443352\n",
                        "\n",
                        "\n",
                        "Epoch 94/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01889701746404171\n",
                        "\n",
                        "\n",
                        "Epoch 95/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.018600940704345703\n",
                        "\n",
                        "\n",
                        "Epoch 96/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01836295798420906\n",
                        "\n",
                        "\n",
                        "Epoch 97/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.018151499330997467\n",
                        "\n",
                        "\n",
                        "Epoch 98/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.017989536747336388\n",
                        "\n",
                        "\n",
                        "Epoch 99/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01783774606883526\n",
                        "\n",
                        "\n",
                        "Epoch 100/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01764609105885029\n",
                        "\n",
                        "\n",
                        "Epoch 101/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01749810017645359\n",
                        "\n",
                        "\n",
                        "Epoch 102/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.017334474250674248\n",
                        "\n",
                        "\n",
                        "Epoch 103/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.017124099656939507\n",
                        "\n",
                        "\n",
                        "Epoch 104/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.016858743503689766\n",
                        "\n",
                        "\n",
                        "Epoch 105/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.016912225633859634\n",
                        "\n",
                        "\n",
                        "Epoch 106/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.016797613352537155\n",
                        "\n",
                        "\n",
                        "Epoch 107/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01653299666941166\n",
                        "\n",
                        "\n",
                        "Epoch 108/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.016592876985669136\n",
                        "\n",
                        "\n",
                        "Epoch 109/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.016466006636619568\n",
                        "\n",
                        "\n",
                        "Epoch 110/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01619207113981247\n",
                        "\n",
                        "\n",
                        "Epoch 111/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01635221391916275\n",
                        "\n",
                        "\n",
                        "Epoch 112/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01618685945868492\n",
                        "\n",
                        "\n",
                        "Epoch 113/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.016117947176098824\n",
                        "\n",
                        "\n",
                        "Epoch 114/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.016012215986847878\n",
                        "\n",
                        "\n",
                        "Epoch 115/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015948904678225517\n",
                        "\n",
                        "\n",
                        "Epoch 116/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01596575416624546\n",
                        "\n",
                        "\n",
                        "Epoch 117/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01570521853864193\n",
                        "\n",
                        "\n",
                        "Epoch 118/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01592368446290493\n",
                        "\n",
                        "\n",
                        "Epoch 119/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015816839411854744\n",
                        "\n",
                        "\n",
                        "Epoch 120/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015622244216501713\n",
                        "\n",
                        "\n",
                        "Epoch 121/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015762243419885635\n",
                        "\n",
                        "\n",
                        "Epoch 122/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015583891421556473\n",
                        "\n",
                        "\n",
                        "Epoch 123/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015701889991760254\n",
                        "\n",
                        "\n",
                        "Epoch 124/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015494021587073803\n",
                        "\n",
                        "\n",
                        "Epoch 125/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015622002072632313\n",
                        "\n",
                        "\n",
                        "Epoch 126/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015511104837059975\n",
                        "\n",
                        "\n",
                        "Epoch 127/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015603628940880299\n",
                        "\n",
                        "\n",
                        "Epoch 128/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015404109843075275\n",
                        "\n",
                        "\n",
                        "Epoch 129/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015567048452794552\n",
                        "\n",
                        "\n",
                        "Epoch 130/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015364127233624458\n",
                        "\n",
                        "\n",
                        "Epoch 131/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015524530783295631\n",
                        "\n",
                        "\n",
                        "Epoch 132/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01534231100231409\n",
                        "\n",
                        "\n",
                        "Epoch 133/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.0154874287545681\n",
                        "\n",
                        "\n",
                        "Epoch 134/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015361813828349113\n",
                        "\n",
                        "\n",
                        "Epoch 135/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015455953776836395\n",
                        "\n",
                        "\n",
                        "Epoch 136/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01525923516601324\n",
                        "\n",
                        "\n",
                        "Epoch 137/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01545299869030714\n",
                        "\n",
                        "\n",
                        "Epoch 138/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015307413414120674\n",
                        "\n",
                        "\n",
                        "Epoch 139/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015426376834511757\n",
                        "\n",
                        "\n",
                        "Epoch 140/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015221788547933102\n",
                        "\n",
                        "\n",
                        "Epoch 141/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015407614409923553\n",
                        "\n",
                        "\n",
                        "Epoch 142/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015264089219272137\n",
                        "\n",
                        "\n",
                        "Epoch 143/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015386228449642658\n",
                        "\n",
                        "\n",
                        "Epoch 144/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015178458765149117\n",
                        "\n",
                        "\n",
                        "Epoch 145/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015400523319840431\n",
                        "\n",
                        "\n",
                        "Epoch 146/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015252192504703999\n",
                        "\n",
                        "\n",
                        "Epoch 147/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015344792045652866\n",
                        "\n",
                        "\n",
                        "Epoch 148/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015157006680965424\n",
                        "\n",
                        "\n",
                        "Epoch 149/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015350034460425377\n",
                        "\n",
                        "\n",
                        "Epoch 150/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015222468413412571\n",
                        "\n",
                        "\n",
                        "Epoch 151/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015341954305768013\n",
                        "\n",
                        "\n",
                        "Epoch 152/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015165376476943493\n",
                        "\n",
                        "\n",
                        "Epoch 153/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015342059545218945\n",
                        "\n",
                        "\n",
                        "Epoch 154/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015221481211483479\n",
                        "\n",
                        "\n",
                        "Epoch 155/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015303839929401875\n",
                        "\n",
                        "\n",
                        "Epoch 156/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015106618404388428\n",
                        "\n",
                        "\n",
                        "Epoch 157/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015330493450164795\n",
                        "\n",
                        "\n",
                        "Epoch 158/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015203982591629028\n",
                        "\n",
                        "\n",
                        "Epoch 159/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015287199057638645\n",
                        "\n",
                        "\n",
                        "Epoch 160/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015086056664586067\n",
                        "\n",
                        "\n",
                        "Epoch 161/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015294261276721954\n",
                        "\n",
                        "\n",
                        "Epoch 162/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.0151720205321908\n",
                        "\n",
                        "\n",
                        "Epoch 163/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015269670635461807\n",
                        "\n",
                        "\n",
                        "Epoch 164/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015094028785824776\n",
                        "\n",
                        "\n",
                        "Epoch 165/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015304201282560825\n",
                        "\n",
                        "\n",
                        "Epoch 166/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015158994123339653\n",
                        "\n",
                        "\n",
                        "Epoch 167/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01528140902519226\n",
                        "\n",
                        "\n",
                        "Epoch 168/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015028383582830429\n",
                        "\n",
                        "\n",
                        "Epoch 169/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015288790687918663\n",
                        "\n",
                        "\n",
                        "Epoch 170/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015113663859665394\n",
                        "\n",
                        "\n",
                        "Epoch 171/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015262640081346035\n",
                        "\n",
                        "\n",
                        "Epoch 172/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015046216547489166\n",
                        "\n",
                        "\n",
                        "Epoch 173/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01530080009251833\n",
                        "\n",
                        "\n",
                        "Epoch 174/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015174386091530323\n",
                        "\n",
                        "\n",
                        "Epoch 175/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01527476403862238\n",
                        "\n",
                        "\n",
                        "Epoch 176/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015037789940834045\n",
                        "\n",
                        "\n",
                        "Epoch 177/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015272285789251328\n",
                        "\n",
                        "\n",
                        "Epoch 178/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015147194266319275\n",
                        "\n",
                        "\n",
                        "Epoch 179/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015225140377879143\n",
                        "\n",
                        "\n",
                        "Epoch 180/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015050236135721207\n",
                        "\n",
                        "\n",
                        "Epoch 181/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015277471393346786\n",
                        "\n",
                        "\n",
                        "Epoch 182/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015152985230088234\n",
                        "\n",
                        "\n",
                        "Epoch 183/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015267695300281048\n",
                        "\n",
                        "\n",
                        "Epoch 184/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01503513753414154\n",
                        "\n",
                        "\n",
                        "Epoch 185/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015275530517101288\n",
                        "\n",
                        "\n",
                        "Epoch 186/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015119777992367744\n",
                        "\n",
                        "\n",
                        "Epoch 187/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015251174569129944\n",
                        "\n",
                        "\n",
                        "Epoch 188/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015026677399873734\n",
                        "\n",
                        "\n",
                        "Epoch 189/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015277381986379623\n",
                        "\n",
                        "\n",
                        "Epoch 190/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015138420276343822\n",
                        "\n",
                        "\n",
                        "Epoch 191/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01522877812385559\n",
                        "\n",
                        "\n",
                        "Epoch 192/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015000011771917343\n",
                        "\n",
                        "\n",
                        "Epoch 193/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015263647772371769\n",
                        "\n",
                        "\n",
                        "Epoch 194/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015108101069927216\n",
                        "\n",
                        "\n",
                        "Epoch 195/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015223940834403038\n",
                        "\n",
                        "\n",
                        "Epoch 196/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015007569454610348\n",
                        "\n",
                        "\n",
                        "Epoch 197/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015267972834408283\n",
                        "\n",
                        "\n",
                        "Epoch 198/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015113160945475101\n",
                        "\n",
                        "\n",
                        "Epoch 199/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01522121299058199\n",
                        "\n",
                        "\n",
                        "Epoch 200/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015013818629086018\n",
                        "\n",
                        "\n",
                        "Epoch 201/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015257644467055798\n",
                        "\n",
                        "\n",
                        "Epoch 202/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015064934268593788\n",
                        "\n",
                        "\n",
                        "Epoch 203/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01524276938289404\n",
                        "\n",
                        "\n",
                        "Epoch 204/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015033222734928131\n",
                        "\n",
                        "\n",
                        "Epoch 205/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.01526985876262188\n",
                        "\n",
                        "\n",
                        "Epoch 206/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015058610588312149\n",
                        "\n",
                        "\n",
                        "Epoch 207/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015245527029037476\n",
                        "\n",
                        "\n",
                        "Epoch 208/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015017746947705746\n",
                        "\n",
                        "\n",
                        "Epoch 209/5000\n",
                        "Learning rate :  0.01\n",
                        "Average loss :  0.015280699357390404\n",
                        "\n",
                        "\n",
                        "Epoch 210/5000\n",
                        "Learning rate :  0.01\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-16-441265f44c69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEpoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Learning rate : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mxi_L\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevxi_L\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlossitem\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxi_L\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprevxi_L\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mUpsilonR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mUpsilonL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXdot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlam\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m<ipython-input-15-ce4b7c1199c8>\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(c, coef, prevcoef, UpsilonR, UpsilonL, xdot, bs, lr, lam, momentum)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mlossval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "Epoch = 5000\n",
                "i = 0\n",
                "lr = 1e-2\n",
                "lam = 0.005\n",
                "temp = 1000\n",
                "while(i<=Epoch):\n",
                "    print(\"\\n\")\n",
                "    print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
                "    print(\"Learning rate : \", lr)\n",
                "    xi_L, prevxi_L, lossitem= training_loop(c, xi_L,prevxi_L,UpsilonR,UpsilonL,Xdot,128,lr=lr,lam=lam,momentum=true)\n",
                "    temp = lossitem\n",
                "    i+=1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Rescale\n",
                "for i in range(UpsilonL.shape[1]):\n",
                "    xi_L[i] = xi_L[i]*scale[i]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([ 0.0000e+00,  0.0000e+00,  2.5754e+00,  4.3693e+04,  1.7815e-02,\n",
                            "        -1.9480e-04,  3.2537e+01,  1.5742e+01,  7.6385e-02,  1.3015e-01,\n",
                            "         5.3211e-02,  3.2463e-02,  2.6711e-02, -7.6385e-02,  7.5675e-03,\n",
                            "        -3.0016e-05,  2.6630e-01, -5.3211e-02,  0.0000e+00, -1.7889e-01,\n",
                            "         0.0000e+00, -3.1443e-01,  0.0000e+00, -3.8776e-01,  0.0000e+00,\n",
                            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.9301e-01,\n",
                            "         9.9382e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.0242e-02,\n",
                            "         0.0000e+00,  1.3098e-01,  9.5228e-02,  0.0000e+00,  2.6271e-02,\n",
                            "         0.0000e+00, -1.3098e-01,  0.0000e+00,  0.0000e+00, -3.8134e-01,\n",
                            "         0.0000e+00, -2.0657e-01,  0.0000e+00, -1.2564e+03, -4.6616e+01,\n",
                            "        -2.9815e+04, -1.5595e-01, -7.7422e+03,  4.4867e-02,  0.0000e+00,\n",
                            "        -8.1482e+04,  0.0000e+00,  0.0000e+00, -5.7781e+00, -1.4709e+04,\n",
                            "         0.0000e+00, -1.3001e-02,  1.3083e+01,  9.9944e+00,  0.0000e+00,\n",
                            "         2.8014e+04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.4745e-02,\n",
                            "         0.0000e+00,  0.0000e+00,  4.7891e+04,  0.0000e+00,  0.0000e+00,\n",
                            "         0.0000e+00,  0.0000e+00], device='cuda:0')"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "xi_L"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "77"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(expr)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([77])"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "xi_L.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "d5784be8b0ed123205c521437a438df309f2d2f16cb6cf8124a1b3e0f87bfce1"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 64-bit ('SystemIdentification': conda)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
