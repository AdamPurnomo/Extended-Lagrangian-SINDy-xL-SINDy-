{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import sys \n",
                "sys.path.append(r'../../Python Script/')\n",
                "\n",
                "from sympy import symbols, simplify, derive_by_array\n",
                "from scipy.integrate import solve_ivp\n",
                "from xLSINDy import *\n",
                "from sympy.physics.mechanics import *\n",
                "from sympy import *\n",
                "import sympy\n",
                "import torch\n",
                "import HLsearch as HL\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "def generate_data(func, time, init_values):\n",
                "    sol = solve_ivp(func,[time[0],time[-1]],init_values,t_eval=time, method='RK45',rtol=1e-10,atol=1e-10)\n",
                "    return sol.y.T, np.array([func(0,sol.y.T[i,:]) for i in range(sol.y.T.shape[0])],dtype=np.float64)\n",
                "\n",
                "def pendulum(t,x):\n",
                "    return x[1],-9.81*np.sin(x[0])\n",
                "\n",
                "# Pendulum rod lengths (m), bob masses (kg).\n",
                "L1, L2 = 1, 1\n",
                "m1, m2 = 1, 1\n",
                "# The gravitational acceleration (m.s-2).\n",
                "g = 9.81\n",
                "tau = 0\n",
                "\n",
                "def doublePendulum(t,y,M=0.0):\n",
                "    q1,q2,q1_t,q2_t = y\n",
                "    q1_2t = (-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L2*g*m2*np.sin(q2) + m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t)/2 - m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t + 2*L2*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q2) - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t + 2*L2*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q2))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2)/(2*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)))/(2*(-m2**2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))**2/(4*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)) + m2*(2*L2**2*np.sin(q2)**2 + 2*L2**2*np.cos(q2)**2)/2)))/(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)\n",
                "    q2_2t = (-L2*g*m2*np.sin(q2) + m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t)/2 - m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t + 2*L2*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q2) - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t + 2*L2*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q2))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2)/(2*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)))/(-m2**2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))**2/(4*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)) + m2*(2*L2**2*np.sin(q2)**2 + 2*L2**2*np.cos(q2)**2)/2)\n",
                "    return q1_t,q2_t,q1_2t,q2_2t"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Saving Directory\n",
                "rootdir = \"../../Double Pendulum/Data/\"\n",
                "\n",
                "num_sample = 100\n",
                "create_data = False\n",
                "training = True\n",
                "save = False\n",
                "noiselevel = 1e-1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "if(create_data):\n",
                "    print(\"Creating Data\")\n",
                "    num_sample = 100\n",
                "    X, Xdot = [], []\n",
                "    for i in range(num_sample):\n",
                "        t = np.arange(0,5,0.01)\n",
                "        theta1 = np.random.uniform(-np.pi, np.pi)\n",
                "        thetadot = np.random.uniform(0,0)\n",
                "        theta2 = np.random.uniform(-np.pi, np.pi)\n",
                "        \n",
                "        y0=np.array([theta1, theta2, thetadot, thetadot])\n",
                "        x,xdot = generate_data(doublePendulum,t,y0)\n",
                "        X.append(x)\n",
                "        Xdot.append(xdot)\n",
                "    X = np.vstack(X)\n",
                "    Xdot = np.vstack(Xdot)\n",
                "    if(save==True):\n",
                "        np.save(rootdir + \"X.npy\", X)\n",
                "        np.save(rootdir + \"Xdot.npy\",Xdot)\n",
                "else:\n",
                "    X = np.load(rootdir + \"X.npy\")\n",
                "    Xdot = np.load(rootdir + \"Xdot.npy\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "#adding noise\n",
                "mu, sigma = 0, noiselevel\n",
                "noise = np.random.normal(mu, sigma, X.shape[0])\n",
                "for i in range(X.shape[1]):\n",
                "    X[:,i] = X[:,i]+noise\n",
                "    Xdot[:,i] = Xdot[:,i]+noise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "states are: (x0, x1, x0_t, x1_t)\n",
                        "states derivatives are:  (x0_t, x1_t, x0_tt, x1_tt)\n"
                    ]
                }
            ],
            "source": [
                "states_dim = 4\n",
                "states = ()\n",
                "states_dot = ()\n",
                "for i in range(states_dim):\n",
                "    if(i<states_dim//2):\n",
                "        states = states + (symbols('x{}'.format(i)),)\n",
                "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
                "    else:\n",
                "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
                "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
                "print('states are:',states)\n",
                "print('states derivatives are: ', states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Turn from sympy to str\n",
                "states_sym = states\n",
                "states_dot_sym = states_dot\n",
                "states = list(str(descr) for descr in states)\n",
                "states_dot = list(str(descr) for descr in states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "#build function expression for the library in str\n",
                "exprdummy = HL.buildFunctionExpressions(1,states_dim,states,use_sine=True)\n",
                "polynom = exprdummy[2:4]\n",
                "trig = exprdummy[4:]\n",
                "polynom = HL.buildFunctionExpressions(2,len(polynom),polynom)\n",
                "trig = HL.buildFunctionExpressions(2, len(trig),trig)\n",
                "product = []\n",
                "for p in polynom:\n",
                "    for t in trig:\n",
                "        product.append(p + '*' + t)\n",
                "expr = polynom + trig + product"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Boundaries for debugging with only the correct terms ###"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Creating library tensor\n",
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Debugging for only known terms\n",
                "\n",
                "expr = np.array(expr)\n",
                "i1 = np.where(expr == 'x0_t**2')[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "## Garbage terms ##\n",
                "\n",
                "'''\n",
                "Explanation :\n",
                "x0_t, x1_t terms are not needed and will always satisfy EL's equation.\n",
                "Since x0_t, x1_t are garbages, we want to avoid (x0_t*sin()**2 + x0_t*cos()**2), thus we remove\n",
                "one of them, either  x0_t*sin()**2 or x0_t*cos()**2. \n",
                "Since the known term is x0_t**2, we also want to avoid the solution of (x0_t**2*sin()**2 + x0_t**2*cos()**2),\n",
                "so we remove either one of x0_t**2*sin()**2 or x0_t**2*cos()**2.\n",
                "'''\n",
                "\n",
                "\n",
                "i2 = np.where(expr == 'x0_t**2*cos(x0)**2')[0]\n",
                "i3 = np.where(expr == 'x0_t**2*cos(x1)**2')[0]\n",
                "i7 = np.where(expr == 'x1_t*cos(x0)**2')[0]\n",
                "i8 = np.where(expr == 'x1_t*cos(x1)**2')[0]\n",
                "i9 = np.where(expr == 'x1_t')[0]\n",
                "i10 = np.where(expr == 'x0_t*cos(x0)**2')[0]\n",
                "i11 = np.where(expr == 'x0_t*cos(x1)**2')[0]\n",
                "i12 = np.where(expr == 'x0_t')[0]\n",
                "i13 = np.where(expr == 'cos(x0)**2')[0]\n",
                "i14 = np.where(expr == 'cos(x1)**2')[0]\n",
                "\n",
                "#Deleting unused terms and removing known terms \n",
                "idx = np.arange(0,len(expr))\n",
                "idx = np.delete(idx,[i1,i2,i3,i7,i8,i9,i10,i11,i12,i13,i14])\n",
                "known_expr = expr[i1].tolist()\n",
                "expr = np.delete(expr,[i1,i2,i3,i7,i8,i9,i10,i11,i12,i13,i14])\n",
                "\n",
                "\n",
                "#non-penalty index from prev knowledge\n",
                "i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "nonpenaltyidx = [i4, i5, i6]\n",
                "\n",
                "expr = expr.tolist()\n",
                "\n",
                "Zeta_ = Zeta[:,:,i1,:].clone().detach()\n",
                "Eta_ = Eta[:,:,i1,:].clone().detach()\n",
                "Delta_ = Delta[:,i1,:].clone().detach()\n",
                "\n",
                "Zeta = Zeta[:,:,idx,:]\n",
                "Eta = Eta[:,:,idx,:]\n",
                "Delta = Delta[:,idx,:]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Moving to Cuda\n",
                "device = 'cuda:0'\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n",
                "\n",
                "Zeta_ = Zeta_.to(device)\n",
                "Eta_ = Eta_.to(device)\n",
                "Delta_ = Delta_.to(device)\n",
                "\n",
                "UpsilonL = Upsilonforward(Zeta, Eta, Delta, Xdot, device)\n",
                "UpsilonR = Upsilonforward(Zeta_, Eta_, Delta_, Xdot, device)\n",
                "# scale = torch.ones(UpsilonL.shape[1])\n",
                "\n",
                "# # Scaling for upsilon\n",
                "# for i in range(UpsilonL.shape[1]):\n",
                "#    scale[i] = UpsilonL[:,i,:].max() - UpsilonL[:,i,:].min()\n",
                "#    if(scale[i]!=0):\n",
                "#        UpsilonL[:,i,:] = UpsilonL[:,i,:]/scale[i] \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Initialize coefficient\n",
                "\n",
                "xi_L = torch.zeros(len(expr), device=device).data.uniform_(-20,20)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "c = torch.ones(len(known_expr), device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "def loss(pred, targ):\n",
                "    loss = torch.mean((pred - targ)**2) \n",
                "    return loss "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clip(w, alpha):\n",
                "    clipped = torch.minimum(w,alpha)\n",
                "    clipped = torch.maximum(clipped,-alpha)\n",
                "    return clipped\n",
                "\n",
                "def proxL1norm(w_hat, alpha, nonpenaltyidx):\n",
                "    if(torch.is_tensor(alpha)==False):\n",
                "        alpha = torch.tensor(alpha)\n",
                "    w = w_hat - clip(w_hat,alpha)\n",
                "    for idx in nonpenaltyidx:\n",
                "        w[idx] = w_hat[idx]\n",
                "    return w"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "def training_loop(c,coef, prevcoef, UpsilonR, UpsilonL, xdot, bs, lr, lam, momentum=True):\n",
                "    loss_list = []\n",
                "    tl = xdot.shape[0]\n",
                "    n = xdot.shape[1]\n",
                "\n",
                "    if(torch.is_tensor(xdot)==False):\n",
                "        xdot = torch.from_numpy(xdot).to(device).float()\n",
                "    \n",
                "    v = coef.clone().detach().requires_grad_(True)\n",
                "    prev = v\n",
                "    \n",
                "    for i in range(tl//bs):\n",
                "                \n",
                "        #computing acceleration with momentum\n",
                "        if(momentum==True):\n",
                "            vhat = (v + ((i-1)/(i+2))*(v - prev)).clone().detach().requires_grad_(True)\n",
                "        else:\n",
                "            vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
                "   \n",
                "        prev = v\n",
                "\n",
                "        #Computing loss\n",
                "        upsilonL = UpsilonL[:,:,i*bs:(i+1)*bs]\n",
                "        upsilonR = UpsilonR[:,:,i*bs:(i+1)*bs]\n",
                " \n",
                "        #x_t = xdot[i*bs:(i+1)*bs,:]\n",
                "\n",
                "        #forward\n",
                "        pred = torch.einsum('jkl,k->jl', upsilonL, vhat)\n",
                "        targ = -torch.einsum('jkl,k->jl', upsilonR, c)\n",
                "        \n",
                "        lossval = loss(pred, targ)\n",
                "        \n",
                "        #Backpropagation\n",
                "        lossval.backward()\n",
                "\n",
                "        with torch.no_grad():\n",
                "            v = vhat - lr*vhat.grad\n",
                "            v = (proxL1norm(v,lr*lam,nonpenaltyidx))\n",
                "            \n",
                "            # Manually zero the gradients after updating weights\n",
                "            vhat.grad = None\n",
                "        \n",
                "        \n",
                "    \n",
                "        \n",
                "        loss_list.append(lossval.item())\n",
                "    print(\"Average loss : \" , torch.tensor(loss_list).mean().item())\n",
                "    return v, prevcoef, torch.tensor(loss_list).mean().item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "Epoch 0/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.561190366744995\n",
                        "\n",
                        "\n",
                        "Epoch 1/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.289092540740967\n",
                        "\n",
                        "\n",
                        "Epoch 2/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.1849002838134766\n",
                        "\n",
                        "\n",
                        "Epoch 3/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.1337974071502686\n",
                        "\n",
                        "\n",
                        "Epoch 4/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.090630292892456\n",
                        "\n",
                        "\n",
                        "Epoch 5/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0690395832061768\n",
                        "\n",
                        "\n",
                        "Epoch 6/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0667436122894287\n",
                        "\n",
                        "\n",
                        "Epoch 7/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0587010383605957\n",
                        "\n",
                        "\n",
                        "Epoch 8/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0580215454101562\n",
                        "\n",
                        "\n",
                        "Epoch 9/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0527122020721436\n",
                        "\n",
                        "\n",
                        "Epoch 10/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0481579303741455\n",
                        "\n",
                        "\n",
                        "Epoch 11/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.046666145324707\n",
                        "\n",
                        "\n",
                        "Epoch 12/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.04699969291687\n",
                        "\n",
                        "\n",
                        "Epoch 13/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0478694438934326\n",
                        "\n",
                        "\n",
                        "Epoch 14/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0382463932037354\n",
                        "\n",
                        "\n",
                        "Epoch 15/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0486745834350586\n",
                        "\n",
                        "\n",
                        "Epoch 16/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.045382022857666\n",
                        "\n",
                        "\n",
                        "Epoch 17/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0490264892578125\n",
                        "\n",
                        "\n",
                        "Epoch 18/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.045748710632324\n",
                        "\n",
                        "\n",
                        "Epoch 19/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0445613861083984\n",
                        "\n",
                        "\n",
                        "Epoch 20/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0377750396728516\n",
                        "\n",
                        "\n",
                        "Epoch 21/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0400173664093018\n",
                        "\n",
                        "\n",
                        "Epoch 22/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0445549488067627\n",
                        "\n",
                        "\n",
                        "Epoch 23/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.045699119567871\n",
                        "\n",
                        "\n",
                        "Epoch 24/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0457165241241455\n",
                        "\n",
                        "\n",
                        "Epoch 25/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0476396083831787\n",
                        "\n",
                        "\n",
                        "Epoch 26/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0482189655303955\n",
                        "\n",
                        "\n",
                        "Epoch 27/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.044663190841675\n",
                        "\n",
                        "\n",
                        "Epoch 28/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.037637233734131\n",
                        "\n",
                        "\n",
                        "Epoch 29/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0387256145477295\n",
                        "\n",
                        "\n",
                        "Epoch 30/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0385749340057373\n",
                        "\n",
                        "\n",
                        "Epoch 31/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0412473678588867\n",
                        "\n",
                        "\n",
                        "Epoch 32/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.038029193878174\n",
                        "\n",
                        "\n",
                        "Epoch 33/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0446202754974365\n",
                        "\n",
                        "\n",
                        "Epoch 34/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0379388332366943\n",
                        "\n",
                        "\n",
                        "Epoch 35/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.038013219833374\n",
                        "\n",
                        "\n",
                        "Epoch 36/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0458974838256836\n",
                        "\n",
                        "\n",
                        "Epoch 37/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.048961639404297\n",
                        "\n",
                        "\n",
                        "Epoch 38/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.03848934173584\n",
                        "\n",
                        "\n",
                        "Epoch 39/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.046492576599121\n",
                        "\n",
                        "\n",
                        "Epoch 40/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0458531379699707\n",
                        "\n",
                        "\n",
                        "Epoch 41/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.049586772918701\n",
                        "\n",
                        "\n",
                        "Epoch 42/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.03820538520813\n",
                        "\n",
                        "\n",
                        "Epoch 43/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.037893533706665\n",
                        "\n",
                        "\n",
                        "Epoch 44/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.044644355773926\n",
                        "\n",
                        "\n",
                        "Epoch 45/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.044755697250366\n",
                        "\n",
                        "\n",
                        "Epoch 46/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0382819175720215\n",
                        "\n",
                        "\n",
                        "Epoch 47/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.038210153579712\n",
                        "\n",
                        "\n",
                        "Epoch 48/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0478768348693848\n",
                        "\n",
                        "\n",
                        "Epoch 49/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.042109489440918\n",
                        "\n",
                        "\n",
                        "Epoch 50/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.042203664779663\n",
                        "\n",
                        "\n",
                        "Epoch 51/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0391674041748047\n",
                        "\n",
                        "\n",
                        "Epoch 52/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0455007553100586\n",
                        "\n",
                        "\n",
                        "Epoch 53/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0456581115722656\n",
                        "\n",
                        "\n",
                        "Epoch 54/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.039405345916748\n",
                        "\n",
                        "\n",
                        "Epoch 55/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0391714572906494\n",
                        "\n",
                        "\n",
                        "Epoch 56/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.044111490249634\n",
                        "\n",
                        "\n",
                        "Epoch 57/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0391557216644287\n",
                        "\n",
                        "\n",
                        "Epoch 58/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0390772819519043\n",
                        "\n",
                        "\n",
                        "Epoch 59/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.045976161956787\n",
                        "\n",
                        "\n",
                        "Epoch 60/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.044663906097412\n",
                        "\n",
                        "\n",
                        "Epoch 61/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.039503574371338\n",
                        "\n",
                        "\n",
                        "Epoch 62/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0449514389038086\n",
                        "\n",
                        "\n",
                        "Epoch 63/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0389413833618164\n",
                        "\n",
                        "\n",
                        "Epoch 64/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0463109016418457\n",
                        "\n",
                        "\n",
                        "Epoch 65/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0459179878234863\n",
                        "\n",
                        "\n",
                        "Epoch 66/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0506319999694824\n",
                        "\n",
                        "\n",
                        "Epoch 67/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0445430278778076\n",
                        "\n",
                        "\n",
                        "Epoch 68/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.041736364364624\n",
                        "\n",
                        "\n",
                        "Epoch 69/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0381886959075928\n",
                        "\n",
                        "\n",
                        "Epoch 70/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0444743633270264\n",
                        "\n",
                        "\n",
                        "Epoch 71/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.042365312576294\n",
                        "\n",
                        "\n",
                        "Epoch 72/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0455288887023926\n",
                        "\n",
                        "\n",
                        "Epoch 73/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0416033267974854\n",
                        "\n",
                        "\n",
                        "Epoch 74/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0470612049102783\n",
                        "\n",
                        "\n",
                        "Epoch 75/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.045858144760132\n",
                        "\n",
                        "\n",
                        "Epoch 76/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.04461669921875\n",
                        "\n",
                        "\n",
                        "Epoch 77/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.038114309310913\n",
                        "\n",
                        "\n",
                        "Epoch 78/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.037806987762451\n",
                        "\n",
                        "\n",
                        "Epoch 79/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0490989685058594\n",
                        "\n",
                        "\n",
                        "Epoch 80/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0454769134521484\n",
                        "\n",
                        "\n",
                        "Epoch 81/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.04002046585083\n",
                        "\n",
                        "\n",
                        "Epoch 82/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.038144588470459\n",
                        "\n",
                        "\n",
                        "Epoch 83/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.048994541168213\n",
                        "\n",
                        "\n",
                        "Epoch 84/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.045379400253296\n",
                        "\n",
                        "\n",
                        "Epoch 85/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.04547119140625\n",
                        "\n",
                        "\n",
                        "Epoch 86/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.045299530029297\n",
                        "\n",
                        "\n",
                        "Epoch 87/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0452165603637695\n",
                        "\n",
                        "\n",
                        "Epoch 88/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.046548366546631\n",
                        "\n",
                        "\n",
                        "Epoch 89/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.04506516456604\n",
                        "\n",
                        "\n",
                        "Epoch 90/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0481958389282227\n",
                        "\n",
                        "\n",
                        "Epoch 91/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.049159049987793\n",
                        "\n",
                        "\n",
                        "Epoch 92/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.037904977798462\n",
                        "\n",
                        "\n",
                        "Epoch 93/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.048344135284424\n",
                        "\n",
                        "\n",
                        "Epoch 94/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0434882640838623\n",
                        "\n",
                        "\n",
                        "Epoch 95/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0414960384368896\n",
                        "\n",
                        "\n",
                        "Epoch 96/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0418879985809326\n",
                        "\n",
                        "\n",
                        "Epoch 97/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0404889583587646\n",
                        "\n",
                        "\n",
                        "Epoch 98/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0397486686706543\n",
                        "\n",
                        "\n",
                        "Epoch 99/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0479278564453125\n",
                        "\n",
                        "\n",
                        "Epoch 100/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0410499572753906\n",
                        "\n",
                        "\n",
                        "Epoch 101/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.044445514678955\n",
                        "\n",
                        "\n",
                        "Epoch 102/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.037705898284912\n",
                        "\n",
                        "\n",
                        "Epoch 103/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.044264316558838\n",
                        "\n",
                        "\n",
                        "Epoch 104/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0474302768707275\n",
                        "\n",
                        "\n",
                        "Epoch 105/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.043086528778076\n",
                        "\n",
                        "\n",
                        "Epoch 106/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.045776128768921\n",
                        "\n",
                        "\n",
                        "Epoch 107/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0382304191589355\n",
                        "\n",
                        "\n",
                        "Epoch 108/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0386250019073486\n",
                        "\n",
                        "\n",
                        "Epoch 109/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.044602155685425\n",
                        "\n",
                        "\n",
                        "Epoch 110/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0410521030426025\n",
                        "\n",
                        "\n",
                        "Epoch 111/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0446226596832275\n",
                        "\n",
                        "\n",
                        "Epoch 112/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0484397411346436\n",
                        "\n",
                        "\n",
                        "Epoch 113/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0485568046569824\n",
                        "\n",
                        "\n",
                        "Epoch 114/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0456604957580566\n",
                        "\n",
                        "\n",
                        "Epoch 115/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0437605381011963\n",
                        "\n",
                        "\n",
                        "Epoch 116/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.048513174057007\n",
                        "\n",
                        "\n",
                        "Epoch 117/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.040546178817749\n",
                        "\n",
                        "\n",
                        "Epoch 118/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.046661138534546\n",
                        "\n",
                        "\n",
                        "Epoch 119/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0467886924743652\n",
                        "\n",
                        "\n",
                        "Epoch 120/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0487818717956543\n",
                        "\n",
                        "\n",
                        "Epoch 121/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0463643074035645\n",
                        "\n",
                        "\n",
                        "Epoch 122/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0426323413848877\n",
                        "\n",
                        "\n",
                        "Epoch 123/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.042417287826538\n",
                        "\n",
                        "\n",
                        "Epoch 124/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.043668508529663\n",
                        "\n",
                        "\n",
                        "Epoch 125/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0455217361450195\n",
                        "\n",
                        "\n",
                        "Epoch 126/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.043023109436035\n",
                        "\n",
                        "\n",
                        "Epoch 127/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0390541553497314\n",
                        "\n",
                        "\n",
                        "Epoch 128/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0453429222106934\n",
                        "\n",
                        "\n",
                        "Epoch 129/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.048474073410034\n",
                        "\n",
                        "\n",
                        "Epoch 130/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.045807123184204\n",
                        "\n",
                        "\n",
                        "Epoch 131/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.04960298538208\n",
                        "\n",
                        "\n",
                        "Epoch 132/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0455150604248047\n",
                        "\n",
                        "\n",
                        "Epoch 133/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0484795570373535\n",
                        "\n",
                        "\n",
                        "Epoch 134/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0422582626342773\n",
                        "\n",
                        "\n",
                        "Epoch 135/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0448830127716064\n",
                        "\n",
                        "\n",
                        "Epoch 136/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0429189205169678\n",
                        "\n",
                        "\n",
                        "Epoch 137/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.044041395187378\n",
                        "\n",
                        "\n",
                        "Epoch 138/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0438010692596436\n",
                        "\n",
                        "\n",
                        "Epoch 139/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0469584465026855\n",
                        "\n",
                        "\n",
                        "Epoch 140/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0470690727233887\n",
                        "\n",
                        "\n",
                        "Epoch 141/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.046462059020996\n",
                        "\n",
                        "\n",
                        "Epoch 142/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0445914268493652\n",
                        "\n",
                        "\n",
                        "Epoch 143/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0426480770111084\n",
                        "\n",
                        "\n",
                        "Epoch 144/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0416901111602783\n",
                        "\n",
                        "\n",
                        "Epoch 145/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0391886234283447\n",
                        "\n",
                        "\n",
                        "Epoch 146/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0456604957580566\n",
                        "\n",
                        "\n",
                        "Epoch 147/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.039747714996338\n",
                        "\n",
                        "\n",
                        "Epoch 148/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0474181175231934\n",
                        "\n",
                        "\n",
                        "Epoch 149/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.046248197555542\n",
                        "\n",
                        "\n",
                        "Epoch 150/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0474491119384766\n",
                        "\n",
                        "\n",
                        "Epoch 151/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.040278673171997\n",
                        "\n",
                        "\n",
                        "Epoch 152/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0489308834075928\n",
                        "\n",
                        "\n",
                        "Epoch 153/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.048616647720337\n",
                        "\n",
                        "\n",
                        "Epoch 154/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0396738052368164\n",
                        "\n",
                        "\n",
                        "Epoch 155/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0482540130615234\n",
                        "\n",
                        "\n",
                        "Epoch 156/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.038156270980835\n",
                        "\n",
                        "\n",
                        "Epoch 157/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.042963981628418\n",
                        "\n",
                        "\n",
                        "Epoch 158/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0458285808563232\n",
                        "\n",
                        "\n",
                        "Epoch 159/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0471348762512207\n",
                        "\n",
                        "\n",
                        "Epoch 160/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.043156385421753\n",
                        "\n",
                        "\n",
                        "Epoch 161/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0397675037384033\n",
                        "\n",
                        "\n",
                        "Epoch 162/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0490176677703857\n",
                        "\n",
                        "\n",
                        "Epoch 163/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.047377824783325\n",
                        "\n",
                        "\n",
                        "Epoch 164/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0472445487976074\n",
                        "\n",
                        "\n",
                        "Epoch 165/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0453603267669678\n",
                        "\n",
                        "\n",
                        "Epoch 166/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0490081310272217\n",
                        "\n",
                        "\n",
                        "Epoch 167/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0377466678619385\n",
                        "\n",
                        "\n",
                        "Epoch 168/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.037489891052246\n",
                        "\n",
                        "\n",
                        "Epoch 169/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.044936180114746\n",
                        "\n",
                        "\n",
                        "Epoch 170/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.046856641769409\n",
                        "\n",
                        "\n",
                        "Epoch 171/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.044389486312866\n",
                        "\n",
                        "\n",
                        "Epoch 172/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0393855571746826\n",
                        "\n",
                        "\n",
                        "Epoch 173/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.04476261138916\n",
                        "\n",
                        "\n",
                        "Epoch 174/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.048251152038574\n",
                        "\n",
                        "\n",
                        "Epoch 175/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.048383951187134\n",
                        "\n",
                        "\n",
                        "Epoch 176/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.047126293182373\n",
                        "\n",
                        "\n",
                        "Epoch 177/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0396831035614014\n",
                        "\n",
                        "\n",
                        "Epoch 178/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.04217267036438\n",
                        "\n",
                        "\n",
                        "Epoch 179/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.041289806365967\n",
                        "\n",
                        "\n",
                        "Epoch 180/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.045642614364624\n",
                        "\n",
                        "\n",
                        "Epoch 181/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.050976514816284\n",
                        "\n",
                        "\n",
                        "Epoch 182/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.043673276901245\n",
                        "\n",
                        "\n",
                        "Epoch 183/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0490169525146484\n",
                        "\n",
                        "\n",
                        "Epoch 184/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0433616638183594\n",
                        "\n",
                        "\n",
                        "Epoch 185/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0477542877197266\n",
                        "\n",
                        "\n",
                        "Epoch 186/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0471463203430176\n",
                        "\n",
                        "\n",
                        "Epoch 187/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.045560598373413\n",
                        "\n",
                        "\n",
                        "Epoch 188/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0481433868408203\n",
                        "\n",
                        "\n",
                        "Epoch 189/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0484349727630615\n",
                        "\n",
                        "\n",
                        "Epoch 190/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0457868576049805\n",
                        "\n",
                        "\n",
                        "Epoch 191/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0467965602874756\n",
                        "\n",
                        "\n",
                        "Epoch 192/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0447559356689453\n",
                        "\n",
                        "\n",
                        "Epoch 193/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.03841495513916\n",
                        "\n",
                        "\n",
                        "Epoch 194/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.044846534729004\n",
                        "\n",
                        "\n",
                        "Epoch 195/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0425238609313965\n",
                        "\n",
                        "\n",
                        "Epoch 196/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.0434916019439697\n",
                        "\n",
                        "\n",
                        "Epoch 197/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.045933246612549\n",
                        "\n",
                        "\n",
                        "Epoch 198/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.041121482849121\n",
                        "\n",
                        "\n",
                        "Epoch 199/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.040961265563965\n",
                        "\n",
                        "\n",
                        "Epoch 200/200\n",
                        "Learning rate :  5e-05\n",
                        "Average loss :  2.048328161239624\n"
                    ]
                }
            ],
            "source": [
                "Epoch = 200\n",
                "i = 0\n",
                "lr = 5e-5\n",
                "lam = 0\n",
                "temp = 1000\n",
                "while(i<=Epoch):\n",
                "    print(\"\\n\")\n",
                "    print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
                "    print(\"Learning rate : \", lr)\n",
                "    xi_L, prevxi_L, lossitem= training_loop(c, xi_L,prevxi_L,UpsilonR,UpsilonL,Xdot,128,lr=lr,lam=lam,momentum=True)\n",
                "    temp = lossitem\n",
                "    i+=1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result stage 1:  1.0*x0_t*x1_t*sin(x0)*sin(x1) + 0.98*x0_t*x1_t*cos(x0)*cos(x1) + 0.49*x1_t**2 + 19.38*cos(x0) + 9.69*cos(x1)\n"
                    ]
                }
            ],
            "source": [
                "## Thresholding\n",
                "threshold = 1e-1\n",
                "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
                "expr = np.array(expr)[surv_index].tolist()\n",
                "\n",
                "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "\n",
                "## obtaining analytical model\n",
                "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
                "L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-2)\n",
                "print(\"Result stage 1: \", simplify(L))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Redefine computation after thresholding\n",
                "expr.append(known_expr[0])\n",
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)\n",
                "\n",
                "expr = np.array(expr)\n",
                "i1 = np.where(expr == 'x0_t**2')[0]\n",
                "i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "idx = np.arange(0,len(expr))\n",
                "idx = np.delete(idx,i1)\n",
                "known_expr = expr[i1].tolist()\n",
                "expr = np.delete(expr,i1).tolist()\n",
                "nonpenaltyidx = [i4,i5,i6]\n",
                "\n",
                "Zeta_ = Zeta[:,:,i1,:].clone().detach()\n",
                "Eta_ = Eta[:,:,i1,:].clone().detach()\n",
                "Delta_ = Delta[:,i1,:].clone().detach()\n",
                "\n",
                "Zeta = Zeta[:,:,idx,:]\n",
                "Eta = Eta[:,:,idx,:]\n",
                "Delta = Delta[:,idx,:]\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n",
                "Zeta_ = Zeta_.to(device)\n",
                "Eta_ = Eta_.to(device)\n",
                "Delta_ = Delta_.to(device)\n",
                "\n",
                "UpsilonL = Upsilonforward(Zeta, Eta, Delta, Xdot, device)\n",
                "UpsilonR = Upsilonforward(Zeta_, Eta_, Delta_, Xdot, device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Debugging for training with only known terms\n",
                "\n",
                "expr = np.array(expr)\n",
                "i0 = np.where(expr == 'x0_t**2')[0][0]\n",
                "i1 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i2 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i3 = np.where(expr == 'cos(x1)')[0][0]\n",
                "i4 = np.where(expr == 'x0_t*x1_t*cos(x0)*cos(x1)')[0][0]\n",
                "i5 = np.where(expr == 'x0_t*x1_t*sin(x0)*sin(x1)')[0][0]\n",
                "\n",
                "expr = [expr[i0],expr[i1],expr[i2],expr[i3],expr[i4],expr[i5]]\n",
                "\n",
                "#known term\n",
                "expr = np.array(expr)\n",
                "i1 = np.where(expr == 'x0_t**2')[0]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Creating library tensor\n",
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Deleting unused terms \n",
                "idx = np.arange(0,len(expr))\n",
                "idx = np.delete(idx,[i1])\n",
                "known_expr = expr[i1].tolist()\n",
                "\n",
                "## Separating known term and uknown term\n",
                "idx = np.arange(0,len(expr))\n",
                "idx = np.delete(idx,[i1])\n",
                "known_expr = expr[i1].tolist()\n",
                "expr = np.delete(expr,[i1])\n",
                "\n",
                "#non-penalty index from prev knowledge\n",
                "i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "nonpenaltyidx = [i4, i5, i6]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "expr = expr.tolist()\n",
                "\n",
                "Zeta_ = Zeta[:,:,i1,:].clone().detach()\n",
                "Eta_ = Eta[:,:,i1,:].clone().detach()\n",
                "Delta_ = Delta[:,i1,:].clone().detach()\n",
                "\n",
                "Zeta = Zeta[:,:,idx,:]\n",
                "Eta = Eta[:,:,idx,:]\n",
                "Delta = Delta[:,idx,:]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Moving to Cuda\n",
                "device = 'cuda:0'\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n",
                "\n",
                "Zeta_ = Zeta_.to(device)\n",
                "Eta_ = Eta_.to(device)\n",
                "Delta_ = Delta_.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "UpsilonL = Upsilonforward(Zeta, Eta, Delta, Xdot, device)\n",
                "UpsilonR = Upsilonforward(Zeta_, Eta_, Delta_, Xdot, device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Initialize coefficient\n",
                "\n",
                "xi_L = torch.zeros(len(expr), device=device).data.uniform_(-20,20)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "c = torch.ones(len(known_expr), device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Debugging for checking the computation\n",
                "\n",
                "expr = np.array(expr)\n",
                "i0 = np.where(expr == 'x0_t**2')[0][0]\n",
                "i1 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i2 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i3 = np.where(expr == 'cos(x1)')[0][0]\n",
                "i4 = np.where(expr == 'x0_t*x1_t*cos(x0)*cos(x1)')[0][0]\n",
                "i5 = np.where(expr == 'x0_t*x1_t*sin(x0)*sin(x1)')[0][0]\n",
                "\n",
                "expr = [expr[i0],expr[i1],expr[i2],expr[i3],expr[i4],expr[i5]]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "xi_True = torch.ones(len(expr))\n",
                "xi_True[0] = 1\n",
                "xi_True[1] = 0.5\n",
                "xi_True[2] = 19.62\n",
                "xi_True[3] = 9.81\n",
                "xi_True[4] = 1\n",
                "xi_True[5] = 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "expr = np.array(expr)\n",
                "idx = np.arange(0,len(expr))\n",
                "\n",
                "#removing indices\n",
                "i1 = np.where(expr == 'x0_t**2')[0]\n",
                "idx = np.delete(idx,[i1])\n",
                "\n",
                "#separaing known term and unknown terms\n",
                "known_expr = expr[i1].tolist()\n",
                "expr = np.delete(expr,[i1])\n",
                "\n",
                "expr = expr.tolist()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Zeta_ = Zeta[:,:,i1,:].clone().detach()\n",
                "Eta_ = Eta[:,:,i1,:].clone().detach()\n",
                "Delta_ = Delta[:,i1,:].clone().detach()\n",
                "\n",
                "Zeta = Zeta[:,:,idx,:]\n",
                "Eta = Eta[:,:,idx,:]\n",
                "Delta = Delta[:,idx,:]\n",
                "\n",
                "xi_left = xi_True[idx]\n",
                "xi_right = xi_True[i1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Moving to Cuda\n",
                "device = 'cuda:0'\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n",
                "\n",
                "Zeta_ = Zeta_.to(device)\n",
                "Eta_ = Eta_.to(device)\n",
                "Delta_ = Delta_.to(device)\n",
                "\n",
                "xi_left = xi_left.to(device)\n",
                "xi_right = xi_right.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ELtarg = ELforward(xi_right,Zeta_,Eta_,Delta_,Xdot,device)\n",
                "ELpred = -ELforward(xi_left,Zeta,Eta,Delta,Xdot,device)\n",
                "\n",
                "\n",
                "UpsilonL = Upsilonforward(Zeta, Eta, Delta, Xdot, device)\n",
                "UpsilonR = Upsilonforward(Zeta_, Eta_, Delta_, Xdot, device)\n",
                "UPtarg = torch.einsum('jkl,k->jl', UpsilonR, xi_right)\n",
                "UPpred = -torch.einsum('jkl,k->jl', UpsilonL, xi_left)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "t = np.arange(1000)\n",
                "threshold = 1000\n",
                "\n",
                "plt.plot(t,ELtarg[0,:threshold].detach().cpu().numpy())\n",
                "plt.plot(t,ELpred[0,:threshold].detach().cpu().numpy())\n",
                "plt.plot(t,UPtarg[0,:threshold].detach().cpu().numpy())\n",
                "plt.plot(t,UPpred[0,:threshold].detach().cpu().numpy())\n",
                "plt.show()\n",
                "\n",
                "plt.plot(t,ELtarg[1,:threshold].detach().cpu().numpy())\n",
                "plt.plot(t,ELpred[1,:threshold].detach().cpu().numpy())\n",
                "plt.plot(t,UPtarg[1,:threshold].detach().cpu().numpy())\n",
                "plt.plot(t,UPpred[1,:threshold].detach().cpu().numpy())\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.10 ('SystemIdentification')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "a4ed4680c7c46a218b8058c2660cec6a650dc98debbf7bcbd09838ba710de1ba"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
