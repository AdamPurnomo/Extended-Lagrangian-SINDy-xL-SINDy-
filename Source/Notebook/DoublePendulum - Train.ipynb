{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import sys \n",
                "sys.path.append(r'../Python Script/')\n",
                "\n",
                "from sympy import symbols, simplify, derive_by_array\n",
                "from scipy.integrate import solve_ivp\n",
                "from xLSINDy import *\n",
                "from sympy.physics.mechanics import *\n",
                "from sympy import *\n",
                "import sympy\n",
                "import torch\n",
                "import HLsearch as HL\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "def generate_data(func, time, init_values):\n",
                "    sol = solve_ivp(func,[time[0],time[-1]],init_values,t_eval=time, method='RK45',rtol=1e-10,atol=1e-10)\n",
                "    return sol.y.T, np.array([func(0,sol.y.T[i,:]) for i in range(sol.y.T.shape[0])],dtype=np.float64)\n",
                "\n",
                "def pendulum(t,x):\n",
                "    return x[1],-9.81*np.sin(x[0])\n",
                "\n",
                "# Pendulum rod lengths (m), bob masses (kg).\n",
                "L1, L2 = 1, 1\n",
                "m1, m2 = 1, 1\n",
                "# The gravitational acceleration (m.s-2).\n",
                "g = 9.81\n",
                "tau = 0\n",
                "\n",
                "def doublePendulum(t,y,M=0.0):\n",
                "    q1,q2,q1_t,q2_t = y\n",
                "    q1_2t = (-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L2*g*m2*np.sin(q2) + m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t)/2 - m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t + 2*L2*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q2) - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t + 2*L2*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q2))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2)/(2*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)))/(2*(-m2**2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))**2/(4*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)) + m2*(2*L2**2*np.sin(q2)**2 + 2*L2**2*np.cos(q2)**2)/2)))/(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)\n",
                "    q2_2t = (-L2*g*m2*np.sin(q2) + m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t)/2 - m2*(2*L2*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q2)*q2_t + 2*L2*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q2) - 2*L2*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q2)*q2_t + 2*L2*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q2))/2 - m2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))*(-L1*g*m1*np.sin(q1) - L1*g*m2*np.sin(q1) + M + m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t)/2 - m2*(2*L1*(L1*np.sin(q1)*q1_t + L2*np.sin(q2)*q2_t)*np.cos(q1)*q1_t + 2*L1*(-L1*np.sin(q1)*q1_t**2 - L2*np.sin(q2)*q2_t**2)*np.cos(q1) - 2*L1*(L1*np.cos(q1)*q1_t + L2*np.cos(q2)*q2_t)*np.sin(q1)*q1_t + 2*L1*(L1*np.cos(q1)*q1_t**2 + L2*np.cos(q2)*q2_t**2)*np.sin(q1))/2)/(2*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)))/(-m2**2*(2*L1*L2*np.sin(q1)*np.sin(q2) + 2*L1*L2*np.cos(q1)*np.cos(q2))**2/(4*(m1*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2 + m2*(2*L1**2*np.sin(q1)**2 + 2*L1**2*np.cos(q1)**2)/2)) + m2*(2*L2**2*np.sin(q2)**2 + 2*L2**2*np.cos(q2)**2)/2)\n",
                "    return q1_t,q2_t,q1_2t,q2_2t"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Saving Directory\n",
                "rootdir = \"../Double Pendulum/Data/\"\n",
                "\n",
                "num_sample = 100\n",
                "create_data = False\n",
                "training = True\n",
                "save = False\n",
                "noiselevel = 2e-2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "if(create_data):\n",
                "    print(\"Creating Data\")\n",
                "    num_sample = 100\n",
                "    X, Xdot = [], []\n",
                "    for i in range(num_sample):\n",
                "        t = np.arange(0,5,0.01)\n",
                "        theta1 = np.random.uniform(-np.pi, np.pi)\n",
                "        thetadot = np.random.uniform(0,0)\n",
                "        theta2 = np.random.uniform(-np.pi, np.pi)\n",
                "        \n",
                "        y0=np.array([theta1, theta2, thetadot, thetadot])\n",
                "        x,xdot = generate_data(doublePendulum,t,y0)\n",
                "        X.append(x)\n",
                "        Xdot.append(xdot)\n",
                "    X = np.vstack(X)\n",
                "    Xdot = np.vstack(Xdot)\n",
                "    if(save==True):\n",
                "        np.save(rootdir + \"X.npy\", X)\n",
                "        np.save(rootdir + \"Xdot.npy\",Xdot)\n",
                "else:\n",
                "    X = np.load(rootdir + \"X.npy\")\n",
                "    Xdot = np.load(rootdir + \"Xdot.npy\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "#adding noise\n",
                "mu, sigma = 0, noiselevel\n",
                "noise = np.random.normal(mu, sigma, X.shape[0])\n",
                "for i in range(X.shape[1]):\n",
                "    X[:,i] = X[:,i]+noise\n",
                "    Xdot[:,i] = Xdot[:,i]+noise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "states are: (x0, x1, x0_t, x1_t)\n",
                        "states derivatives are:  (x0_t, x1_t, x0_tt, x1_tt)\n"
                    ]
                }
            ],
            "source": [
                "states_dim = 4\n",
                "states = ()\n",
                "states_dot = ()\n",
                "for i in range(states_dim):\n",
                "    if(i<states_dim//2):\n",
                "        states = states + (symbols('x{}'.format(i)),)\n",
                "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
                "    else:\n",
                "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
                "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
                "print('states are:',states)\n",
                "print('states derivatives are: ', states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Turn from sympy to str\n",
                "states_sym = states\n",
                "states_dot_sym = states_dot\n",
                "states = list(str(descr) for descr in states)\n",
                "states_dot = list(str(descr) for descr in states_dot)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "#build function expression for the library in str\n",
                "exprdummy = HL.buildFunctionExpressions(1,states_dim,states,use_sine=True)\n",
                "polynom = exprdummy[2:4]\n",
                "trig = exprdummy[4:]\n",
                "polynom = HL.buildFunctionExpressions(2,len(polynom),polynom)\n",
                "trig = HL.buildFunctionExpressions(2, len(trig),trig)\n",
                "product = []\n",
                "for p in polynom:\n",
                "    for t in trig:\n",
                "        product.append(p + '*' + t)\n",
                "expr = polynom + trig + product"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Creating library tensor\n",
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "## separating known and unknown terms ##\n",
                "expr = np.array(expr)\n",
                "i1 = np.where(expr == 'x0_t**2')[0]\n",
                "\n",
                "## Garbage terms ##\n",
                "\n",
                "'''\n",
                "Explanation :\n",
                "x0_t, x1_t terms are not needed and will always satisfy EL's equation.\n",
                "Since x0_t, x1_t are garbages, we want to avoid (x0_t*sin()**2 + x0_t*cos()**2), thus we remove\n",
                "one of them, either  x0_t*sin()**2 or x0_t*cos()**2. \n",
                "Since the known term is x0_t**2, we also want to avoid the solution of (x0_t**2*sin()**2 + x0_t**2*cos()**2),\n",
                "so we remove either one of x0_t**2*sin()**2 or x0_t**2*cos()**2.\n",
                "'''\n",
                "\n",
                "i2 = np.where(expr == 'x0_t**2*cos(x0)**2')[0]\n",
                "i3 = np.where(expr == 'x0_t**2*cos(x1)**2')[0]\n",
                "i7 = np.where(expr == 'x1_t*cos(x0)**2')[0]\n",
                "i8 = np.where(expr == 'x1_t*cos(x1)**2')[0]\n",
                "i9 = np.where(expr == 'x1_t')[0]\n",
                "i10 = np.where(expr == 'x0_t*cos(x0)**2')[0]\n",
                "i11 = np.where(expr == 'x0_t*cos(x1)**2')[0]\n",
                "i12 = np.where(expr == 'x0_t')[0]\n",
                "i13 = np.where(expr == 'cos(x0)**2')[0]\n",
                "i14 = np.where(expr == 'cos(x1)**2')[0]\n",
                "\n",
                "#Deleting unused terms \n",
                "idx = np.arange(0,len(expr))\n",
                "idx = np.delete(idx,[i1,i2,i3,i7,i8,i9,i10,i11,i12,i13,i14])\n",
                "known_expr = expr[i1].tolist()\n",
                "expr = np.delete(expr,[i1,i2,i3,i7,i8,i9,i10,i11,i12,i13,i14])\n",
                "\n",
                "#non-penalty index from prev knowledge\n",
                "i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "nonpenaltyidx = [i4, i5, i6]\n",
                "\n",
                "expr = expr.tolist()\n",
                "\n",
                "Zeta_ = Zeta[:,:,i1,:].clone().detach()\n",
                "Eta_ = Eta[:,:,i1,:].clone().detach()\n",
                "Delta_ = Delta[:,i1,:].clone().detach()\n",
                "\n",
                "Zeta = Zeta[:,:,idx,:]\n",
                "Eta = Eta[:,:,idx,:]\n",
                "Delta = Delta[:,idx,:]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Moving to Cuda\n",
                "device = 'cuda:0'\n",
                "\n",
                "Zeta = Zeta.to(device)\n",
                "Eta = Eta.to(device)\n",
                "Delta = Delta.to(device)\n",
                "\n",
                "Zeta_ = Zeta_.to(device)\n",
                "Eta_ = Eta_.to(device)\n",
                "Delta_ = Delta_.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "xi_L = torch.ones(len(expr), device=device).data.uniform_(-20,20)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "c = torch.ones(len(known_expr), device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "def loss(pred, targ):\n",
                "    loss = torch.mean((pred - targ)**2) \n",
                "    return loss "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clip(w, alpha):\n",
                "    clipped = torch.minimum(w,alpha)\n",
                "    clipped = torch.maximum(clipped,-alpha)\n",
                "    return clipped\n",
                "\n",
                "def proxL1norm(w_hat, alpha, nonpenaltyidx):\n",
                "    if(torch.is_tensor(alpha)==False):\n",
                "        alpha = torch.tensor(alpha)\n",
                "    w = w_hat - clip(w_hat,alpha)\n",
                "    for idx in nonpenaltyidx:\n",
                "        w[idx] = w_hat[idx]\n",
                "    return w"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "def training_loop(c,coef, prevcoef, RHS, LHS, xdot, bs, lr, lam, momentum=True):\n",
                "    loss_list = []\n",
                "    tl = xdot.shape[0]\n",
                "    n = xdot.shape[1]\n",
                "\n",
                "    Zeta_, Eta_, Delta_ = LHS\n",
                "    Zeta, Eta, Delta = RHS\n",
                "\n",
                "    if(torch.is_tensor(xdot)==False):\n",
                "        xdot = torch.from_numpy(xdot).to(device).float()\n",
                "    \n",
                "    v = coef.clone().detach().requires_grad_(True)\n",
                "    prev = v\n",
                "    \n",
                "    for i in range(tl//bs):\n",
                "                \n",
                "        #computing acceleration with momentum\n",
                "        if(momentum==True):\n",
                "            vhat = (v + ((i-1)/(i+2))*(v - prev)).clone().detach().requires_grad_(True)\n",
                "        else:\n",
                "            vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
                "   \n",
                "        prev = v\n",
                "\n",
                "        #Computing loss\n",
                "        zeta = Zeta[:,:,:,i*bs:(i+1)*bs]\n",
                "        eta = Eta[:,:,:,i*bs:(i+1)*bs]\n",
                "        delta = Delta[:,:,i*bs:(i+1)*bs]\n",
                "\n",
                "        zeta_ = Zeta_[:,:,:,i*bs:(i+1)*bs]\n",
                "        eta_ = Eta_[:,:,:,i*bs:(i+1)*bs]\n",
                "        delta_ = Delta_[:,:,i*bs:(i+1)*bs]\n",
                "        \n",
                "        x_t = xdot[i*bs:(i+1)*bs,:]\n",
                "\n",
                "        #forward\n",
                "        pred = -ELforward(vhat,zeta,eta,delta,x_t,device)\n",
                "        targ = ELforward(c,zeta_,eta_,delta_,x_t,device)\n",
                "        \n",
                "        lossval = loss(pred, targ)\n",
                "        \n",
                "        #Backpropagation\n",
                "        lossval.backward()\n",
                "\n",
                "        with torch.no_grad():\n",
                "            v = vhat - lr*vhat.grad\n",
                "            v = (proxL1norm(v,lr*lam,nonpenaltyidx))\n",
                "            \n",
                "            # Manually zero the gradients after updating weights\n",
                "            vhat.grad = None\n",
                "        \n",
                "        \n",
                "    \n",
                "        \n",
                "        loss_list.append(lossval.item())\n",
                "    print(\"Average loss : \" , torch.tensor(loss_list).mean().item())\n",
                "    return v, prevcoef, torch.tensor(loss_list).mean().item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "Epoch 0/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  22352.80078125\n",
                        "\n",
                        "\n",
                        "Epoch 1/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  2217.086669921875\n",
                        "\n",
                        "\n",
                        "Epoch 2/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  1339.8870849609375\n",
                        "\n",
                        "\n",
                        "Epoch 3/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  931.8349609375\n",
                        "\n",
                        "\n",
                        "Epoch 4/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  682.3389282226562\n",
                        "\n",
                        "\n",
                        "Epoch 5/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  516.7910766601562\n",
                        "\n",
                        "\n",
                        "Epoch 6/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  402.0439453125\n",
                        "\n",
                        "\n",
                        "Epoch 7/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  320.4057922363281\n",
                        "\n",
                        "\n",
                        "Epoch 8/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  261.10504150390625\n",
                        "\n",
                        "\n",
                        "Epoch 9/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  217.13409423828125\n",
                        "\n",
                        "\n",
                        "Epoch 10/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  183.96060180664062\n",
                        "\n",
                        "\n",
                        "Epoch 11/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  158.63507080078125\n",
                        "\n",
                        "\n",
                        "Epoch 12/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  139.05479431152344\n",
                        "\n",
                        "\n",
                        "Epoch 13/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  123.29337310791016\n",
                        "\n",
                        "\n",
                        "Epoch 14/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  110.59022521972656\n",
                        "\n",
                        "\n",
                        "Epoch 15/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  100.1723861694336\n",
                        "\n",
                        "\n",
                        "Epoch 16/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  91.33917236328125\n",
                        "\n",
                        "\n",
                        "Epoch 17/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  83.7408447265625\n",
                        "\n",
                        "\n",
                        "Epoch 18/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  77.13174438476562\n",
                        "\n",
                        "\n",
                        "Epoch 19/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  71.39391326904297\n",
                        "\n",
                        "\n",
                        "Epoch 20/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  66.4783935546875\n",
                        "\n",
                        "\n",
                        "Epoch 21/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  62.22494888305664\n",
                        "\n",
                        "\n",
                        "Epoch 22/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  58.40877914428711\n",
                        "\n",
                        "\n",
                        "Epoch 23/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  54.97248458862305\n",
                        "\n",
                        "\n",
                        "Epoch 24/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  51.872676849365234\n",
                        "\n",
                        "\n",
                        "Epoch 25/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  49.07454299926758\n",
                        "\n",
                        "\n",
                        "Epoch 26/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  46.503204345703125\n",
                        "\n",
                        "\n",
                        "Epoch 27/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  44.15037155151367\n",
                        "\n",
                        "\n",
                        "Epoch 28/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  42.031822204589844\n",
                        "\n",
                        "\n",
                        "Epoch 29/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  40.11555099487305\n",
                        "\n",
                        "\n",
                        "Epoch 30/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  38.415252685546875\n",
                        "\n",
                        "\n",
                        "Epoch 31/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  36.873172760009766\n",
                        "\n",
                        "\n",
                        "Epoch 32/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  35.465972900390625\n",
                        "\n",
                        "\n",
                        "Epoch 33/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  34.21780014038086\n",
                        "\n",
                        "\n",
                        "Epoch 34/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  33.049007415771484\n",
                        "\n",
                        "\n",
                        "Epoch 35/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  31.98050880432129\n",
                        "\n",
                        "\n",
                        "Epoch 36/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  30.9833984375\n",
                        "\n",
                        "\n",
                        "Epoch 37/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  30.099658966064453\n",
                        "\n",
                        "\n",
                        "Epoch 38/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  29.26424789428711\n",
                        "\n",
                        "\n",
                        "Epoch 39/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  28.518346786499023\n",
                        "\n",
                        "\n",
                        "Epoch 40/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  27.83740997314453\n",
                        "\n",
                        "\n",
                        "Epoch 41/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  27.236600875854492\n",
                        "\n",
                        "\n",
                        "Epoch 42/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  26.687227249145508\n",
                        "\n",
                        "\n",
                        "Epoch 43/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  26.172861099243164\n",
                        "\n",
                        "\n",
                        "Epoch 44/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  25.70499038696289\n",
                        "\n",
                        "\n",
                        "Epoch 45/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  25.28866195678711\n",
                        "\n",
                        "\n",
                        "Epoch 46/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  24.91525650024414\n",
                        "\n",
                        "\n",
                        "Epoch 47/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  24.579574584960938\n",
                        "\n",
                        "\n",
                        "Epoch 48/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  24.258790969848633\n",
                        "\n",
                        "\n",
                        "Epoch 49/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  23.96475601196289\n",
                        "\n",
                        "\n",
                        "Epoch 50/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  23.68834686279297\n",
                        "\n",
                        "\n",
                        "Epoch 51/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  23.452165603637695\n",
                        "\n",
                        "\n",
                        "Epoch 52/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  23.239458084106445\n",
                        "\n",
                        "\n",
                        "Epoch 53/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  23.03708267211914\n",
                        "\n",
                        "\n",
                        "Epoch 54/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  22.834335327148438\n",
                        "\n",
                        "\n",
                        "Epoch 55/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  22.639307022094727\n",
                        "\n",
                        "\n",
                        "Epoch 56/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  22.45534896850586\n",
                        "\n",
                        "\n",
                        "Epoch 57/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  22.279111862182617\n",
                        "\n",
                        "\n",
                        "Epoch 58/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  22.10095977783203\n",
                        "\n",
                        "\n",
                        "Epoch 59/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  21.93403434753418\n",
                        "\n",
                        "\n",
                        "Epoch 60/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  21.77458381652832\n",
                        "\n",
                        "\n",
                        "Epoch 61/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  21.623245239257812\n",
                        "\n",
                        "\n",
                        "Epoch 62/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  21.47747802734375\n",
                        "\n",
                        "\n",
                        "Epoch 63/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  21.331117630004883\n",
                        "\n",
                        "\n",
                        "Epoch 64/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  21.194032669067383\n",
                        "\n",
                        "\n",
                        "Epoch 65/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  21.067180633544922\n",
                        "\n",
                        "\n",
                        "Epoch 66/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.959503173828125\n",
                        "\n",
                        "\n",
                        "Epoch 67/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.86957550048828\n",
                        "\n",
                        "\n",
                        "Epoch 68/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.776168823242188\n",
                        "\n",
                        "\n",
                        "Epoch 69/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.688894271850586\n",
                        "\n",
                        "\n",
                        "Epoch 70/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.601877212524414\n",
                        "\n",
                        "\n",
                        "Epoch 71/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.527042388916016\n",
                        "\n",
                        "\n",
                        "Epoch 72/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.449512481689453\n",
                        "\n",
                        "\n",
                        "Epoch 73/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.37732696533203\n",
                        "\n",
                        "\n",
                        "Epoch 74/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.30511474609375\n",
                        "\n",
                        "\n",
                        "Epoch 75/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.240808486938477\n",
                        "\n",
                        "\n",
                        "Epoch 76/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.168800354003906\n",
                        "\n",
                        "\n",
                        "Epoch 77/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.106229782104492\n",
                        "\n",
                        "\n",
                        "Epoch 78/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  20.0429630279541\n",
                        "\n",
                        "\n",
                        "Epoch 79/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.98622703552246\n",
                        "\n",
                        "\n",
                        "Epoch 80/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.931989669799805\n",
                        "\n",
                        "\n",
                        "Epoch 81/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.875486373901367\n",
                        "\n",
                        "\n",
                        "Epoch 82/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.821483612060547\n",
                        "\n",
                        "\n",
                        "Epoch 83/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.77097511291504\n",
                        "\n",
                        "\n",
                        "Epoch 84/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.718547821044922\n",
                        "\n",
                        "\n",
                        "Epoch 85/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.669246673583984\n",
                        "\n",
                        "\n",
                        "Epoch 86/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.61945915222168\n",
                        "\n",
                        "\n",
                        "Epoch 87/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.56760597229004\n",
                        "\n",
                        "\n",
                        "Epoch 88/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.51637840270996\n",
                        "\n",
                        "\n",
                        "Epoch 89/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.467466354370117\n",
                        "\n",
                        "\n",
                        "Epoch 90/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.414106369018555\n",
                        "\n",
                        "\n",
                        "Epoch 91/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.365947723388672\n",
                        "\n",
                        "\n",
                        "Epoch 92/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.314090728759766\n",
                        "\n",
                        "\n",
                        "Epoch 93/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.263904571533203\n",
                        "\n",
                        "\n",
                        "Epoch 94/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.215038299560547\n",
                        "\n",
                        "\n",
                        "Epoch 95/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.164997100830078\n",
                        "\n",
                        "\n",
                        "Epoch 96/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.115842819213867\n",
                        "\n",
                        "\n",
                        "Epoch 97/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.066917419433594\n",
                        "\n",
                        "\n",
                        "Epoch 98/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  19.019643783569336\n",
                        "\n",
                        "\n",
                        "Epoch 99/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  18.970624923706055\n",
                        "\n",
                        "\n",
                        "Epoch 100/100\n",
                        "Learning rate :  5e-06\n",
                        "Average loss :  18.922592163085938\n"
                    ]
                }
            ],
            "source": [
                "Epoch = 100\n",
                "i = 0\n",
                "lr = 5e-6\n",
                "lam = 1\n",
                "temp = 1000\n",
                "RHS = [Zeta, Eta, Delta]\n",
                "LHS = [Zeta_, Eta_, Delta_]\n",
                "while(i<=Epoch):\n",
                "    print(\"\\n\")\n",
                "    print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
                "    print(\"Learning rate : \", lr)\n",
                "    xi_L, prevxi_L, lossitem= training_loop(c, xi_L,prevxi_L,RHS,LHS,Xdot,128,lr=lr,lam=lam)\n",
                "    temp = lossitem\n",
                "    i+=1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result stage 1:  -0.02*x0_t**2*sin(x0) + 0.01*x0_t**2*sin(2*x0) + 0.01*x0_t**2*sin(x1) - 0.005*x0_t**2*sin(2*x1) - 0.005*x0_t**2*sin(x0 - x1) - 0.005*x0_t**2*sin(x0 + x1) - 1.81*x0_t**2*cos(x0) + 0.93*x0_t**2*cos(2*x0) - 1.16*x0_t**2*cos(x1) + 0.455*x0_t**2*cos(2*x1) + 0.165*x0_t**2*cos(x0 - x1) + 0.225*x0_t**2*cos(x0 + x1) - 1.385*x0_t**2 + 0.03*x0_t*x1_t*cos(2*x0) - 0.02*x0_t*x1_t*cos(x1) - 0.02*x0_t*x1_t*cos(2*x1) - 0.0700000000000001*x0_t*x1_t*cos(x0 - x1) + 0.73*x0_t*x1_t*cos(x0 + x1) - 0.11*x0_t*x1_t + 0.02*x1_t**2*sin(x1) + 0.01*x1_t**2*cos(x0) + 0.03*x1_t**2*cos(2*x0) + 0.01*x1_t**2*cos(x1) + 0.245*x1_t**2*cos(2*x1) - 0.01*x1_t**2*cos(x0 - x1) + 0.06*x1_t**2*cos(x0 + x1) - 0.135*x1_t**2 - 12.4*cos(x0) - 12.83*cos(x1)\n"
                    ]
                }
            ],
            "source": [
                "## Thresholding\n",
                "threshold = 1e-2\n",
                "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
                "expr = np.array(expr)[surv_index].tolist()\n",
                "\n",
                "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
                "prevxi_L = xi_L.clone().detach()\n",
                "\n",
                "## obtaining analytical model\n",
                "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
                "L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-3)\n",
                "print(\"Result stage 1: \", simplify(L))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "Epoch 0/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  235.5391845703125\n",
                        "\n",
                        "\n",
                        "Epoch 1/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  181.90042114257812\n",
                        "\n",
                        "\n",
                        "Epoch 2/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  178.54721069335938\n",
                        "\n",
                        "\n",
                        "Epoch 3/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  175.19723510742188\n",
                        "\n",
                        "\n",
                        "Epoch 4/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  171.1828155517578\n",
                        "\n",
                        "\n",
                        "Epoch 5/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  167.35308837890625\n",
                        "\n",
                        "\n",
                        "Epoch 6/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  163.60707092285156\n",
                        "\n",
                        "\n",
                        "Epoch 7/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  159.95431518554688\n",
                        "\n",
                        "\n",
                        "Epoch 8/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  156.41357421875\n",
                        "\n",
                        "\n",
                        "Epoch 9/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  152.96307373046875\n",
                        "\n",
                        "\n",
                        "Epoch 10/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  149.62008666992188\n",
                        "\n",
                        "\n",
                        "Epoch 11/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  146.3472900390625\n",
                        "\n",
                        "\n",
                        "Epoch 12/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  143.16241455078125\n",
                        "\n",
                        "\n",
                        "Epoch 13/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  140.05975341796875\n",
                        "\n",
                        "\n",
                        "Epoch 14/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  137.01336669921875\n",
                        "\n",
                        "\n",
                        "Epoch 15/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  134.09117126464844\n",
                        "\n",
                        "\n",
                        "Epoch 16/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  131.2501220703125\n",
                        "\n",
                        "\n",
                        "Epoch 17/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  128.46649169921875\n",
                        "\n",
                        "\n",
                        "Epoch 18/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  125.76636505126953\n",
                        "\n",
                        "\n",
                        "Epoch 19/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  123.13841247558594\n",
                        "\n",
                        "\n",
                        "Epoch 20/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  120.57178497314453\n",
                        "\n",
                        "\n",
                        "Epoch 21/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  118.07072448730469\n",
                        "\n",
                        "\n",
                        "Epoch 22/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  115.6354751586914\n",
                        "\n",
                        "\n",
                        "Epoch 23/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  113.2488784790039\n",
                        "\n",
                        "\n",
                        "Epoch 24/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  110.93163299560547\n",
                        "\n",
                        "\n",
                        "Epoch 25/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  108.66764068603516\n",
                        "\n",
                        "\n",
                        "Epoch 26/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  106.45916748046875\n",
                        "\n",
                        "\n",
                        "Epoch 27/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  104.30387878417969\n",
                        "\n",
                        "\n",
                        "Epoch 28/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  102.19924926757812\n",
                        "\n",
                        "\n",
                        "Epoch 29/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  100.13371276855469\n",
                        "\n",
                        "\n",
                        "Epoch 30/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  98.1300048828125\n",
                        "\n",
                        "\n",
                        "Epoch 31/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  96.16532135009766\n",
                        "\n",
                        "\n",
                        "Epoch 32/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  94.2493896484375\n",
                        "\n",
                        "\n",
                        "Epoch 33/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  92.3829345703125\n",
                        "\n",
                        "\n",
                        "Epoch 34/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  90.55146026611328\n",
                        "\n",
                        "\n",
                        "Epoch 35/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  88.7676773071289\n",
                        "\n",
                        "\n",
                        "Epoch 36/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  87.01659393310547\n",
                        "\n",
                        "\n",
                        "Epoch 37/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  85.3100357055664\n",
                        "\n",
                        "\n",
                        "Epoch 38/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  83.63916015625\n",
                        "\n",
                        "\n",
                        "Epoch 39/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  82.01048278808594\n",
                        "\n",
                        "\n",
                        "Epoch 40/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  80.4104995727539\n",
                        "\n",
                        "\n",
                        "Epoch 41/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  78.84817504882812\n",
                        "\n",
                        "\n",
                        "Epoch 42/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  77.31866455078125\n",
                        "\n",
                        "\n",
                        "Epoch 43/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  75.82421112060547\n",
                        "\n",
                        "\n",
                        "Epoch 44/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  74.35693359375\n",
                        "\n",
                        "\n",
                        "Epoch 45/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  72.9151611328125\n",
                        "\n",
                        "\n",
                        "Epoch 46/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  71.51486206054688\n",
                        "\n",
                        "\n",
                        "Epoch 47/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  70.14043426513672\n",
                        "\n",
                        "\n",
                        "Epoch 48/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  68.79280090332031\n",
                        "\n",
                        "\n",
                        "Epoch 49/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  67.4708480834961\n",
                        "\n",
                        "\n",
                        "Epoch 50/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  66.17809295654297\n",
                        "\n",
                        "\n",
                        "Epoch 51/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  64.91364288330078\n",
                        "\n",
                        "\n",
                        "Epoch 52/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  63.67649841308594\n",
                        "\n",
                        "\n",
                        "Epoch 53/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  62.4654541015625\n",
                        "\n",
                        "\n",
                        "Epoch 54/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  61.27545166015625\n",
                        "\n",
                        "\n",
                        "Epoch 55/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  60.110557556152344\n",
                        "\n",
                        "\n",
                        "Epoch 56/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  58.968929290771484\n",
                        "\n",
                        "\n",
                        "Epoch 57/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  57.853004455566406\n",
                        "\n",
                        "\n",
                        "Epoch 58/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  56.765594482421875\n",
                        "\n",
                        "\n",
                        "Epoch 59/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  55.700927734375\n",
                        "\n",
                        "\n",
                        "Epoch 60/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  54.658390045166016\n",
                        "\n",
                        "\n",
                        "Epoch 61/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  53.63072967529297\n",
                        "\n",
                        "\n",
                        "Epoch 62/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  52.62388229370117\n",
                        "\n",
                        "\n",
                        "Epoch 63/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  51.64476013183594\n",
                        "\n",
                        "\n",
                        "Epoch 64/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  50.683860778808594\n",
                        "\n",
                        "\n",
                        "Epoch 65/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  49.73817443847656\n",
                        "\n",
                        "\n",
                        "Epoch 66/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  48.81207275390625\n",
                        "\n",
                        "\n",
                        "Epoch 67/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  47.904571533203125\n",
                        "\n",
                        "\n",
                        "Epoch 68/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  47.01475524902344\n",
                        "\n",
                        "\n",
                        "Epoch 69/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  46.144134521484375\n",
                        "\n",
                        "\n",
                        "Epoch 70/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  45.28646469116211\n",
                        "\n",
                        "\n",
                        "Epoch 71/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  44.44527816772461\n",
                        "\n",
                        "\n",
                        "Epoch 72/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  43.620731353759766\n",
                        "\n",
                        "\n",
                        "Epoch 73/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  42.8136100769043\n",
                        "\n",
                        "\n",
                        "Epoch 74/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  42.0225715637207\n",
                        "\n",
                        "\n",
                        "Epoch 75/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  41.246063232421875\n",
                        "\n",
                        "\n",
                        "Epoch 76/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  40.483890533447266\n",
                        "\n",
                        "\n",
                        "Epoch 77/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  39.73802947998047\n",
                        "\n",
                        "\n",
                        "Epoch 78/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  39.00556182861328\n",
                        "\n",
                        "\n",
                        "Epoch 79/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  38.286678314208984\n",
                        "\n",
                        "\n",
                        "Epoch 80/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  37.582374572753906\n",
                        "\n",
                        "\n",
                        "Epoch 81/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  36.89221954345703\n",
                        "\n",
                        "\n",
                        "Epoch 82/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  36.21418380737305\n",
                        "\n",
                        "\n",
                        "Epoch 83/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  35.549232482910156\n",
                        "\n",
                        "\n",
                        "Epoch 84/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  34.8964958190918\n",
                        "\n",
                        "\n",
                        "Epoch 85/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  34.25819396972656\n",
                        "\n",
                        "\n",
                        "Epoch 86/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  33.6303825378418\n",
                        "\n",
                        "\n",
                        "Epoch 87/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  33.01372528076172\n",
                        "\n",
                        "\n",
                        "Epoch 88/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  32.41145324707031\n",
                        "\n",
                        "\n",
                        "Epoch 89/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  31.818143844604492\n",
                        "\n",
                        "\n",
                        "Epoch 90/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  31.236858367919922\n",
                        "\n",
                        "\n",
                        "Epoch 91/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  30.667673110961914\n",
                        "\n",
                        "\n",
                        "Epoch 92/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  30.10671043395996\n",
                        "\n",
                        "\n",
                        "Epoch 93/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  29.55936050415039\n",
                        "\n",
                        "\n",
                        "Epoch 94/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  29.021482467651367\n",
                        "\n",
                        "\n",
                        "Epoch 95/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  28.493885040283203\n",
                        "\n",
                        "\n",
                        "Epoch 96/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  27.9758243560791\n",
                        "\n",
                        "\n",
                        "Epoch 97/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  27.466094970703125\n",
                        "\n",
                        "\n",
                        "Epoch 98/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  26.96546173095703\n",
                        "\n",
                        "\n",
                        "Epoch 99/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  26.474611282348633\n",
                        "\n",
                        "\n",
                        "Epoch 100/100\n",
                        "Learning rate :  7.000000000000001e-06\n",
                        "Average loss :  25.99118423461914\n",
                        "Result stage 2: -0.509*x0_t**2*sin(x0)**2 - 0.106*x0_t**2*sin(x1)**2 + 0.362*x0_t**2*cos(x0)*cos(x1) - 0.495*x0_t**2*cos(x0) - 0.32*x0_t**2*cos(x1) + 0.318*x0_t*x1_t*sin(x0)*sin(x1) + 0.59*x0_t*x1_t*cos(x0)*cos(x1) + 0.115*x1_t**2*sin(x0)**2 + 0.129*x1_t**2*cos(x1) + 0.2*x1_t**2 + 5.618*cos(x0) + 3.2*cos(x1)\n",
                        "\n",
                        "\n",
                        "Epoch 0/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  12.736615180969238\n",
                        "\n",
                        "\n",
                        "Epoch 1/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  11.65677547454834\n",
                        "\n",
                        "\n",
                        "Epoch 2/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  11.328630447387695\n",
                        "\n",
                        "\n",
                        "Epoch 3/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  11.025206565856934\n",
                        "\n",
                        "\n",
                        "Epoch 4/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  10.730908393859863\n",
                        "\n",
                        "\n",
                        "Epoch 5/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  10.44459342956543\n",
                        "\n",
                        "\n",
                        "Epoch 6/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  10.166399002075195\n",
                        "\n",
                        "\n",
                        "Epoch 7/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  9.895331382751465\n",
                        "\n",
                        "\n",
                        "Epoch 8/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  9.631665229797363\n",
                        "\n",
                        "\n",
                        "Epoch 9/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  9.3755464553833\n",
                        "\n",
                        "\n",
                        "Epoch 10/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  9.126715660095215\n",
                        "\n",
                        "\n",
                        "Epoch 11/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  8.884079933166504\n",
                        "\n",
                        "\n",
                        "Epoch 12/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  8.64759635925293\n",
                        "\n",
                        "\n",
                        "Epoch 13/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  8.417861938476562\n",
                        "\n",
                        "\n",
                        "Epoch 14/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  8.194533348083496\n",
                        "\n",
                        "\n",
                        "Epoch 15/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  7.976770877838135\n",
                        "\n",
                        "\n",
                        "Epoch 16/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  7.764831066131592\n",
                        "\n",
                        "\n",
                        "Epoch 17/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  7.558635234832764\n",
                        "\n",
                        "\n",
                        "Epoch 18/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  7.358268737792969\n",
                        "\n",
                        "\n",
                        "Epoch 19/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  7.163842678070068\n",
                        "\n",
                        "\n",
                        "Epoch 20/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  6.974123477935791\n",
                        "\n",
                        "\n",
                        "Epoch 21/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  6.789380073547363\n",
                        "\n",
                        "\n",
                        "Epoch 22/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  6.6098785400390625\n",
                        "\n",
                        "\n",
                        "Epoch 23/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  6.4349164962768555\n",
                        "\n",
                        "\n",
                        "Epoch 24/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  6.264791011810303\n",
                        "\n",
                        "\n",
                        "Epoch 25/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  6.099311828613281\n",
                        "\n",
                        "\n",
                        "Epoch 26/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  5.93803596496582\n",
                        "\n",
                        "\n",
                        "Epoch 27/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  5.781194686889648\n",
                        "\n",
                        "\n",
                        "Epoch 28/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  5.6285905838012695\n",
                        "\n",
                        "\n",
                        "Epoch 29/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  5.480302810668945\n",
                        "\n",
                        "\n",
                        "Epoch 30/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  5.335635185241699\n",
                        "\n",
                        "\n",
                        "Epoch 31/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  5.194323539733887\n",
                        "\n",
                        "\n",
                        "Epoch 32/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  5.056934356689453\n",
                        "\n",
                        "\n",
                        "Epoch 33/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.923280715942383\n",
                        "\n",
                        "\n",
                        "Epoch 34/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.793135643005371\n",
                        "\n",
                        "\n",
                        "Epoch 35/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.66652250289917\n",
                        "\n",
                        "\n",
                        "Epoch 36/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.543511390686035\n",
                        "\n",
                        "\n",
                        "Epoch 37/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.423868656158447\n",
                        "\n",
                        "\n",
                        "Epoch 38/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.308125972747803\n",
                        "\n",
                        "\n",
                        "Epoch 39/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.196384429931641\n",
                        "\n",
                        "\n",
                        "Epoch 40/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  4.087795257568359\n",
                        "\n",
                        "\n",
                        "Epoch 41/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.981696367263794\n",
                        "\n",
                        "\n",
                        "Epoch 42/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.8779990673065186\n",
                        "\n",
                        "\n",
                        "Epoch 43/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.7765040397644043\n",
                        "\n",
                        "\n",
                        "Epoch 44/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.67734432220459\n",
                        "\n",
                        "\n",
                        "Epoch 45/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.5802788734436035\n",
                        "\n",
                        "\n",
                        "Epoch 46/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.4855475425720215\n",
                        "\n",
                        "\n",
                        "Epoch 47/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.3931586742401123\n",
                        "\n",
                        "\n",
                        "Epoch 48/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.303126335144043\n",
                        "\n",
                        "\n",
                        "Epoch 49/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.216177463531494\n",
                        "\n",
                        "\n",
                        "Epoch 50/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.132126808166504\n",
                        "\n",
                        "\n",
                        "Epoch 51/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  3.050887107849121\n",
                        "\n",
                        "\n",
                        "Epoch 52/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.9718098640441895\n",
                        "\n",
                        "\n",
                        "Epoch 53/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.894775152206421\n",
                        "\n",
                        "\n",
                        "Epoch 54/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.8199920654296875\n",
                        "\n",
                        "\n",
                        "Epoch 55/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.746899366378784\n",
                        "\n",
                        "\n",
                        "Epoch 56/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.6757829189300537\n",
                        "\n",
                        "\n",
                        "Epoch 57/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.6064436435699463\n",
                        "\n",
                        "\n",
                        "Epoch 58/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.5387649536132812\n",
                        "\n",
                        "\n",
                        "Epoch 59/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.4726688861846924\n",
                        "\n",
                        "\n",
                        "Epoch 60/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.408393383026123\n",
                        "\n",
                        "\n",
                        "Epoch 61/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.3460097312927246\n",
                        "\n",
                        "\n",
                        "Epoch 62/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.2854111194610596\n",
                        "\n",
                        "\n",
                        "Epoch 63/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.22641658782959\n",
                        "\n",
                        "\n",
                        "Epoch 64/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.168999195098877\n",
                        "\n",
                        "\n",
                        "Epoch 65/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.1134932041168213\n",
                        "\n",
                        "\n",
                        "Epoch 66/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.0596566200256348\n",
                        "\n",
                        "\n",
                        "Epoch 67/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  2.0069944858551025\n",
                        "\n",
                        "\n",
                        "Epoch 68/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.9556150436401367\n",
                        "\n",
                        "\n",
                        "Epoch 69/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.9055776596069336\n",
                        "\n",
                        "\n",
                        "Epoch 70/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.85672128200531\n",
                        "\n",
                        "\n",
                        "Epoch 71/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.808940052986145\n",
                        "\n",
                        "\n",
                        "Epoch 72/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.7624245882034302\n",
                        "\n",
                        "\n",
                        "Epoch 73/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.7168992757797241\n",
                        "\n",
                        "\n",
                        "Epoch 74/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.6725229024887085\n",
                        "\n",
                        "\n",
                        "Epoch 75/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.629439115524292\n",
                        "\n",
                        "\n",
                        "Epoch 76/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.5875095129013062\n",
                        "\n",
                        "\n",
                        "Epoch 77/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.5466254949569702\n",
                        "\n",
                        "\n",
                        "Epoch 78/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.5066158771514893\n",
                        "\n",
                        "\n",
                        "Epoch 79/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.4676109552383423\n",
                        "\n",
                        "\n",
                        "Epoch 80/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.4297335147857666\n",
                        "\n",
                        "\n",
                        "Epoch 81/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.3928883075714111\n",
                        "\n",
                        "\n",
                        "Epoch 82/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.3571220636367798\n",
                        "\n",
                        "\n",
                        "Epoch 83/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.3223016262054443\n",
                        "\n",
                        "\n",
                        "Epoch 84/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.2886598110198975\n",
                        "\n",
                        "\n",
                        "Epoch 85/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.2561348676681519\n",
                        "\n",
                        "\n",
                        "Epoch 86/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.2244678735733032\n",
                        "\n",
                        "\n",
                        "Epoch 87/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.1935960054397583\n",
                        "\n",
                        "\n",
                        "Epoch 88/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.1634795665740967\n",
                        "\n",
                        "\n",
                        "Epoch 89/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.1342003345489502\n",
                        "\n",
                        "\n",
                        "Epoch 90/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.1056227684020996\n",
                        "\n",
                        "\n",
                        "Epoch 91/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.0775582790374756\n",
                        "\n",
                        "\n",
                        "Epoch 92/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.050164818763733\n",
                        "\n",
                        "\n",
                        "Epoch 93/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  1.023808240890503\n",
                        "\n",
                        "\n",
                        "Epoch 94/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.9984133243560791\n",
                        "\n",
                        "\n",
                        "Epoch 95/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.973773717880249\n",
                        "\n",
                        "\n",
                        "Epoch 96/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.9499277472496033\n",
                        "\n",
                        "\n",
                        "Epoch 97/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.9268378019332886\n",
                        "\n",
                        "\n",
                        "Epoch 98/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.9045587778091431\n",
                        "\n",
                        "\n",
                        "Epoch 99/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.8828372955322266\n",
                        "\n",
                        "\n",
                        "Epoch 100/100\n",
                        "Learning rate :  9e-06\n",
                        "Average loss :  0.861572265625\n",
                        "Result stage 3: -0.115*x0_t**2*sin(x0)**2 - 0.122*x0_t**2*cos(x0) + 0.816*x0_t*x1_t*sin(x0)*sin(x1) + 0.902*x0_t*x1_t*cos(x0)*cos(x1) + 0.433*x1_t**2 + 15.993*cos(x0) + 8.187*cos(x1)\n",
                        "\n",
                        "\n",
                        "Epoch 0/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.9878623485565186\n",
                        "\n",
                        "\n",
                        "Epoch 1/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.9245224595069885\n",
                        "\n",
                        "\n",
                        "Epoch 2/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.8867202997207642\n",
                        "\n",
                        "\n",
                        "Epoch 3/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.8511260747909546\n",
                        "\n",
                        "\n",
                        "Epoch 4/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.8174137473106384\n",
                        "\n",
                        "\n",
                        "Epoch 5/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.7857003808021545\n",
                        "\n",
                        "\n",
                        "Epoch 6/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.7556887269020081\n",
                        "\n",
                        "\n",
                        "Epoch 7/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.7270165085792542\n",
                        "\n",
                        "\n",
                        "Epoch 8/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.6999238133430481\n",
                        "\n",
                        "\n",
                        "Epoch 9/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.6744715571403503\n",
                        "\n",
                        "\n",
                        "Epoch 10/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.6506461501121521\n",
                        "\n",
                        "\n",
                        "Epoch 11/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.6280414462089539\n",
                        "\n",
                        "\n",
                        "Epoch 12/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.6061789989471436\n",
                        "\n",
                        "\n",
                        "Epoch 13/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.5852641463279724\n",
                        "\n",
                        "\n",
                        "Epoch 14/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.5650780200958252\n",
                        "\n",
                        "\n",
                        "Epoch 15/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.5455228686332703\n",
                        "\n",
                        "\n",
                        "Epoch 16/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.5267309546470642\n",
                        "\n",
                        "\n",
                        "Epoch 17/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.5085046291351318\n",
                        "\n",
                        "\n",
                        "Epoch 18/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.4907627999782562\n",
                        "\n",
                        "\n",
                        "Epoch 19/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.47354429960250854\n",
                        "\n",
                        "\n",
                        "Epoch 20/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.4569042921066284\n",
                        "\n",
                        "\n",
                        "Epoch 21/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.44082868099212646\n",
                        "\n",
                        "\n",
                        "Epoch 22/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.42540010809898376\n",
                        "\n",
                        "\n",
                        "Epoch 23/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.41041266918182373\n",
                        "\n",
                        "\n",
                        "Epoch 24/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.3960012197494507\n",
                        "\n",
                        "\n",
                        "Epoch 25/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.38211286067962646\n",
                        "\n",
                        "\n",
                        "Epoch 26/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.3688245713710785\n",
                        "\n",
                        "\n",
                        "Epoch 27/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.356090247631073\n",
                        "\n",
                        "\n",
                        "Epoch 28/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.34382301568984985\n",
                        "\n",
                        "\n",
                        "Epoch 29/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.33214372396469116\n",
                        "\n",
                        "\n",
                        "Epoch 30/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.3212859630584717\n",
                        "\n",
                        "\n",
                        "Epoch 31/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.31113845109939575\n",
                        "\n",
                        "\n",
                        "Epoch 32/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.301705926656723\n",
                        "\n",
                        "\n",
                        "Epoch 33/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.2931542694568634\n",
                        "\n",
                        "\n",
                        "Epoch 34/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.2852746546268463\n",
                        "\n",
                        "\n",
                        "Epoch 35/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.2777978479862213\n",
                        "\n",
                        "\n",
                        "Epoch 36/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.2706248462200165\n",
                        "\n",
                        "\n",
                        "Epoch 37/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.26369673013687134\n",
                        "\n",
                        "\n",
                        "Epoch 38/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.25699830055236816\n",
                        "\n",
                        "\n",
                        "Epoch 39/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.25054705142974854\n",
                        "\n",
                        "\n",
                        "Epoch 40/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.24432186782360077\n",
                        "\n",
                        "\n",
                        "Epoch 41/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.2383415400981903\n",
                        "\n",
                        "\n",
                        "Epoch 42/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.23258759081363678\n",
                        "\n",
                        "\n",
                        "Epoch 43/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.2270086109638214\n",
                        "\n",
                        "\n",
                        "Epoch 44/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.2216213196516037\n",
                        "\n",
                        "\n",
                        "Epoch 45/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.21639758348464966\n",
                        "\n",
                        "\n",
                        "Epoch 46/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.21135391294956207\n",
                        "\n",
                        "\n",
                        "Epoch 47/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.20649421215057373\n",
                        "\n",
                        "\n",
                        "Epoch 48/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.2017660290002823\n",
                        "\n",
                        "\n",
                        "Epoch 49/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.197145015001297\n",
                        "\n",
                        "\n",
                        "Epoch 50/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.19265703856945038\n",
                        "\n",
                        "\n",
                        "Epoch 51/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.18829737603664398\n",
                        "\n",
                        "\n",
                        "Epoch 52/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.1840653270483017\n",
                        "\n",
                        "\n",
                        "Epoch 53/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.17994995415210724\n",
                        "\n",
                        "\n",
                        "Epoch 54/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.1759461760520935\n",
                        "\n",
                        "\n",
                        "Epoch 55/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.1720358431339264\n",
                        "\n",
                        "\n",
                        "Epoch 56/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.16822372376918793\n",
                        "\n",
                        "\n",
                        "Epoch 57/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.16452659666538239\n",
                        "\n",
                        "\n",
                        "Epoch 58/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.1609196662902832\n",
                        "\n",
                        "\n",
                        "Epoch 59/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.15739502012729645\n",
                        "\n",
                        "\n",
                        "Epoch 60/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.15397848188877106\n",
                        "\n",
                        "\n",
                        "Epoch 61/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.15065908432006836\n",
                        "\n",
                        "\n",
                        "Epoch 62/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.14741872251033783\n",
                        "\n",
                        "\n",
                        "Epoch 63/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.1442565619945526\n",
                        "\n",
                        "\n",
                        "Epoch 64/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.141179159283638\n",
                        "\n",
                        "\n",
                        "Epoch 65/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.13816823065280914\n",
                        "\n",
                        "\n",
                        "Epoch 66/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.13524575531482697\n",
                        "\n",
                        "\n",
                        "Epoch 67/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.1324150115251541\n",
                        "\n",
                        "\n",
                        "Epoch 68/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.12964728474617004\n",
                        "\n",
                        "\n",
                        "Epoch 69/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.12695863842964172\n",
                        "\n",
                        "\n",
                        "Epoch 70/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.12436071783304214\n",
                        "\n",
                        "\n",
                        "Epoch 71/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.12183696776628494\n",
                        "\n",
                        "\n",
                        "Epoch 72/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.11939889937639236\n",
                        "\n",
                        "\n",
                        "Epoch 73/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.11704470962285995\n",
                        "\n",
                        "\n",
                        "Epoch 74/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.11476423591375351\n",
                        "\n",
                        "\n",
                        "Epoch 75/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.1125834658741951\n",
                        "\n",
                        "\n",
                        "Epoch 76/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.11046841740608215\n",
                        "\n",
                        "\n",
                        "Epoch 77/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.10844505578279495\n",
                        "\n",
                        "\n",
                        "Epoch 78/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.10651276260614395\n",
                        "\n",
                        "\n",
                        "Epoch 79/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.10465370863676071\n",
                        "\n",
                        "\n",
                        "Epoch 80/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.1028706356883049\n",
                        "\n",
                        "\n",
                        "Epoch 81/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.10116724669933319\n",
                        "\n",
                        "\n",
                        "Epoch 82/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.09955264627933502\n",
                        "\n",
                        "\n",
                        "Epoch 83/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.09800685197114944\n",
                        "\n",
                        "\n",
                        "Epoch 84/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.09655529260635376\n",
                        "\n",
                        "\n",
                        "Epoch 85/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.09519392251968384\n",
                        "\n",
                        "\n",
                        "Epoch 86/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.09391359239816666\n",
                        "\n",
                        "\n",
                        "Epoch 87/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.09272965788841248\n",
                        "\n",
                        "\n",
                        "Epoch 88/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.09164060652256012\n",
                        "\n",
                        "\n",
                        "Epoch 89/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.09065144509077072\n",
                        "\n",
                        "\n",
                        "Epoch 90/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08978177607059479\n",
                        "\n",
                        "\n",
                        "Epoch 91/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08905016630887985\n",
                        "\n",
                        "\n",
                        "Epoch 92/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08843071013689041\n",
                        "\n",
                        "\n",
                        "Epoch 93/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08787719160318375\n",
                        "\n",
                        "\n",
                        "Epoch 94/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08738076686859131\n",
                        "\n",
                        "\n",
                        "Epoch 95/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08696405589580536\n",
                        "\n",
                        "\n",
                        "Epoch 96/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.0865941047668457\n",
                        "\n",
                        "\n",
                        "Epoch 97/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08624192327260971\n",
                        "\n",
                        "\n",
                        "Epoch 98/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.0859132930636406\n",
                        "\n",
                        "\n",
                        "Epoch 99/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.08562880009412766\n",
                        "\n",
                        "\n",
                        "Epoch 100/100\n",
                        "Learning rate :  1.1e-05\n",
                        "Average loss :  0.0853542611002922\n",
                        "Result stage 4: 0.99*x0_t*x1_t*sin(x0)*sin(x1) + 0.994*x0_t*x1_t*cos(x0)*cos(x1) + 0.495*x1_t**2 + 19.343*cos(x0) + 9.663*cos(x1)\n",
                        "\n",
                        "\n",
                        "Epoch 0/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.10033753514289856\n",
                        "\n",
                        "\n",
                        "Epoch 1/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.09632691740989685\n",
                        "\n",
                        "\n",
                        "Epoch 2/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.09343750029802322\n",
                        "\n",
                        "\n",
                        "Epoch 3/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0908585637807846\n",
                        "\n",
                        "\n",
                        "Epoch 4/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08858856558799744\n",
                        "\n",
                        "\n",
                        "Epoch 5/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08663448691368103\n",
                        "\n",
                        "\n",
                        "Epoch 6/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08501239866018295\n",
                        "\n",
                        "\n",
                        "Epoch 7/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08376985043287277\n",
                        "\n",
                        "\n",
                        "Epoch 8/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08302655071020126\n",
                        "\n",
                        "\n",
                        "Epoch 9/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08270056545734406\n",
                        "\n",
                        "\n",
                        "Epoch 10/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08251547068357468\n",
                        "\n",
                        "\n",
                        "Epoch 11/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0823860764503479\n",
                        "\n",
                        "\n",
                        "Epoch 12/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08228778094053268\n",
                        "\n",
                        "\n",
                        "Epoch 13/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08221019804477692\n",
                        "\n",
                        "\n",
                        "Epoch 14/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08214014768600464\n",
                        "\n",
                        "\n",
                        "Epoch 15/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08207947015762329\n",
                        "\n",
                        "\n",
                        "Epoch 16/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08202219009399414\n",
                        "\n",
                        "\n",
                        "Epoch 17/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0819731131196022\n",
                        "\n",
                        "\n",
                        "Epoch 18/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08193626254796982\n",
                        "\n",
                        "\n",
                        "Epoch 19/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0819011852145195\n",
                        "\n",
                        "\n",
                        "Epoch 20/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08186561614274979\n",
                        "\n",
                        "\n",
                        "Epoch 21/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08183245360851288\n",
                        "\n",
                        "\n",
                        "Epoch 22/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08179962635040283\n",
                        "\n",
                        "\n",
                        "Epoch 23/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08176758885383606\n",
                        "\n",
                        "\n",
                        "Epoch 24/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08173596858978271\n",
                        "\n",
                        "\n",
                        "Epoch 25/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08171775937080383\n",
                        "\n",
                        "\n",
                        "Epoch 26/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08170757442712784\n",
                        "\n",
                        "\n",
                        "Epoch 27/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08169714361429214\n",
                        "\n",
                        "\n",
                        "Epoch 28/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0816873237490654\n",
                        "\n",
                        "\n",
                        "Epoch 29/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08167897909879684\n",
                        "\n",
                        "\n",
                        "Epoch 30/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08167117089033127\n",
                        "\n",
                        "\n",
                        "Epoch 31/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08166391402482986\n",
                        "\n",
                        "\n",
                        "Epoch 32/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08165594190359116\n",
                        "\n",
                        "\n",
                        "Epoch 33/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08164822310209274\n",
                        "\n",
                        "\n",
                        "Epoch 34/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08164107799530029\n",
                        "\n",
                        "\n",
                        "Epoch 35/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08163295686244965\n",
                        "\n",
                        "\n",
                        "Epoch 36/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08162510395050049\n",
                        "\n",
                        "\n",
                        "Epoch 37/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08161813020706177\n",
                        "\n",
                        "\n",
                        "Epoch 38/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0816105306148529\n",
                        "\n",
                        "\n",
                        "Epoch 39/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08160332590341568\n",
                        "\n",
                        "\n",
                        "Epoch 40/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08159571886062622\n",
                        "\n",
                        "\n",
                        "Epoch 41/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08158741146326065\n",
                        "\n",
                        "\n",
                        "Epoch 42/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08158097416162491\n",
                        "\n",
                        "\n",
                        "Epoch 43/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08157310634851456\n",
                        "\n",
                        "\n",
                        "Epoch 44/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08156614005565643\n",
                        "\n",
                        "\n",
                        "Epoch 45/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08155983686447144\n",
                        "\n",
                        "\n",
                        "Epoch 46/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0815536379814148\n",
                        "\n",
                        "\n",
                        "Epoch 47/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08154571801424026\n",
                        "\n",
                        "\n",
                        "Epoch 48/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08153900504112244\n",
                        "\n",
                        "\n",
                        "Epoch 49/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08153173327445984\n",
                        "\n",
                        "\n",
                        "Epoch 50/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08152512460947037\n",
                        "\n",
                        "\n",
                        "Epoch 51/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08151815086603165\n",
                        "\n",
                        "\n",
                        "Epoch 52/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08151083439588547\n",
                        "\n",
                        "\n",
                        "Epoch 53/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08150427043437958\n",
                        "\n",
                        "\n",
                        "Epoch 54/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08149779587984085\n",
                        "\n",
                        "\n",
                        "Epoch 55/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08149107545614243\n",
                        "\n",
                        "\n",
                        "Epoch 56/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08148454874753952\n",
                        "\n",
                        "\n",
                        "Epoch 57/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08147954940795898\n",
                        "\n",
                        "\n",
                        "Epoch 58/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08147789537906647\n",
                        "\n",
                        "\n",
                        "Epoch 59/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08147667348384857\n",
                        "\n",
                        "\n",
                        "Epoch 60/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08147583156824112\n",
                        "\n",
                        "\n",
                        "Epoch 61/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0814744234085083\n",
                        "\n",
                        "\n",
                        "Epoch 62/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0814732015132904\n",
                        "\n",
                        "\n",
                        "Epoch 63/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0814720168709755\n",
                        "\n",
                        "\n",
                        "Epoch 64/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08147098124027252\n",
                        "\n",
                        "\n",
                        "Epoch 65/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08147019892930984\n",
                        "\n",
                        "\n",
                        "Epoch 66/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08146891742944717\n",
                        "\n",
                        "\n",
                        "Epoch 67/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08146771043539047\n",
                        "\n",
                        "\n",
                        "Epoch 68/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08146721869707108\n",
                        "\n",
                        "\n",
                        "Epoch 69/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0814661830663681\n",
                        "\n",
                        "\n",
                        "Epoch 70/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08146501332521439\n",
                        "\n",
                        "\n",
                        "Epoch 71/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08146390318870544\n",
                        "\n",
                        "\n",
                        "Epoch 72/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08146283775568008\n",
                        "\n",
                        "\n",
                        "Epoch 73/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08146215230226517\n",
                        "\n",
                        "\n",
                        "Epoch 74/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08146101981401443\n",
                        "\n",
                        "\n",
                        "Epoch 75/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145999163389206\n",
                        "\n",
                        "\n",
                        "Epoch 76/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0814586877822876\n",
                        "\n",
                        "\n",
                        "Epoch 77/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145704120397568\n",
                        "\n",
                        "\n",
                        "Epoch 78/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145557343959808\n",
                        "\n",
                        "\n",
                        "Epoch 79/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145492523908615\n",
                        "\n",
                        "\n",
                        "Epoch 80/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145300298929214\n",
                        "\n",
                        "\n",
                        "Epoch 81/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145204186439514\n",
                        "\n",
                        "\n",
                        "Epoch 82/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145122230052948\n",
                        "\n",
                        "\n",
                        "Epoch 83/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145134150981903\n",
                        "\n",
                        "\n",
                        "Epoch 84/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145144581794739\n",
                        "\n",
                        "\n",
                        "Epoch 85/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145129680633545\n",
                        "\n",
                        "\n",
                        "Epoch 86/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0814509466290474\n",
                        "\n",
                        "\n",
                        "Epoch 87/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145137131214142\n",
                        "\n",
                        "\n",
                        "Epoch 88/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0814511775970459\n",
                        "\n",
                        "\n",
                        "Epoch 89/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145132660865784\n",
                        "\n",
                        "\n",
                        "Epoch 90/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145134150981903\n",
                        "\n",
                        "\n",
                        "Epoch 91/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145182579755783\n",
                        "\n",
                        "\n",
                        "Epoch 92/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145181089639664\n",
                        "\n",
                        "\n",
                        "Epoch 93/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145131170749664\n",
                        "\n",
                        "\n",
                        "Epoch 94/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145151287317276\n",
                        "\n",
                        "\n",
                        "Epoch 95/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145132660865784\n",
                        "\n",
                        "\n",
                        "Epoch 96/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145156502723694\n",
                        "\n",
                        "\n",
                        "Epoch 97/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145155012607574\n",
                        "\n",
                        "\n",
                        "Epoch 98/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.0814516693353653\n",
                        "\n",
                        "\n",
                        "Epoch 99/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145138621330261\n",
                        "\n",
                        "\n",
                        "Epoch 100/100\n",
                        "Learning rate :  1.3e-05\n",
                        "Average loss :  0.08145181089639664\n",
                        "Result stage 5: 0.999*x0_t*x1_t*sin(x0)*sin(x1) + 1.002*x0_t*x1_t*cos(x0)*cos(x1) + 0.5*x1_t**2 + 19.574*cos(x0) + 9.79*cos(x1)\n"
                    ]
                }
            ],
            "source": [
                "## Next round selection ##\n",
                "for stage in range(4):\n",
                "\n",
                "    #Redefine computation after thresholding\n",
                "    expr.append(known_expr[0])\n",
                "    Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot, scaling=False)\n",
                "\n",
                "    expr = np.array(expr)\n",
                "    i1 = np.where(expr == 'x0_t**2')[0]\n",
                "    i4 = np.where(expr == 'x1_t**2')[0][0]\n",
                "    i5 = np.where(expr == 'cos(x0)')[0][0]\n",
                "    i6 = np.where(expr == 'cos(x1)')[0][0]\n",
                "    idx = np.arange(0,len(expr))\n",
                "    idx = np.delete(idx,i1)\n",
                "    known_expr = expr[i1].tolist()\n",
                "    expr = np.delete(expr,i1).tolist()\n",
                "    nonpenaltyidx = [i4,i5,i6]\n",
                "\n",
                "    Zeta_ = Zeta[:,:,i1,:].clone().detach()\n",
                "    Eta_ = Eta[:,:,i1,:].clone().detach()\n",
                "    Delta_ = Delta[:,i1,:].clone().detach()\n",
                "\n",
                "    Zeta = Zeta[:,:,idx,:]\n",
                "    Eta = Eta[:,:,idx,:]\n",
                "    Delta = Delta[:,idx,:]\n",
                "\n",
                "    Zeta = Zeta.to(device)\n",
                "    Eta = Eta.to(device)\n",
                "    Delta = Delta.to(device)\n",
                "    Zeta_ = Zeta_.to(device)\n",
                "    Eta_ = Eta_.to(device)\n",
                "    Delta_ = Delta_.to(device)\n",
                "\n",
                "    Epoch = 100\n",
                "    i = 0\n",
                "    lr += 2e-6\n",
                "    if(stage==3):\n",
                "        lam = 0\n",
                "    else:\n",
                "        lam = 0.1\n",
                "    temp = 1000\n",
                "    RHS = [Zeta, Eta, Delta]\n",
                "    LHS = [Zeta_, Eta_, Delta_]\n",
                "    while(i<=Epoch):\n",
                "        print(\"\\n\")\n",
                "        print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
                "        print(\"Learning rate : \", lr)\n",
                "        xi_L, prevxi_L, lossitem= training_loop(c, xi_L,prevxi_L,RHS,LHS,Xdot,128,lr=lr,lam=lam)\n",
                "        i+=1\n",
                "        if(temp <= 1e-3):\n",
                "            break\n",
                "    \n",
                "    ## Thresholding\n",
                "    threshold = 1e-1\n",
                "    surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
                "    expr = np.array(expr)[surv_index].tolist()\n",
                "\n",
                "    xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
                "    prevxi_L = xi_L.clone().detach()\n",
                "\n",
                "    ## obtaining analytical model\n",
                "    xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=3)\n",
                "    L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-1)\n",
                "    print(\"Result stage \" + str(stage+2) + \":\" , simplify(L))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Adding known terms\n",
                "L = str(simplify(L)) + \" + \" + known_expr[0]\n",
                "print(L)\n",
                "\n",
                "expr = expr + known_expr\n",
                "xi_L = torch.cat((xi_L, c))\n",
                "mask = torch.ones(len(xi_L),device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if(save==True):\n",
                "    #Saving Equation in string\n",
                "    text_file = open(rootdir + \"lagrangian_\" + str(noiselevel)+ \"_noise.txt\", \"w\")\n",
                "    text_file.write(L)\n",
                "    text_file.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "d5784be8b0ed123205c521437a438df309f2d2f16cb6cf8124a1b3e0f87bfce1"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 64-bit ('SystemIdentification': conda)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
