{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import numpy as np\r\n",
                "import sys \r\n",
                "sys.path.append(r'../Python Script/')\r\n",
                "\r\n",
                "from sympy import symbols, simplify, derive_by_array\r\n",
                "from scipy.integrate import solve_ivp\r\n",
                "from xLSINDy import *\r\n",
                "from sympy.physics.mechanics import *\r\n",
                "from sympy import *\r\n",
                "import sympy\r\n",
                "import torch\r\n",
                "import sys \r\n",
                "sys.path.append(r'../../../HLsearch/')\r\n",
                "import HLsearch as HL"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import time\r\n",
                "\r\n",
                "def generate_data(func, time, init_values):\r\n",
                "    sol = solve_ivp(func,[time[0],time[-1]],init_values,t_eval=time, method='RK45',rtol=1e-10,atol=1e-10)\r\n",
                "    return sol.y.T, np.array([func(0,sol.y.T[i,:]) for i in range(sol.y.T.shape[0])],dtype=np.float64)\r\n",
                "\r\n",
                "def pendulum(t,x):\r\n",
                "    return x[1],-9.81*np.sin(x[0])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "#Saving Directory\r\n",
                "rootdir = \"../Single Pendulum/Data/\"\r\n",
                "\r\n",
                "num_sample = 100\r\n",
                "create_data = False\r\n",
                "training = True\r\n",
                "save = False\r\n",
                "noiselevel = 2e-2"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "if(create_data):\r\n",
                "    X, Xdot = [], []\r\n",
                "    for i in range(num_sample):\r\n",
                "        t = np.arange(0,5,0.01)\r\n",
                "\r\n",
                "        theta = np.random.uniform(-np.pi, np.pi)\r\n",
                "        thetadot = np.random.uniform(-2.1,2.1)\r\n",
                "        cond = 0.5*thetadot**2 - np.cos(theta)\r\n",
                "        #checking condition so that it does not go full loop\r\n",
                "        while(cond>0.99):\r\n",
                "            theta = np.random.uniform(-np.pi, np.pi)\r\n",
                "            thetadot = np.random.uniform(-2.1,2.1)\r\n",
                "            cond = 0.5*thetadot**2 - np.cos(theta)\r\n",
                "        \r\n",
                "        y_0 = np.array([theta, thetadot])\r\n",
                "        x,xdot = generate_data(pendulum, t, y_0)\r\n",
                "        X.append(x)\r\n",
                "        Xdot.append(xdot)\r\n",
                "    if(save==True):\r\n",
                "        np.save(rootdir + \"X.npy\", X)\r\n",
                "        np.save(rootdir + \"Xdot.npy\",Xdot)\r\n",
                "else:\r\n",
                "    X = np.load(rootdir + \"X.npy\")\r\n",
                "    Xdot = np.load(rootdir + \"Xdot.npy\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "#adding noise\r\n",
                "mu, sigma = 0, noiselevel\r\n",
                "noise = np.random.normal(mu, sigma, X.shape[0])\r\n",
                "for i in range(X.shape[1]):\r\n",
                "    X[:,i] = X[:,i]+noise\r\n",
                "    Xdot[:,i] = Xdot[:,i]+noise"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "states_dim = 2\r\n",
                "states = ()\r\n",
                "states_dot = ()\r\n",
                "for i in range(states_dim):\r\n",
                "    if(i<states_dim//2):\r\n",
                "        states = states + (symbols('x{}'.format(i)),)\r\n",
                "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\r\n",
                "    else:\r\n",
                "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\r\n",
                "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\r\n",
                "print('states are:',states)\r\n",
                "print('states derivatives are: ', states_dot)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "states are: (x0, x0_t)\n",
                        "states derivatives are:  (x0_t, x0_tt)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "#Turn from sympy to str\r\n",
                "states_sym = states\r\n",
                "states_dot_sym = states_dot\r\n",
                "states = list(str(descr) for descr in states)\r\n",
                "states_dot = list(str(descr) for descr in states_dot)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "#build function expression for the library in str\r\n",
                "expr= HL.buildFunctionExpressions(2,states_dim,states,use_sine=True)\r\n",
                "expr.pop(5)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'x0*x0_t'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 8
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "device = 'cuda:0'"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,scaling=True)\r\n",
                "Eta = Eta.to(device)\r\n",
                "Zeta = Zeta.to(device)\r\n",
                "Delta = Delta.to(device)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "mask = torch.ones(len(expr),device=device)\r\n",
                "xi_L = torch.ones(len(expr), device=device).data.uniform_(-10,10)\r\n",
                "prevxi_L = xi_L.clone().detach()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "def loss(pred, targ, coef):\r\n",
                "    loss = torch.mean((pred - targ)**2) \r\n",
                "    return loss "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "def clip(w, alpha):\r\n",
                "    clipped = torch.minimum(w,alpha)\r\n",
                "    clipped = torch.maximum(clipped,-alpha)\r\n",
                "    return clipped\r\n",
                "\r\n",
                "def proxL1norm(w_hat, alpha):\r\n",
                "    if(torch.is_tensor(alpha)==False):\r\n",
                "        alpha = torch.tensor(alpha)\r\n",
                "    w = w_hat - clip(w_hat,alpha)\r\n",
                "    return w\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "def training_loop(coef, prevcoef, Zeta, Eta, Delta,xdot, bs, lr, lam):\r\n",
                "    loss_list = []\r\n",
                "    tl = xdot.shape[0]\r\n",
                "    n = xdot.shape[1]\r\n",
                "\r\n",
                "    if(torch.is_tensor(xdot)==False):\r\n",
                "        xdot = torch.from_numpy(xdot).to(device).float()\r\n",
                "    \r\n",
                "    for i in range(tl//bs):\r\n",
                "        \r\n",
                "        #v = coef.clone().detach().requires_grad_(True)\r\n",
                "        \r\n",
                "        #computing acceleration with momentum\r\n",
                "        v = (coef + ((i-1)/(i+2))*(coef - prevcoef)).clone().detach().requires_grad_(True)\r\n",
                "        \r\n",
                "        prevcoef = coef.clone().detach()\r\n",
                "\r\n",
                "\r\n",
                "        #Computing loss\r\n",
                "        zeta = Zeta[:,:,:,i*bs:(i+1)*bs]\r\n",
                "        eta = Eta[:,:,:,i*bs:(i+1)*bs]\r\n",
                "        delta = Delta[:,:,i*bs:(i+1)*bs]\r\n",
                "        x_t = xdot[i*bs:(i+1)*bs,:]\r\n",
                "\r\n",
                "        #forward\r\n",
                "        q_tt_pred = lagrangianforward(v,mask,zeta,eta,delta,x_t,device)\r\n",
                "        q_tt_true = xdot[i*bs:(i+1)*bs,n//2:].T\r\n",
                "        \r\n",
                "\r\n",
                "        #tau_pred = tauforward(coef,mask,zeta,eta,delta,x_t)\r\n",
                "        #tau_true = torch.zeros(tau_pred.shape,device=device)\r\n",
                "        #tau_true[0,:] = 1\r\n",
                "        \r\n",
                "        lossval = loss(q_tt_pred, q_tt_true, coef)\r\n",
                "        \r\n",
                "        #Backpropagation\r\n",
                "               \r\n",
                "        lossval.backward()\r\n",
                "        with torch.no_grad():\r\n",
                "            vhat = v - lr*v.grad\r\n",
                "            coef = (proxL1norm(vhat,lr*lam)).clone().detach()\r\n",
                "\r\n",
                "        \r\n",
                "    \r\n",
                "        \r\n",
                "        loss_list.append(lossval.item())\r\n",
                "    print(\"Average loss : \" , torch.tensor(loss_list).mean().item())\r\n",
                "    return coef, prevcoef, torch.tensor(loss_list).mean().item()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "Epoch = 100\r\n",
                "i = 1\r\n",
                "lr = 1e-4\r\n",
                "lam = 0.1\r\n",
                "temp = 1000\r\n",
                "while(i<=Epoch):\r\n",
                "    print(\"\\n\")\r\n",
                "    print(\"Stage 1\")\r\n",
                "    print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\r\n",
                "    print(\"Learning rate : \", lr)\r\n",
                "    xi_L , prevxi_L, lossitem= training_loop(xi_L,prevxi_L,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\r\n",
                "    if(temp <=5e-3):\r\n",
                "        break\r\n",
                "    if(temp <=1e-1):\r\n",
                "        lr = 1e-5\r\n",
                "    temp = lossitem\r\n",
                "    i+=1"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 1/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  39.20124816894531\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 2/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  38.194419860839844\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 3/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  37.73448181152344\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 4/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  37.42933654785156\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 5/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  37.19251251220703\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 6/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  36.99118423461914\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 7/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  36.80839920043945\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 8/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  36.63421630859375\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 9/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  36.46073532104492\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 10/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  36.28203201293945\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 11/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  36.09148025512695\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 12/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  35.88092803955078\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 13/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  35.645904541015625\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 14/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  35.368473052978516\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 15/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  35.020164489746094\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 16/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  34.556907653808594\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 17/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  33.88372802734375\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 18/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  32.78184509277344\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 19/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  30.56793785095215\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 20/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  22.482696533203125\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 21/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  3.6023762226104736\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 22/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.5906461477279663\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 23/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.341633141040802\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 24/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.20523060858249664\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 25/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.11939970403909683\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 26/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.07252026349306107\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 27/100\n",
                        "Learning rate :  0.0001\n",
                        "Average loss :  0.04962746053934097\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 28/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.0360381118953228\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 29/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.03514101728796959\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 30/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.03405505791306496\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 31/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.033123716711997986\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 32/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.03232048079371452\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 33/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.03160100430250168\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 34/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.031036585569381714\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 35/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.03053375892341137\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 36/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.030123932287096977\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 37/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02972639724612236\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 38/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02922600507736206\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 39/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.028849810361862183\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 40/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.028442611917853355\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 41/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.028003675863146782\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 42/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.027652816846966743\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 43/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.027370193973183632\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 44/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.027155954390764236\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 45/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026962358504533768\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 46/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026745779439806938\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 47/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02660105749964714\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 48/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02643328532576561\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 49/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026262054219841957\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 50/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026163199916481972\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 51/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02613731287419796\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 52/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.0261452067643404\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 53/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02613120712339878\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 54/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02613663114607334\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 55/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02614029124379158\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 56/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026130395010113716\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 57/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026127422228455544\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 58/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026131832972168922\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 59/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02613765560090542\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 60/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026128338649868965\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 61/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02612275630235672\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 62/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026148907840251923\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 63/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026128025725483894\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 64/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026126045733690262\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 65/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026139449328184128\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 66/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026133466511964798\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 67/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026126636192202568\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 68/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026136333122849464\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 69/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026138443499803543\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 70/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026129020377993584\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 71/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026124609634280205\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 72/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026136502623558044\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 73/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026132678613066673\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 74/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026125626638531685\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 75/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026127370074391365\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 76/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026131637394428253\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 77/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02613312378525734\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 78/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026132067665457726\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 79/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026132434606552124\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 80/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026135332882404327\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 81/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026129115372896194\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 82/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02612326666712761\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 83/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02612936869263649\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 84/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026137445122003555\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 85/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02614341303706169\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 86/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026133693754673004\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 87/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026134414598345757\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 88/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026132335886359215\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 89/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026128651574254036\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 90/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.02613227255642414\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 91/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026138050481677055\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 92/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026137588545680046\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 93/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026129204779863358\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 94/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026133425533771515\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 95/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026138320565223694\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 96/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026137469336390495\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 97/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026132818311452866\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 98/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026128072291612625\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 99/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026129767298698425\n",
                        "\n",
                        "\n",
                        "Stage 1\n",
                        "Epoch 100/100\n",
                        "Learning rate :  1e-05\n",
                        "Average loss :  0.026131603866815567\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "## Thresholding small indices ##\r\n",
                "threshold = 1e-2\r\n",
                "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\r\n",
                "expr = np.array(expr)[surv_index].tolist()\r\n",
                "\r\n",
                "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\r\n",
                "prevxi_L = xi_L.clone().detach()\r\n",
                "mask = torch.ones(len(expr),device=device)\r\n",
                "\r\n",
                "## obtaining analytical model\r\n",
                "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\r\n",
                "L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-2)\r\n",
                "print(simplify(L))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0.29*x0_t**2 + 2.92*x0_t + 11.41*cos(x0)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "## Next round Selection ##\r\n",
                "for stage in range(2):\r\n",
                "    \r\n",
                "    #Redefine computation after thresholding\r\n",
                "    Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,scaling=False)\r\n",
                "    Eta = Eta.to(device)\r\n",
                "    Zeta = Zeta.to(device)\r\n",
                "    Delta = Delta.to(device)\r\n",
                "\r\n",
                "    #Training\r\n",
                "    Epoch = 100\r\n",
                "    i = 1\r\n",
                "    lr = 3e-5\r\n",
                "    if(stage==1):\r\n",
                "        lam = 0\r\n",
                "    else:\r\n",
                "        lam = 0.01\r\n",
                "    temp = 1000\r\n",
                "    while(i<=Epoch):\r\n",
                "        print(\"\\n\")\r\n",
                "        print(\"Stage \" + str(stage+2))\r\n",
                "        print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\r\n",
                "        print(\"Learning rate : \", lr)\r\n",
                "        xi_L , prevxi_L, lossitem= training_loop(xi_L,prevxi_L,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\r\n",
                "        temp = lossitem\r\n",
                "        if(temp <=1e-6):\r\n",
                "            break\r\n",
                "        i+=1\r\n",
                "    \r\n",
                "    ## Thresholding small indices ##\r\n",
                "    threshold = 1e-1\r\n",
                "    surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\r\n",
                "    expr = np.array(expr)[surv_index].tolist()\r\n",
                "\r\n",
                "    xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\r\n",
                "    prevxi_L = xi_L.clone().detach()\r\n",
                "    mask = torch.ones(len(expr),device=device)\r\n",
                "\r\n",
                "    ## obtaining analytical model\r\n",
                "    xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=3)\r\n",
                "    L = HL.generateExpression(xi_Lcpu,expr,threshold=1e-2)\r\n",
                "    print(\"Result stage \" + str(stage+2) + \":\" , simplify(L))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "\n",
                        "\n",
                        "Epoch 1/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.5810737013816833\n",
                        "\n",
                        "\n",
                        "Epoch 2/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612227573990822\n",
                        "\n",
                        "\n",
                        "Epoch 3/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127925142645836\n",
                        "\n",
                        "\n",
                        "Epoch 4/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026125963777303696\n",
                        "\n",
                        "\n",
                        "Epoch 5/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612675353884697\n",
                        "\n",
                        "\n",
                        "Epoch 6/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026130858808755875\n",
                        "\n",
                        "\n",
                        "Epoch 7/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612759917974472\n",
                        "\n",
                        "\n",
                        "Epoch 8/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026126032695174217\n",
                        "\n",
                        "\n",
                        "Epoch 9/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026126069948077202\n",
                        "\n",
                        "\n",
                        "Epoch 10/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02611963078379631\n",
                        "\n",
                        "\n",
                        "Epoch 11/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026125838980078697\n",
                        "\n",
                        "\n",
                        "Epoch 12/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.0261271670460701\n",
                        "\n",
                        "\n",
                        "Epoch 13/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612527459859848\n",
                        "\n",
                        "\n",
                        "Epoch 14/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026122204959392548\n",
                        "\n",
                        "\n",
                        "Epoch 15/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026118112727999687\n",
                        "\n",
                        "\n",
                        "Epoch 16/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612839639186859\n",
                        "\n",
                        "\n",
                        "Epoch 17/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026125876232981682\n",
                        "\n",
                        "\n",
                        "Epoch 18/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612188830971718\n",
                        "\n",
                        "\n",
                        "Epoch 19/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026122640818357468\n",
                        "\n",
                        "\n",
                        "Epoch 20/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124760508537292\n",
                        "\n",
                        "\n",
                        "Epoch 21/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124216616153717\n",
                        "\n",
                        "\n",
                        "Epoch 22/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026120048016309738\n",
                        "\n",
                        "\n",
                        "Epoch 23/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026125404983758926\n",
                        "\n",
                        "\n",
                        "Epoch 24/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02611944079399109\n",
                        "\n",
                        "\n",
                        "Epoch 25/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026120875030755997\n",
                        "\n",
                        "\n",
                        "Epoch 26/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612108550965786\n",
                        "\n",
                        "\n",
                        "Epoch 27/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124749332666397\n",
                        "\n",
                        "\n",
                        "Epoch 28/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612648718059063\n",
                        "\n",
                        "\n",
                        "Epoch 29/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02611820586025715\n",
                        "\n",
                        "\n",
                        "Epoch 30/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026121661067008972\n",
                        "\n",
                        "\n",
                        "Epoch 31/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124579831957817\n",
                        "\n",
                        "\n",
                        "Epoch 32/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127368211746216\n",
                        "\n",
                        "\n",
                        "Epoch 33/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612767182290554\n",
                        "\n",
                        "\n",
                        "Epoch 34/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026123497635126114\n",
                        "\n",
                        "\n",
                        "Epoch 35/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127219200134277\n",
                        "\n",
                        "\n",
                        "Epoch 36/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612353302538395\n",
                        "\n",
                        "\n",
                        "Epoch 37/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612079493701458\n",
                        "\n",
                        "\n",
                        "Epoch 38/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026119183748960495\n",
                        "\n",
                        "\n",
                        "Epoch 39/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612294815480709\n",
                        "\n",
                        "\n",
                        "Epoch 40/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026119451969861984\n",
                        "\n",
                        "\n",
                        "Epoch 41/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612367831170559\n",
                        "\n",
                        "\n",
                        "Epoch 42/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026120800524950027\n",
                        "\n",
                        "\n",
                        "Epoch 43/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026128090918064117\n",
                        "\n",
                        "\n",
                        "Epoch 44/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612418867647648\n",
                        "\n",
                        "\n",
                        "Epoch 45/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026117809116840363\n",
                        "\n",
                        "\n",
                        "Epoch 46/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02611745148897171\n",
                        "\n",
                        "\n",
                        "Epoch 47/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026128580793738365\n",
                        "\n",
                        "\n",
                        "Epoch 48/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026113368570804596\n",
                        "\n",
                        "\n",
                        "Epoch 49/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127809658646584\n",
                        "\n",
                        "\n",
                        "Epoch 50/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612685225903988\n",
                        "\n",
                        "\n",
                        "Epoch 51/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026119770482182503\n",
                        "\n",
                        "\n",
                        "Epoch 52/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026123039424419403\n",
                        "\n",
                        "\n",
                        "Epoch 53/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127159595489502\n",
                        "\n",
                        "\n",
                        "Epoch 54/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612840197980404\n",
                        "\n",
                        "\n",
                        "Epoch 55/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026119403541088104\n",
                        "\n",
                        "\n",
                        "Epoch 56/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127930730581284\n",
                        "\n",
                        "\n",
                        "Epoch 57/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612251788377762\n",
                        "\n",
                        "\n",
                        "Epoch 58/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026118425652384758\n",
                        "\n",
                        "\n",
                        "Epoch 59/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02613321878015995\n",
                        "\n",
                        "\n",
                        "Epoch 60/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612222358584404\n",
                        "\n",
                        "\n",
                        "Epoch 61/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124391704797745\n",
                        "\n",
                        "\n",
                        "Epoch 62/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026118053123354912\n",
                        "\n",
                        "\n",
                        "Epoch 63/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026128925383090973\n",
                        "\n",
                        "\n",
                        "Epoch 64/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124101132154465\n",
                        "\n",
                        "\n",
                        "Epoch 65/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127925142645836\n",
                        "\n",
                        "\n",
                        "Epoch 66/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026116717606782913\n",
                        "\n",
                        "\n",
                        "Epoch 67/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612164057791233\n",
                        "\n",
                        "\n",
                        "Epoch 68/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.0261261984705925\n",
                        "\n",
                        "\n",
                        "Epoch 69/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026129011064767838\n",
                        "\n",
                        "\n",
                        "Epoch 70/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026120567694306374\n",
                        "\n",
                        "\n",
                        "Epoch 71/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026125457137823105\n",
                        "\n",
                        "\n",
                        "Epoch 72/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026120584458112717\n",
                        "\n",
                        "\n",
                        "Epoch 73/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026119699701666832\n",
                        "\n",
                        "\n",
                        "Epoch 74/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612662874162197\n",
                        "\n",
                        "\n",
                        "Epoch 75/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026121173053979874\n",
                        "\n",
                        "\n",
                        "Epoch 76/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612912654876709\n",
                        "\n",
                        "\n",
                        "Epoch 77/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612229809165001\n",
                        "\n",
                        "\n",
                        "Epoch 78/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612459287047386\n",
                        "\n",
                        "\n",
                        "Epoch 79/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026121988892555237\n",
                        "\n",
                        "\n",
                        "Epoch 80/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026133257895708084\n",
                        "\n",
                        "\n",
                        "Epoch 81/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.0261258315294981\n",
                        "\n",
                        "\n",
                        "Epoch 82/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02611876092851162\n",
                        "\n",
                        "\n",
                        "Epoch 83/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612598054111004\n",
                        "\n",
                        "\n",
                        "Epoch 84/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026128610596060753\n",
                        "\n",
                        "\n",
                        "Epoch 85/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026122458279132843\n",
                        "\n",
                        "\n",
                        "Epoch 86/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124905794858932\n",
                        "\n",
                        "\n",
                        "Epoch 87/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026118412613868713\n",
                        "\n",
                        "\n",
                        "Epoch 88/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026119206100702286\n",
                        "\n",
                        "\n",
                        "Epoch 89/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026126667857170105\n",
                        "\n",
                        "\n",
                        "Epoch 90/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612191252410412\n",
                        "\n",
                        "\n",
                        "Epoch 91/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02611788734793663\n",
                        "\n",
                        "\n",
                        "Epoch 92/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026128429919481277\n",
                        "\n",
                        "\n",
                        "Epoch 93/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612242102622986\n",
                        "\n",
                        "\n",
                        "Epoch 94/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612992376089096\n",
                        "\n",
                        "\n",
                        "Epoch 95/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127692312002182\n",
                        "\n",
                        "\n",
                        "Epoch 96/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124417781829834\n",
                        "\n",
                        "\n",
                        "Epoch 97/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.0261252298951149\n",
                        "\n",
                        "\n",
                        "Epoch 98/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026125214993953705\n",
                        "\n",
                        "\n",
                        "Epoch 99/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026120642200112343\n",
                        "\n",
                        "\n",
                        "Epoch 100/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124177500605583\n",
                        "Result stage 2: 0.581*x0_t**2 + 2.613*x0_t + 11.402*cos(x0)\n",
                        "\n",
                        "\n",
                        "Epoch 1/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026117995381355286\n",
                        "\n",
                        "\n",
                        "Epoch 2/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124373078346252\n",
                        "\n",
                        "\n",
                        "Epoch 3/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02611895278096199\n",
                        "\n",
                        "\n",
                        "Epoch 4/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127619668841362\n",
                        "\n",
                        "\n",
                        "Epoch 5/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026126939803361893\n",
                        "\n",
                        "\n",
                        "Epoch 6/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612542361021042\n",
                        "\n",
                        "\n",
                        "Epoch 7/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026128480210900307\n",
                        "\n",
                        "\n",
                        "Epoch 8/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026129286736249924\n",
                        "\n",
                        "\n",
                        "Epoch 9/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612781524658203\n",
                        "\n",
                        "\n",
                        "Epoch 10/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026125149801373482\n",
                        "\n",
                        "\n",
                        "Epoch 11/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02611895278096199\n",
                        "\n",
                        "\n",
                        "Epoch 12/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127619668841362\n",
                        "\n",
                        "\n",
                        "Epoch 13/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026126939803361893\n",
                        "\n",
                        "\n",
                        "Epoch 14/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026119599118828773\n",
                        "\n",
                        "\n",
                        "Epoch 15/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612503059208393\n",
                        "\n",
                        "\n",
                        "Epoch 16/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026123227551579475\n",
                        "\n",
                        "\n",
                        "Epoch 17/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612249180674553\n",
                        "\n",
                        "\n",
                        "Epoch 18/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026123909279704094\n",
                        "\n",
                        "\n",
                        "Epoch 19/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026123767718672752\n",
                        "\n",
                        "\n",
                        "Epoch 20/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612297795712948\n",
                        "\n",
                        "\n",
                        "Epoch 21/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124564930796623\n",
                        "\n",
                        "\n",
                        "Epoch 22/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026123201474547386\n",
                        "\n",
                        "\n",
                        "Epoch 23/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612345479428768\n",
                        "\n",
                        "\n",
                        "Epoch 24/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026121266186237335\n",
                        "\n",
                        "\n",
                        "Epoch 25/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026126163080334663\n",
                        "\n",
                        "\n",
                        "Epoch 26/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026122987270355225\n",
                        "\n",
                        "\n",
                        "Epoch 27/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026125743985176086\n",
                        "\n",
                        "\n",
                        "Epoch 28/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612137980759144\n",
                        "\n",
                        "\n",
                        "Epoch 29/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612440660595894\n",
                        "\n",
                        "\n",
                        "Epoch 30/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026121849194169044\n",
                        "\n",
                        "\n",
                        "Epoch 31/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612229622900486\n",
                        "\n",
                        "\n",
                        "Epoch 32/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612454816699028\n",
                        "\n",
                        "\n",
                        "Epoch 33/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612297795712948\n",
                        "\n",
                        "\n",
                        "Epoch 34/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124564930796623\n",
                        "\n",
                        "\n",
                        "Epoch 35/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026123201474547386\n",
                        "\n",
                        "\n",
                        "Epoch 36/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612345479428768\n",
                        "\n",
                        "\n",
                        "Epoch 37/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026121266186237335\n",
                        "\n",
                        "\n",
                        "Epoch 38/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612481638789177\n",
                        "\n",
                        "\n",
                        "Epoch 39/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026122920215129852\n",
                        "\n",
                        "\n",
                        "Epoch 40/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026125136762857437\n",
                        "\n",
                        "\n",
                        "Epoch 41/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124054566025734\n",
                        "\n",
                        "\n",
                        "Epoch 42/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026122942566871643\n",
                        "\n",
                        "\n",
                        "Epoch 43/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612297795712948\n",
                        "\n",
                        "\n",
                        "Epoch 44/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124564930796623\n",
                        "\n",
                        "\n",
                        "Epoch 45/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026123201474547386\n",
                        "\n",
                        "\n",
                        "Epoch 46/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612345479428768\n",
                        "\n",
                        "\n",
                        "Epoch 47/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026121266186237335\n",
                        "\n",
                        "\n",
                        "Epoch 48/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612481638789177\n",
                        "\n",
                        "\n",
                        "Epoch 49/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026121849194169044\n",
                        "\n",
                        "\n",
                        "Epoch 50/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612229622900486\n",
                        "\n",
                        "\n",
                        "Epoch 51/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612454816699028\n",
                        "\n",
                        "\n",
                        "Epoch 52/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612297795712948\n",
                        "\n",
                        "\n",
                        "Epoch 53/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124564930796623\n",
                        "\n",
                        "\n",
                        "Epoch 54/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026123201474547386\n",
                        "\n",
                        "\n",
                        "Epoch 55/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612345479428768\n",
                        "\n",
                        "\n",
                        "Epoch 56/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026121266186237335\n",
                        "\n",
                        "\n",
                        "Epoch 57/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612541802227497\n",
                        "\n",
                        "\n",
                        "Epoch 58/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612505480647087\n",
                        "\n",
                        "\n",
                        "Epoch 59/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026125136762857437\n",
                        "\n",
                        "\n",
                        "Epoch 60/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026122942566871643\n",
                        "\n",
                        "\n",
                        "Epoch 61/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026123346760869026\n",
                        "\n",
                        "\n",
                        "Epoch 62/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026121020317077637\n",
                        "\n",
                        "\n",
                        "Epoch 63/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612481638789177\n",
                        "\n",
                        "\n",
                        "Epoch 64/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026121849194169044\n",
                        "\n",
                        "\n",
                        "Epoch 65/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026123251765966415\n",
                        "\n",
                        "\n",
                        "Epoch 66/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612350694835186\n",
                        "\n",
                        "\n",
                        "Epoch 67/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026122188195586205\n",
                        "\n",
                        "\n",
                        "Epoch 68/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612284943461418\n",
                        "\n",
                        "\n",
                        "Epoch 69/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612123265862465\n",
                        "\n",
                        "\n",
                        "Epoch 70/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026125682517886162\n",
                        "\n",
                        "\n",
                        "Epoch 71/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612578496336937\n",
                        "\n",
                        "\n",
                        "Epoch 72/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124851778149605\n",
                        "\n",
                        "\n",
                        "Epoch 73/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612285129725933\n",
                        "\n",
                        "\n",
                        "Epoch 74/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612297795712948\n",
                        "\n",
                        "\n",
                        "Epoch 75/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026124564930796623\n",
                        "\n",
                        "\n",
                        "Epoch 76/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026123201474547386\n",
                        "\n",
                        "\n",
                        "Epoch 77/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.0261224452406168\n",
                        "\n",
                        "\n",
                        "Epoch 78/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612284943461418\n",
                        "\n",
                        "\n",
                        "Epoch 79/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612123265862465\n",
                        "\n",
                        "\n",
                        "Epoch 80/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026125682517886162\n",
                        "\n",
                        "\n",
                        "Epoch 81/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026128945872187614\n",
                        "\n",
                        "\n",
                        "Epoch 82/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.0261275265365839\n",
                        "\n",
                        "\n",
                        "Epoch 83/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612888067960739\n",
                        "\n",
                        "\n",
                        "Epoch 84/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026131944730877876\n",
                        "\n",
                        "\n",
                        "Epoch 85/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612842246890068\n",
                        "\n",
                        "\n",
                        "Epoch 86/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127658784389496\n",
                        "\n",
                        "\n",
                        "Epoch 87/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026128308847546577\n",
                        "\n",
                        "\n",
                        "Epoch 88/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127830147743225\n",
                        "\n",
                        "\n",
                        "Epoch 89/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612888067960739\n",
                        "\n",
                        "\n",
                        "Epoch 90/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026132045313715935\n",
                        "\n",
                        "\n",
                        "Epoch 91/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026128336787223816\n",
                        "\n",
                        "\n",
                        "Epoch 92/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127392426133156\n",
                        "\n",
                        "\n",
                        "Epoch 93/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127727702260017\n",
                        "\n",
                        "\n",
                        "Epoch 94/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127364486455917\n",
                        "\n",
                        "\n",
                        "Epoch 95/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026128308847546577\n",
                        "\n",
                        "\n",
                        "Epoch 96/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127830147743225\n",
                        "\n",
                        "\n",
                        "Epoch 97/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.02612888067960739\n",
                        "\n",
                        "\n",
                        "Epoch 98/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026132045313715935\n",
                        "\n",
                        "\n",
                        "Epoch 99/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026128336787223816\n",
                        "\n",
                        "\n",
                        "Epoch 100/100\n",
                        "Learning rate :  3e-05\n",
                        "Average loss :  0.026127392426133156\n",
                        "Result stage 3: 0.583*x0_t**2 + 2.613*x0_t + 11.429*cos(x0)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "if(save==True):\r\n",
                "    #Saving Equation in string\r\n",
                "    text_file = open(rootdir + \"lagrangian_\" + str(noiselevel)+ \"_noise.txt\", \"w\")\r\n",
                "    text_file.write(L)\r\n",
                "    text_file.close()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit ('SystemIdentification': conda)"
        },
        "interpreter": {
            "hash": "d5784be8b0ed123205c521437a438df309f2d2f16cb6cf8124a1b3e0f87bfce1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}